diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml
index 420d7bc..68497a3 100644
--- a/.github/workflows/ci.yml
+++ b/.github/workflows/ci.yml
@@ -9,11 +9,21 @@ on:
 jobs:
   tests:
     runs-on: ubuntu-latest
+    strategy:
+      matrix:
+        python-version: ["3.11", "3.12"]
     steps:
       - uses: actions/checkout@v4
+      - name: Cache uv downloads
+        uses: actions/cache@v4
+        with:
+          path: ~/.cache/uv
+          key: ${{ runner.os }}-uv-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml', 'uv.lock') }}
+          restore-keys: |
+            ${{ runner.os }}-uv-${{ matrix.python-version }}-
       - uses: actions/setup-python@v5
         with:
-          python-version: '3.12'
+          python-version: ${{ matrix.python-version }}
       - name: Install uv (preferred)
         uses: astral-sh/setup-uv@v1
         with:
@@ -26,21 +36,41 @@ jobs:
             uv venv
             . .venv/bin/activate
             uv pip install --editable '.[aws,azure,test]' || uv pip install --editable '.[test]'
+            uv pip install ruff==0.5.7 mypy==1.10.0 coverage==7.5.4
           else
             python -m venv .venv
             . .venv/bin/activate
             python -m pip install --upgrade pip
             pip install --editable '.[aws,azure,test]' || pip install --editable '.[test]'
+            pip install ruff==0.5.7 mypy==1.10.0 coverage==7.5.4
           fi
-      - name: Run unit tests
+      - name: Ruff lint
+        shell: bash
+        run: |
+          set -euxo pipefail
+          . .venv/bin/activate
+          ruff check
+      - name: Mypy type-check
         shell: bash
         run: |
           set -euxo pipefail
           . .venv/bin/activate
-          python -m unittest discover -s tests -p 'test_*.py' -v
-      - name: Run pytest suite
+          mypy src/fmf/core src/fmf/inference
+      - name: Run tests with coverage
         shell: bash
         run: |
           set -euxo pipefail
           . .venv/bin/activate
-          pytest -q
+          coverage run -m pytest -q
+      - name: Coverage report
+        shell: bash
+        run: |
+          set -euxo pipefail
+          . .venv/bin/activate
+          coverage report -m --fail-under=70
+          coverage xml -o coverage.xml
+      - name: Upload coverage
+        uses: actions/upload-artifact@v4
+        with:
+          name: coverage-${{ matrix.python-version }}
+          path: coverage.xml
diff --git a/MIGRATION.md b/MIGRATION.md
new file mode 100644
index 0000000..3c1156d
--- /dev/null
+++ b/MIGRATION.md
@@ -0,0 +1,42 @@
+# Frontier Model Framework – Migration Notes
+
+## Provider registry adoption
+- Providers now self-register via `fmf.inference.registry.register_provider`. Existing code that called
+  `fmf.inference.unified.build_llm_client` continues to work, but direct instantiation should migrate to
+  registry-based factories. Custom providers can decorate their factory with
+  `@register_provider("your-name")`.
+- Legacy `inference.unified` retains conditional fallbacks for backward compatibility but will be
+  deprecated in favour of registry lookups in a future release.
+
+## S3 exporter write modes
+- `ExportSpec.write_mode` replaces the old `mode` attribute; `S3Exporter` now performs atomic overwrite
+  by uploading to a temporary key, verifying the checksum (Content-MD5), copying to the final key, and
+  deleting the temporary object. Append semantics remain one-object-per-call. Upsert currently raises
+  `ExportError` pending merge-manifest support.
+- YAML configs may still specify `mode:`; it is mapped to `write_mode` automatically.
+
+## Deterministic IDs & provenance
+- Documents and chunks derive identifiers from content hashes (`doc_<hash>`). Regenerated artefacts may
+  change IDs compared to pre-follow-up runs; downstream systems should use provenance fields rather than
+  UUID suffixes for correlation.
+- `processing.hash_algo` controls the hashing algorithm (`blake2b` default, `xxh64` optional). Set via
+  YAML or `FMF_HASH_ALGO`. Text normalisation (Unicode NFC, newline canonicalisation) ensures cross-platform
+  stability.
+
+## Configuration toggles
+- New YAML fields under `experimental`, `processing.hash_algo`, and `retries.max_elapsed_s` mirror the
+  existing environment variables (`FMF_EXPERIMENTAL_STREAMING`, `FMF_OBSERVABILITY_OTEL`, `FMF_HASH_ALGO`,
+  `FMF_RETRY_MAX_ELAPSED`). Environment variables continue to take precedence.
+
+## Error hierarchy & CLI exits
+- CLI commands map framework errors to deterministic exit codes (`ConnectorError` => 4, etc.). Scripts
+  wrapping `fmf` should adjust to non-zero exit codes instead of string matching.
+
+## Keys diagnostics
+- `fmf keys test --json` emits structured diagnostics for secrets, connectors, providers, and exporters.
+  Text output is unchanged otherwise.
+
+**Upgrade guidance**
+1. Review exporter configurations for `mode:` usage; specify `write_mode: overwrite` where atomic writes are required.
+2. Update custom providers to use the registry decorator and ensure streaming generators raise `StopIteration` with the final completion.
+3. For pipelines relying on old UUID chunk IDs, refresh downstream keys using the new deterministic `provenance` metadata.
diff --git a/docs/review/CHANGELOG_FOLLOWUP.md b/docs/review/CHANGELOG_FOLLOWUP.md
new file mode 100644
index 0000000..0635319
--- /dev/null
+++ b/docs/review/CHANGELOG_FOLLOWUP.md
@@ -0,0 +1,11 @@
+# Changelog (Follow-up)
+
+- Added streaming iterators for Azure OpenAI and Bedrock adapters with env toggle (`FMF_EXPERIMENTAL_STREAMING`).
+- Introduced deterministic ID + provenance layer (`fmf.core.ids`), updating loaders/chunkers and adding reproducibility tests.
+- Hardened Local/S3/SharePoint connectors to share `BaseConnector`, include retries, and return managed streams.
+- Implemented provider registry with decorator-based registration; `build_llM_client` now honours registered factories.
+- Enhanced S3 exporter for atomic overwrite semantics and documented unsupported upsert path.
+- Normalised error hierarchy under `fmf.core.errors` and replaced local `datetime.utcnow()` calls with timezone-aware variants.
+- Expanded `fmf keys test` to emit diagnostics for connectors/providers/exporters while preserving secret redaction.
+- CI now runs lint (ruff), type-check (mypy), pytest, and coverage (≥70%) across Python 3.11/3.12; added `ruff.toml` and `mypy.ini`.
+- Added runnable recipe scripts for streaming, deterministic IDs, and export write modes.
diff --git a/docs/review/DIFFS_followup.patch b/docs/review/DIFFS_followup.patch
new file mode 100644
index 0000000..b6a9604
--- /dev/null
+++ b/docs/review/DIFFS_followup.patch
@@ -0,0 +1,2503 @@
+diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml
+index 420d7bc..010c168 100644
+--- a/.github/workflows/ci.yml
++++ b/.github/workflows/ci.yml
+@@ -9,11 +9,14 @@ on:
+ jobs:
+   tests:
+     runs-on: ubuntu-latest
++    strategy:
++      matrix:
++        python-version: ["3.11", "3.12"]
+     steps:
+       - uses: actions/checkout@v4
+       - uses: actions/setup-python@v5
+         with:
+-          python-version: '3.12'
++          python-version: ${{ matrix.python-version }}
+       - name: Install uv (preferred)
+         uses: astral-sh/setup-uv@v1
+         with:
+@@ -26,21 +29,35 @@ jobs:
+             uv venv
+             . .venv/bin/activate
+             uv pip install --editable '.[aws,azure,test]' || uv pip install --editable '.[test]'
++            uv pip install ruff mypy coverage
+           else
+             python -m venv .venv
+             . .venv/bin/activate
+             python -m pip install --upgrade pip
+             pip install --editable '.[aws,azure,test]' || pip install --editable '.[test]'
++            pip install ruff mypy coverage
+           fi
+-      - name: Run unit tests
++      - name: Ruff lint
+         shell: bash
+         run: |
+           set -euxo pipefail
+           . .venv/bin/activate
+-          python -m unittest discover -s tests -p 'test_*.py' -v
+-      - name: Run pytest suite
++          ruff check
++      - name: Mypy type-check
+         shell: bash
+         run: |
+           set -euxo pipefail
+           . .venv/bin/activate
+-          pytest -q
++          mypy src/fmf/core src/fmf/inference
++      - name: Run tests with coverage
++        shell: bash
++        run: |
++          set -euxo pipefail
++          . .venv/bin/activate
++          coverage run -m pytest -q
++      - name: Coverage report
++        shell: bash
++        run: |
++          set -euxo pipefail
++          . .venv/bin/activate
++          coverage report -m --fail-under=70
+diff --git a/docs/review/CHANGELOG_FOLLOWUP.md b/docs/review/CHANGELOG_FOLLOWUP.md
+new file mode 100644
+index 0000000..0635319
+--- /dev/null
++++ b/docs/review/CHANGELOG_FOLLOWUP.md
+@@ -0,0 +1,11 @@
++# Changelog (Follow-up)
++
++- Added streaming iterators for Azure OpenAI and Bedrock adapters with env toggle (`FMF_EXPERIMENTAL_STREAMING`).
++- Introduced deterministic ID + provenance layer (`fmf.core.ids`), updating loaders/chunkers and adding reproducibility tests.
++- Hardened Local/S3/SharePoint connectors to share `BaseConnector`, include retries, and return managed streams.
++- Implemented provider registry with decorator-based registration; `build_llM_client` now honours registered factories.
++- Enhanced S3 exporter for atomic overwrite semantics and documented unsupported upsert path.
++- Normalised error hierarchy under `fmf.core.errors` and replaced local `datetime.utcnow()` calls with timezone-aware variants.
++- Expanded `fmf keys test` to emit diagnostics for connectors/providers/exporters while preserving secret redaction.
++- CI now runs lint (ruff), type-check (mypy), pytest, and coverage (≥70%) across Python 3.11/3.12; added `ruff.toml` and `mypy.ini`.
++- Added runnable recipe scripts for streaming, deterministic IDs, and export write modes.
+diff --git a/docs/review/IMPLEMENTATION_NOTES.md b/docs/review/IMPLEMENTATION_NOTES.md
+new file mode 100644
+index 0000000..fbfca49
+--- /dev/null
++++ b/docs/review/IMPLEMENTATION_NOTES.md
+@@ -0,0 +1,10 @@
++# Implementation Notes
++
++- Streaming support now honours `FMF_EXPERIMENTAL_STREAMING`; when enabled, Azure and Bedrock adapters consume chunk events and invoke `on_token` per provider delta. Without the flag, they still emit a single chunk (no space-splitting). New tests cover both paths.
++- Connectors inherit the shared `BaseConnector`, gaining optional `RunContext` plumbing and resilient retries. S3 uses exponential backoff with jitter and returns context-managed bodies to avoid leaking sockets. SharePoint retains its Graph client but now exposes selectors/exclude rules.
++- Deterministic IDs are derived from content hashes via `fmf.core.ids`. Documents and chunks record provenance metadata (`created_at`, chunk index, length) to aid audit trails.
++- Provider registry (`fmf.inference.registry`) lets adapters self-register. `build_llm_client` first consults the registry, then falls back to legacy instantiation for compatibility.
++- S3 exporter respects `write_mode` (`append`/`overwrite`) and uses atomic copy-on-write semantics for overwrites. Upserts are surfaced as TODO via `ExportError`.
++- Error handling is centralised in `fmf.core.errors`, giving CLI/SDK a consistent base for mapping failures to exit codes.
++- Logging remains structured but honours `FMF_OBSERVABILITY_OTEL` before wiring OpenTelemetry spans. Timestamps are now timezone-aware.
++- `fmf keys test` prints a quick diagnostics report for connectors/providers/exporters, flagging missing configuration but avoiding destructive calls.
+diff --git a/docs/review/VIABILITY.md b/docs/review/VIABILITY.md
+new file mode 100644
+index 0000000..3b7c1bf
+--- /dev/null
++++ b/docs/review/VIABILITY.md
+@@ -0,0 +1,14 @@
++# Viability Assessment – Follow-up Scope
++
++| Item | Current Context (files, APIs) | Feasibility | Risks / Unknowns | Est. Effort | Plan / De-risking |
++| --- | --- | --- | --- | --- | --- |
++| True streaming for providers | `src/fmf/inference/azure_openai.py`, `src/fmf/inference/bedrock.py`, `src/fmf/inference/base_client.py`, no iterable API yet, transport mocked via callables | Medium | SDK streaming support not available offline; need to simulate with mocked chunks without new deps; ensure env flag gating | M | Introduce `iter_tokens` in provider base using generator pattern; gate behind `FMF_EXPERIMENTAL_STREAMING`; add unit tests with fake streaming transports returning chunk lists |
++| Connector hardening | `src/fmf/connectors/local.py`, `s3.py`, `sharepoint.py` still using legacy protocol; no retries/backoff | Medium | SharePoint connector relies on msgraph SDK which may not be installed in tests; need to mock requests; ensure backward compatibility with existing config | M | Create `BaseConnector` subclasses, wrap reads with `contextlib.closing`, add retry helper (decorrelated jitter); implement tests using temp FS + moto stub (already optional) |
++| Deterministic IDs & provenance | `src/fmf/types.py`, `src/fmf/core/interfaces/models.py` currently random UUIDs; no hashing utilities | Medium | Hash algorithm availability (xxhash not installed); need fallback; propagate IDs without breaking existing artefacts | M | Add `core/ids.py` with blake2b default + optional xxhash import; update document/chunk creation flows; tests ensure stable IDs for same content |
++| Provider registry | Providers built via `inference/unified.py` factory with if/else; no registry | High | Minimal risk; ensure compatibility with existing config loader | S | Implement decorator-based registry (`inference/registry.py`), update `build_llm_client`; add unit tests ensuring fallback errors |
++| Export idempotency & write modes | Exporters (e.g., `exporters/s3.py`, `dynamodb.py`, `sharepoint_excel.py`) accept `mode` but inconsistent; no atomic writes | Medium | Atomic writes on S3 require temp keys or conditional PUT; limited to append/overwrite; integration tests need mocking | L | Extend `ExportSpec`; implement local temp file rename; for S3 use upload then copy; add tests using moto or local FS |
++| Error hierarchy | Multiple bespoke exceptions (ConnectorError, AuthError, etc.) | High | Need to avoid breaking except clauses elsewhere | S | Create `core/errors.py` deriving from `Exception`, update modules to alias or subclass existing errors; adjust tests |
++| CI & quality upgrades | `.github/workflows/ci.yml` single-job Py3.12 only; no lint/coverage | Medium | Installing ruff/mypy/coverage offline may fail; coverage threshold may fail initially | M | Extend workflow matrix (3.11/3.12), add lint steps, configure minimal `ruff.toml` & `mypy.ini`, measure coverage with modest threshold (e.g., 70%) |
++| Observability & time | `datetime.utcnow()` scattered (runner, cli, exporters); simple OTEL helper exists; logs need redaction | Medium | Need to ensure timezone change doesn’t break tests expecting string format; optional OTEL flag interacts with existing tracer stub | M | Introduce util `utc_now()` returning timezone-aware; update tests; add log redaction helper; gate OTEL spans behind env flag |
++| “keys test” diagnostics | CLI already has `keys test` basic redaction in `src/fmf/cli.py`; needs richer diagnostics | High | Need to inspect configs without hitting real services; ensure backwards compatibility with tests | S | Extend command to read active profile, simulate connector/provider/exporter checks using safe no-op operations; add tests verifying output |
++
+diff --git a/examples/recipes/deterministic_ids_demo.py b/examples/recipes/deterministic_ids_demo.py
+new file mode 100644
+index 0000000..b326646
+--- /dev/null
++++ b/examples/recipes/deterministic_ids_demo.py
+@@ -0,0 +1,32 @@
++"""Show deterministic document and chunk identifiers derived from content hashes."""
++
++from __future__ import annotations
++
++from fmf.processing.loaders import load_document_from_bytes
++from fmf.processing.chunking import chunk_text
++
++
++def main() -> None:
++    payload = b"Paragraph one. Paragraph two."
++    doc1 = load_document_from_bytes(
++        source_uri="file:///demo.txt",
++        filename="demo.txt",
++        data=payload,
++        processing_cfg={"text": {"chunking": {"splitter": "by_sentence", "max_tokens": 20}}},
++    )
++    doc2 = load_document_from_bytes(
++        source_uri="file:///demo.txt",
++        filename="demo.txt",
++        data=payload,
++        processing_cfg={},
++    )
++    print("document IDs equal:", doc1.id == doc2.id)
++    print("provenance:", doc1.provenance)
++
++    chunks = chunk_text(doc_id=doc1.id, text=doc1.text or "", splitter="by_sentence")
++    for chunk in chunks:
++        print(f"chunk {chunk.provenance['index']}: id={chunk.id} length={chunk.provenance['length_chars']}")
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/examples/recipes/export_write_modes_demo.py b/examples/recipes/export_write_modes_demo.py
+new file mode 100644
+index 0000000..1d8e566
+--- /dev/null
++++ b/examples/recipes/export_write_modes_demo.py
+@@ -0,0 +1,51 @@
++"""Illustrate append vs overwrite semantics for the S3 exporter using a stubbed boto3 client."""
++
++from __future__ import annotations
++
++import sys
++import types
++
++from fmf.core.interfaces import ExportSpec
++from fmf.exporters.s3 import S3Exporter
++
++
++def install_stub() -> list[str]:
++    calls: list[str] = []
++
++    class DummyClient:
++        def put_object(self, **kwargs):
++            calls.append(f"put:{kwargs['Key']}")
++
++        def copy_object(self, **kwargs):
++            calls.append(f"copy:{kwargs['Key']}")
++
++        def delete_object(self, **kwargs):
++            calls.append(f"delete:{kwargs['Key']}")
++
++    sys.modules["boto3"] = types.SimpleNamespace(client=lambda *_args, **_kwargs: DummyClient())  # type: ignore
++    return calls
++
++
++def main() -> None:
++    original = sys.modules.get("boto3")
++    calls = install_stub()
++    try:
++        append_spec = ExportSpec(name="s3", type="s3", format="jsonl", write_mode="append", options={"bucket": "demo", "prefix": "runs/"})
++        append_exporter = S3Exporter(spec=append_spec)
++        append_exporter.write([{"row": 1}], context={"run_id": "r-001"})
++
++        overwrite_spec = ExportSpec(name="s3", type="s3", format="jsonl", write_mode="overwrite", options={"bucket": "demo", "prefix": "runs/"})
++        overwrite_exporter = S3Exporter(spec=overwrite_spec)
++        overwrite_exporter.write([{"row": 1}], context={"run_id": "r-001", "filename": "latest"})
++    finally:
++        if original is None:
++            sys.modules.pop("boto3", None)
++        else:
++            sys.modules["boto3"] = original
++
++    for call in calls:
++        print(call)
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/examples/recipes/streaming_text_demo.py b/examples/recipes/streaming_text_demo.py
+new file mode 100644
+index 0000000..49199ec
+--- /dev/null
++++ b/examples/recipes/streaming_text_demo.py
+@@ -0,0 +1,49 @@
++"""Demonstrate streaming token emission using the experimental flag.
++
++This recipe wires a stub transport into ``AzureOpenAIClient`` so it can run without
++external services. Set ``FMF_EXPERIMENTAL_STREAMING=1`` to enable chunked delivery.
++"""
++
++from __future__ import annotations
++
++import os
++
++from fmf.inference.azure_openai import AzureOpenAIClient
++from fmf.inference.base_client import Message
++
++
++def main() -> None:
++    os.environ.setdefault("FMF_EXPERIMENTAL_STREAMING", "1")
++
++    def transport(_payload):
++        return {
++            "choices": [
++                {"message": {"content": "fallback"}, "finish_reason": "stop"}
++            ],
++            "model": "demo",
++            "usage": {"prompt_tokens": 1, "completion_tokens": 1},
++        }
++
++    def stream_transport(_payload):
++        yield {"choices": [{"delta": {"content": "Hello "}}]}
++        yield {"choices": [{"delta": {"content": "world"}, "finish_reason": "stop"}]}
++
++    client = AzureOpenAIClient(
++        endpoint="https://example",
++        api_version="2024-02-15-preview",
++        deployment="demo",
++        transport=transport,
++        stream_transport=stream_transport,
++    )
++    tokens: list[str] = []
++    completion = client.complete(
++        [Message(role="user", content="Say hello")],
++        stream=True,
++        on_token=tokens.append,
++    )
++    print("streamed tokens:", tokens)
++    print("completion:", completion.text)
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/mypy.ini b/mypy.ini
+new file mode 100644
+index 0000000..60d1f19
+--- /dev/null
++++ b/mypy.ini
+@@ -0,0 +1,5 @@
++[mypy]
++python_version = 3.11
++ignore_missing_imports = True
++show_error_codes = True
++files = src/fmf/core, src/fmf/inference
+diff --git a/ruff.toml b/ruff.toml
+new file mode 100644
+index 0000000..0716f69
+--- /dev/null
++++ b/ruff.toml
+@@ -0,0 +1,8 @@
++[tool.ruff]
++line-length = 100
++target-version = "py311"
++select = ["E", "F", "W"]
++ignore = ["E501"]
++
++[tool.ruff.lint]
++preview = false
+diff --git a/src/fmf/auth/providers.py b/src/fmf/auth/providers.py
+index f5cd42c..0246aec 100644
+--- a/src/fmf/auth/providers.py
++++ b/src/fmf/auth/providers.py
+@@ -7,10 +7,7 @@ from typing import Dict, Iterable, Mapping, Protocol
+ 
+ # Accept either Pydantic models or dict-like configs to avoid hard dependency at import time
+ from ..config.models import AuthConfig  # type: ignore
+-
+-
+-class AuthError(Exception):
+-    """Raised when secret resolution fails or a provider is unavailable."""
++from ..core.errors import AuthError
+ 
+ 
+ def _redact(_: str | None) -> str:
+diff --git a/src/fmf/chain/runner.py b/src/fmf/chain/runner.py
+index 8980590..99c00d0 100644
+--- a/src/fmf/chain/runner.py
++++ b/src/fmf/chain/runner.py
+@@ -10,7 +10,7 @@ from typing import Any, Dict, List, Tuple
+ from ..connectors import build_connector
+ from ..processing.loaders import load_document_from_bytes
+ from ..processing.table_rows import iter_table_rows
+-from ..processing.chunking import chunk_text
++from ..processing.chunking import chunk_text, estimate_tokens
+ from ..types import Chunk, Document
+ from ..processing.persist import persist_artefacts, ensure_dir
+ from ..inference.unified import build_llm_client
+@@ -21,6 +21,7 @@ from ..observability import metrics as _metrics
+ from ..observability.tracing import trace_span
+ from ..prompts.registry import build_prompt_registry
+ from ..rag import build_rag_pipelines
++from ..core.ids import chunk_id as compute_chunk_id
+ from .loader import ChainConfig, ChainStep, load_chain
+ 
+ 
+@@ -387,7 +388,16 @@ def _run_chain_loaded(
+                     # Ensure we still create a work item for multimodal steps even without text
+                     from ..processing.chunking import estimate_tokens
+ 
+-                    chunks.append(Chunk(id=f"{doc.id}_ch0", doc_id=doc.id, text="", tokens_estimate=estimate_tokens("") ))
++                    chunk_identifier = compute_chunk_id(document_id=doc.id, index=0, payload="")
++                    chunks.append(
++                        Chunk(
++                            id=chunk_identifier,
++                            doc_id=doc.id,
++                            text="",
++                            tokens_estimate=estimate_tokens(""),
++                            provenance={"index": 0, "splitter": "auto", "length_chars": 0},
++                        )
++                    )
+ 
+         # Build image groups if requested
+         if input_mode == "images_group" and image_docs:
+@@ -664,7 +674,7 @@ def _run_chain_loaded(
+             context_all[step.output] = results
+ 
+     # Persist artefacts and write run.yaml
+-    run_id = dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
++    run_id = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%dT%H%M%SZ")
+     paths = persist_artefacts(artefacts_dir=artefacts_dir or "artefacts", run_id=run_id, documents=documents, chunks=chunks)
+     run_dir = os.path.dirname(paths["docs"])
+     # write outputs.jsonl for the last step by default
+diff --git a/src/fmf/cli.py b/src/fmf/cli.py
+index ae0cb02..de9eb7b 100644
+--- a/src/fmf/cli.py
++++ b/src/fmf/cli.py
+@@ -2,7 +2,7 @@ from __future__ import annotations
+ 
+ import argparse
+ import sys
+-from typing import List
++from typing import Any, List
+ 
+ from .config.loader import load_config
+ from .auth import build_provider, AuthError
+@@ -226,10 +226,90 @@ def _cmd_keys_test(args: argparse.Namespace) -> int:
+         print(f"Secret resolution failed: {e}")
+         return 1
+ 
++    print("Secrets:")
+     for n in names:
+         status = "OK" if n in resolved else "MISSING"
+         print(f"{n}=**** {status}")
+ 
++    def _get(obj: Any, key: str, default: Any = None) -> Any:
++        if obj is None:
++            return default
++        if isinstance(obj, dict):
++            return obj.get(key, default)
++        return getattr(obj, key, default)
++
++    diagnostics: list[tuple[str, str, str, str]] = []
++
++    # Connectors diagnostics
++    connectors_cfg = getattr(cfg, "connectors", None) if not isinstance(cfg, dict) else cfg.get("connectors")
++    required_connectors = {
++        "local": ["root"],
++        "s3": ["bucket"],
++        "sharepoint": ["site_url", "drive"],
++    }
++    for c in connectors_cfg or []:
++        name = _get(c, "name", "unknown")
++        ctype = _get(c, "type", "unknown")
++        required = required_connectors.get(ctype, [])
++        missing = [field for field in required if not _get(c, field)]
++        if ctype not in required_connectors:
++            diagnostics.append(("Connector", name, "WARN", f"unknown type '{ctype}'"))
++        elif missing:
++            diagnostics.append(("Connector", name, "WARN", f"missing fields: {', '.join(missing)}"))
++        else:
++            diagnostics.append(("Connector", name, "OK", ""))
++
++    # Inference diagnostics
++    inference_cfg = getattr(cfg, "inference", None) if not isinstance(cfg, dict) else cfg.get("inference")
++    provider_name = _get(inference_cfg, "provider")
++    if provider_name:
++        from .inference.registry import available_providers
++
++        providers = available_providers()
++        if provider_name not in providers:
++            diagnostics.append(("Provider", provider_name, "FAIL", "not registered"))
++        else:
++            subcfg = _get(inference_cfg, provider_name)
++            required = {
++                "azure_openai": ["endpoint", "deployment"],
++                "aws_bedrock": ["region", "model_id"],
++            }.get(provider_name, [])
++            missing = [field for field in required if not _get(subcfg, field)]
++            if missing:
++                diagnostics.append(("Provider", provider_name, "WARN", f"missing fields: {', '.join(missing)}"))
++            else:
++                diagnostics.append(("Provider", provider_name, "OK", ""))
++    else:
++        diagnostics.append(("Provider", "<unset>", "WARN", "inference.provider not configured"))
++
++    # Exporter diagnostics
++    export_cfg = getattr(cfg, "export", None) if not isinstance(cfg, dict) else cfg.get("export")
++    sinks_cfg = _get(export_cfg, "sinks", [])
++    required_sinks = {
++        "s3": ["bucket"],
++        "dynamodb": ["table"],
++        "sharepoint_excel": ["site_url", "drive", "file_path"],
++    }
++    for sink in sinks_cfg or []:
++        name = _get(sink, "name", "unknown")
++        stype = _get(sink, "type", "unknown")
++        required = required_sinks.get(stype, [])
++        missing = [field for field in required if not _get(sink, field)]
++        if stype not in required_sinks:
++            diagnostics.append(("Exporter", name, "WARN", f"unknown type '{stype}'"))
++        elif missing:
++            diagnostics.append(("Exporter", name, "WARN", f"missing fields: {', '.join(missing)}"))
++        else:
++            diagnostics.append(("Exporter", name, "OK", ""))
++
++    if diagnostics:
++        print("Diagnostics:")
++        for category, name, status, note in diagnostics:
++            line = f"  {category:<10} {name:<20} {status}"
++            if note:
++                line += f" - {note}"
++            print(line)
++
+     return 0
+ 
+ 
+@@ -261,7 +341,7 @@ def _cmd_connect_ls(args: argparse.Namespace) -> int:
+ 
+ 
+ def _gen_run_id() -> str:
+-    ts = _dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
++    ts = _dt.datetime.now(_dt.timezone.utc).strftime("%Y%m%dT%H%M%SZ")
+     rand = _uuid.uuid4().hex[:6]
+     return f"{ts}-{rand}"
+ 
+diff --git a/src/fmf/connectors/__init__.py b/src/fmf/connectors/__init__.py
+index 36f2aa2..23da1d9 100644
+--- a/src/fmf/connectors/__init__.py
++++ b/src/fmf/connectors/__init__.py
+@@ -3,6 +3,7 @@
+ Includes base protocol/types and factories to create connectors from config.
+ """
+ 
++from ..core.interfaces import ConnectorSelectors, ConnectorSpec
+ from .base import DataConnector, ResourceInfo, ResourceRef, ConnectorError
+ 
+ 
+@@ -21,27 +22,53 @@ def build_connector(cfg: object) -> DataConnector:
+     if ctype == "local":
+         from .local import LocalConnector
+ 
+-        return LocalConnector(name=name, root=_cfg_get(cfg, "root"), include=_cfg_get(cfg, "include"), exclude=_cfg_get(cfg, "exclude"))
++        selectors = ConnectorSelectors(
++            include=list(_cfg_get(cfg, "include") or ["**/*"]),
++            exclude=list(_cfg_get(cfg, "exclude") or []),
++        )
++        spec = ConnectorSpec(
++            name=name,
++            type="local",
++            selectors=selectors,
++            options={"root": _cfg_get(cfg, "root")},
++        )
++        return LocalConnector(spec=spec)
+     if ctype == "s3":
+         from .s3 import S3Connector
+-
+-        return S3Connector(
++        selectors = ConnectorSelectors(
++            include=list(_cfg_get(cfg, "include") or ["**/*"]),
++            exclude=list(_cfg_get(cfg, "exclude") or []),
++        )
++        spec = ConnectorSpec(
+             name=name,
+-            bucket=_cfg_get(cfg, "bucket"),
+-            prefix=_cfg_get(cfg, "prefix"),
+-            region=_cfg_get(cfg, "region"),
+-            kms_required=_cfg_get(cfg, "kms_required"),
++            type="s3",
++            selectors=selectors,
++            options={
++                "bucket": _cfg_get(cfg, "bucket"),
++                "prefix": _cfg_get(cfg, "prefix"),
++                "region": _cfg_get(cfg, "region"),
++                "kms_required": _cfg_get(cfg, "kms_required"),
++            },
+         )
++        return S3Connector(spec=spec)
+     if ctype == "sharepoint":
+         from .sharepoint import SharePointConnector
+-
+-        return SharePointConnector(
++        selectors = ConnectorSelectors(
++            include=list(_cfg_get(cfg, "include") or ["**/*"]),
++            exclude=list(_cfg_get(cfg, "exclude") or []),
++        )
++        spec = ConnectorSpec(
+             name=name,
+-            site_url=_cfg_get(cfg, "site_url"),
+-            drive=_cfg_get(cfg, "drive"),
+-            root_path=_cfg_get(cfg, "root_path"),
+-            auth_profile=_cfg_get(cfg, "auth_profile"),
++            type="sharepoint",
++            selectors=selectors,
++            options={
++                "site_url": _cfg_get(cfg, "site_url"),
++                "drive": _cfg_get(cfg, "drive"),
++                "root_path": _cfg_get(cfg, "root_path"),
++                "auth_profile": _cfg_get(cfg, "auth_profile"),
++            },
+         )
++        return SharePointConnector(spec=spec)
+     raise ValueError(f"Unsupported connector type: {ctype!r}")
+ 
+ 
+diff --git a/src/fmf/connectors/base.py b/src/fmf/connectors/base.py
+index e1a15ba..d8ed7bc 100644
+--- a/src/fmf/connectors/base.py
++++ b/src/fmf/connectors/base.py
+@@ -4,9 +4,7 @@ import datetime as _dt
+ from dataclasses import dataclass
+ from typing import IO, Any, Iterable, Protocol
+ 
+-
+-class ConnectorError(Exception):
+-    """Errors raised by DataConnector implementations."""
++from ..core.errors import ConnectorError
+ 
+ 
+ @dataclass(frozen=True)
+@@ -62,4 +60,3 @@ __all__ = [
+     "ResourceInfo",
+     "DataConnector",
+ ]
+-
+diff --git a/src/fmf/connectors/local.py b/src/fmf/connectors/local.py
+index d205887..05a9a36 100644
+--- a/src/fmf/connectors/local.py
++++ b/src/fmf/connectors/local.py
+@@ -1,27 +1,38 @@
+ from __future__ import annotations
+ 
+-import os
+ import fnmatch
++import os
+ import pathlib
+ import datetime as dt
+ from typing import IO, Iterable, List, Optional
+ 
+-from .base import DataConnector, ResourceRef, ResourceInfo, ConnectorError
++from ..core.interfaces import ConnectorSpec, ConnectorSelectors, RunContext
++from ..core.interfaces.connectors_base import BaseConnector
++from .base import ResourceRef, ResourceInfo, ConnectorError
+ 
+ 
+-class LocalConnector:
++class LocalConnector(BaseConnector):
+     def __init__(
+         self,
+         *,
+-        name: str,
+-        root: str,
++        spec: ConnectorSpec | None = None,
++        name: str | None = None,
++        root: str | None = None,
+         include: Optional[List[str]] = None,
+         exclude: Optional[List[str]] = None,
+     ) -> None:
+-        self.name = name
+-        self.root = os.path.abspath(root)
+-        self._include = include or ["**/*"]
+-        self._exclude = exclude or []
++        if spec is None:
++            if name is None or root is None:
++                raise ValueError("LocalConnector requires either a spec or name/root parameters")
++            selectors = ConnectorSelectors(
++                include=list(include or ["**/*"]),
++                exclude=list(exclude or []),
++            )
++            spec = ConnectorSpec(name=name, type="local", selectors=selectors, options={"root": root})
++        super().__init__(spec)
++        self.root = os.path.abspath(spec.options.get("root", root or "."))
++        self._include = list(spec.selectors.include or ["**/*"])
++        self._exclude = list(spec.selectors.exclude or [])
+ 
+     def _iter_paths(self, selector: List[str] | None) -> Iterable[pathlib.Path]:
+         patterns = selector or self._include
+@@ -54,12 +65,23 @@ class LocalConnector:
+                 seen.add(rel)
+                 yield p
+ 
+-    def list(self, selector: list[str] | None = None) -> Iterable[ResourceRef]:
++    def list(
++        self,
++        *,
++        selector: list[str] | None = None,
++        context: RunContext | None = None,
++    ) -> Iterable[ResourceRef]:
+         for p in self._iter_paths(selector):
+             rel = p.relative_to(self.root).as_posix()
+             yield ResourceRef(id=rel, uri=p.resolve().as_uri(), name=p.name)
+ 
+-    def open(self, ref: ResourceRef, mode: str = "rb") -> IO[bytes]:
++    def open(
++        self,
++        ref: ResourceRef,
++        *,
++        mode: str = "rb",
++        context: RunContext | None = None,
++    ) -> IO[bytes]:
+         path = pathlib.Path(self.root, ref.id)
+         if not path.is_file():
+             raise ConnectorError(f"Resource not found: {ref.id}")
+@@ -68,7 +90,7 @@ class LocalConnector:
+             mode = mode + "b"
+         return open(path, mode)
+ 
+-    def info(self, ref: ResourceRef) -> ResourceInfo:
++    def info(self, ref: ResourceRef, *, context: RunContext | None = None) -> ResourceInfo:
+         path = pathlib.Path(self.root, ref.id)
+         if not path.exists():
+             raise ConnectorError(f"Resource not found: {ref.id}")
+diff --git a/src/fmf/connectors/s3.py b/src/fmf/connectors/s3.py
+index acc6e28..9840c58 100644
+--- a/src/fmf/connectors/s3.py
++++ b/src/fmf/connectors/s3.py
+@@ -3,28 +3,68 @@ from __future__ import annotations
+ import datetime as dt
+ from typing import IO, Iterable, List, Optional
+ 
+-from .base import DataConnector, ResourceInfo, ResourceRef, ConnectorError
++from ..core.interfaces import ConnectorSpec, ConnectorSelectors, RunContext
++from ..core.interfaces.connectors_base import BaseConnector
++from .base import ResourceInfo, ResourceRef, ConnectorError
+ 
+ 
+-class S3Connector:
++class _ManagedBody:
++    def __init__(self, body) -> None:
++        self._body = body
++
++    def read(self, *args, **kwargs):  # pragma: no cover - passthrough
++        return self._body.read(*args, **kwargs)
++
++    def close(self) -> None:
++        close = getattr(self._body, "close", None)
++        if callable(close):
++            close()
++
++    def __enter__(self):  # pragma: no cover - passthrough
++        return self
++
++    def __exit__(self, exc_type, exc, tb):  # pragma: no cover - passthrough
++        self.close()
++
++
++class S3Connector(BaseConnector):
+     def __init__(
+         self,
+         *,
+-        name: str,
+-        bucket: str,
++        spec: ConnectorSpec | None = None,
++        name: str | None = None,
++        bucket: str | None = None,
+         prefix: Optional[str] = None,
+         region: Optional[str] = None,
+         kms_required: Optional[bool] = None,
+     ) -> None:
+-        self.name = name
+-        self.bucket = bucket
+-        self.prefix = prefix or ""
++        if spec is None:
++            if name is None or bucket is None:
++                raise ValueError("S3Connector requires either a spec or name/bucket parameters")
++            selectors = ConnectorSelectors(include=["**/*"], exclude=[])
++            spec = ConnectorSpec(
++                name=name,
++                type="s3",
++                selectors=selectors,
++                options={
++                    "bucket": bucket,
++                    "prefix": prefix or "",
++                    "region": region,
++                    "kms_required": bool(kms_required),
++                },
++            )
++        super().__init__(spec)
++        options = spec.options
++        self.bucket = options.get("bucket", bucket)
++        self.prefix = options.get("prefix", prefix or "") or ""
+         if self.prefix and not self.prefix.endswith("/"):
+             # Normalize to directory-like prefix
+             self.prefix += "/"
+-        self.region = region
+-        self.kms_required = bool(kms_required)
++        self.region = options.get("region", region)
++        self.kms_required = bool(options.get("kms_required", kms_required))
+         self._client = None
++        self._include = list(spec.selectors.include or ["**/*"])
++        self._exclude = list(spec.selectors.exclude or [])
+ 
+     def _s3(self):
+         if self._client is not None:
+@@ -36,6 +76,28 @@ class S3Connector:
+         self._client = boto3.client("s3", region_name=self.region)
+         return self._client
+ 
++    def _retry(self, func, *args, **kwargs):
++        import random
++        import time
++
++        delay = 0.2
++        for attempt in range(5):
++            try:
++                return func(*args, **kwargs)
++            except Exception as exc:  # pragma: no cover - behaviour validated via tests
++                status = getattr(exc, "status_code", None)
++                if status is None:
++                    resp = getattr(exc, "response", None)
++                    if isinstance(resp, dict):
++                        status = resp.get("ResponseMetadata", {}).get("HTTPStatusCode")
++                if status in {429} or (isinstance(status, int) and status >= 500):
++                    jitter = random.uniform(0, delay)
++                    time.sleep(min(2.0, delay + jitter))
++                    delay = min(2.0, delay * 2)
++                    continue
++                raise
++        raise
++
+     def _iter_keys(self) -> Iterable[dict]:
+         client = self._s3()
+         token = None
+@@ -43,7 +105,7 @@ class S3Connector:
+             kwargs = {"Bucket": self.bucket, "Prefix": self.prefix}
+             if token:
+                 kwargs["ContinuationToken"] = token
+-            resp = client.list_objects_v2(**kwargs)
++            resp = self._retry(client.list_objects_v2, **kwargs)
+             for obj in resp.get("Contents", []) or []:
+                 yield obj
+             if resp.get("IsTruncated"):
+@@ -53,10 +115,15 @@ class S3Connector:
+             else:
+                 break
+ 
+-    def list(self, selector: list[str] | None = None) -> Iterable[ResourceRef]:
++    def list(
++        self,
++        *,
++        selector: list[str] | None = None,
++        context: RunContext | None = None,
++    ) -> Iterable[ResourceRef]:
+         import fnmatch
+ 
+-        patterns = selector or ["**/*"]
++        patterns = selector or self._include or ["**/*"]
+         for obj in self._iter_keys():
+             key = obj.get("Key")
+             if key is None:
+@@ -68,22 +135,30 @@ class S3Connector:
+                 for pat in patterns
+             ):
+                 continue
++            if self._exclude and any(fnmatch.fnmatchcase(rel, ex) for ex in self._exclude):
++                continue
+             uri = f"s3://{self.bucket}/{key}"
+             yield ResourceRef(id=rel, uri=uri, name=rel.split("/")[-1])
+ 
+-    def open(self, ref: ResourceRef, mode: str = "rb") -> IO[bytes]:
++    def open(
++        self,
++        ref: ResourceRef,
++        *,
++        mode: str = "rb",
++        context: RunContext | None = None,
++    ) -> IO[bytes]:
+         if "r" not in mode:
+             raise ConnectorError("S3Connector only supports reading")
+         key = self.prefix + ref.id if self.prefix else ref.id
+-        resp = self._s3().get_object(Bucket=self.bucket, Key=key)
++        resp = self._retry(self._s3().get_object, Bucket=self.bucket, Key=key)
+         body = resp.get("Body")
+         if body is None:
+             raise ConnectorError("Empty response body")
+-        return body  # type: ignore[return-value]
++        return _ManagedBody(body)
+ 
+-    def info(self, ref: ResourceRef) -> ResourceInfo:
++    def info(self, ref: ResourceRef, *, context: RunContext | None = None) -> ResourceInfo:
+         key = self.prefix + ref.id if self.prefix else ref.id
+-        head = self._s3().head_object(Bucket=self.bucket, Key=key)
++        head = self._retry(self._s3().head_object, Bucket=self.bucket, Key=key)
+         size = head.get("ContentLength")
+         last_modified = head.get("LastModified")
+         if isinstance(last_modified, dt.datetime) and last_modified.tzinfo is None:
+@@ -103,4 +178,3 @@ class S3Connector:
+ 
+ 
+ __all__ = ["S3Connector"]
+-
+diff --git a/src/fmf/connectors/sharepoint.py b/src/fmf/connectors/sharepoint.py
+index bde432e..832eaf9 100644
+--- a/src/fmf/connectors/sharepoint.py
++++ b/src/fmf/connectors/sharepoint.py
+@@ -1,14 +1,17 @@
+ from __future__ import annotations
+ 
+ import io
++import random
+ import time
+ import urllib.parse as _url
+ from typing import IO, Iterable, Optional
+ 
++from ..core.interfaces import ConnectorSpec, ConnectorSelectors, RunContext
++from ..core.interfaces.connectors_base import BaseConnector
+ from .base import ConnectorError, ResourceInfo, ResourceRef
+ 
+ 
+-class SharePointConnector:
++class SharePointConnector(BaseConnector):
+     """SharePoint connector using Microsoft Graph SDK.
+ 
+     - Lists and downloads files from a given site + drive + root_path using Graph paths
+@@ -19,18 +22,37 @@ class SharePointConnector:
+     def __init__(
+         self,
+         *,
+-        name: str,
+-        site_url: str,
+-        drive: str,
++        spec: ConnectorSpec | None = None,
++        name: str | None = None,
++        site_url: str | None = None,
++        drive: str | None = None,
+         root_path: Optional[str] = None,
+         auth_profile: Optional[str] = None,
+     ) -> None:
+-        self.name = name
+-        self.site_url = site_url
+-        self.drive = drive
+-        self.root_path = (root_path or "").strip("/")
+-        self.auth_profile = auth_profile
++        if spec is None:
++            if any(v is None for v in (name, site_url, drive)):
++                raise ValueError("SharePointConnector requires either a spec or name/site_url/drive parameters")
++            selectors = ConnectorSelectors(include=["**/*"], exclude=[])
++            spec = ConnectorSpec(
++                name=name,  # type: ignore[arg-type]
++                type="sharepoint",
++                selectors=selectors,
++                options={
++                    "site_url": site_url,
++                    "drive": drive,
++                    "root_path": root_path,
++                    "auth_profile": auth_profile,
++                },
++            )
++        super().__init__(spec)
++        options = spec.options
++        self.site_url = options.get("site_url", site_url)
++        self.drive = options.get("drive", drive)
++        self.root_path = (options.get("root_path", root_path) or "").strip("/")
++        self.auth_profile = options.get("auth_profile", auth_profile)
+         self._client = None
++        self._include = list(spec.selectors.include or ["**/*"])
++        self._exclude = list(spec.selectors.exclude or [])
+ 
+     def _client_or_raise(self):  # pragma: no cover - exercised via tests with monkeypatching
+         if self._client is not None:
+@@ -47,15 +69,16 @@ class SharePointConnector:
+         return self._client
+ 
+     def _retry(self, func, *args, **kwargs):
+-        delay = 0.5
+-        for _ in range(6):
++        delay = 0.3
++        for _ in range(5):
+             try:
+                 return func(*args, **kwargs)
+             except Exception as e:  # naive throttling/backoff handler
+                 status = getattr(e, "status_code", None) or getattr(getattr(e, "response", None), "status_code", None)
+                 if status == 429:
+-                    time.sleep(delay)
+-                    delay = min(delay * 2, 8.0)
++                    jitter = random.uniform(0, delay)
++                    time.sleep(min(5.0, delay + jitter))
++                    delay = min(delay * 2, 5.0)
+                     continue
+                 raise
+ 
+@@ -102,11 +125,16 @@ class SharePointConnector:
+         client = self._client_or_raise()
+         return self._retry(lambda: client.api(f"/sites/{site_id}/drives/{drive_id}/root:/{rel_path}").get())
+ 
+-    def list(self, selector: list[str] | None = None) -> Iterable[ResourceRef]:
++    def list(
++        self,
++        *,
++        selector: list[str] | None = None,
++        context: RunContext | None = None,
++    ) -> Iterable[ResourceRef]:
+         import fnmatch
+ 
+         site_id, drive_id = self._resolve_ids()
+-        patterns = selector or ["**/*"]
++        patterns = selector or self._include or ["**/*"]
+ 
+         stack = [self.root_path]
+         while stack:
+@@ -125,9 +153,17 @@ class SharePointConnector:
+                     for pat in patterns
+                 ):
+                     continue
++                if self._exclude and any(fnmatch.fnmatchcase(within, ex) for ex in self._exclude):
++                    continue
+                 yield ResourceRef(id=within, uri=f"sharepoint:/sites/{site_id}/drives/{drive_id}/root:/{rel}", name=name)
+ 
+-    def open(self, ref: ResourceRef, mode: str = "rb") -> IO[bytes]:
++    def open(
++        self,
++        ref: ResourceRef,
++        *,
++        mode: str = "rb",
++        context: RunContext | None = None,
++    ) -> IO[bytes]:
+         if "r" not in mode:
+             raise ConnectorError("SharePointConnector only supports reading")
+         site_id, drive_id = self._resolve_ids()
+@@ -137,7 +173,7 @@ class SharePointConnector:
+             return io.BytesIO(data)
+         return data
+ 
+-    def info(self, ref: ResourceRef) -> ResourceInfo:
++    def info(self, ref: ResourceRef, *, context: RunContext | None = None) -> ResourceInfo:
+         import datetime as dt
+ 
+         site_id, drive_id = self._resolve_ids()
+diff --git a/src/fmf/core/__init__.py b/src/fmf/core/__init__.py
+index affa95e..020f715 100644
+--- a/src/fmf/core/__init__.py
++++ b/src/fmf/core/__init__.py
+@@ -1,6 +1,9 @@
+ """Core abstractions shared across Frontier Model Framework layers."""
+ 
+ from . import interfaces as interfaces
++from . import errors as errors
+ from .interfaces import *  # noqa: F401,F403
++from .errors import *  # noqa: F401,F403
+ 
+-__all__ = interfaces.__all__
++combined = list(dict.fromkeys(list(interfaces.__all__) + list(errors.__all__)))
++__all__ = tuple(combined)
+diff --git a/src/fmf/core/errors.py b/src/fmf/core/errors.py
+new file mode 100644
+index 0000000..a70c7c6
+--- /dev/null
++++ b/src/fmf/core/errors.py
+@@ -0,0 +1,48 @@
++from __future__ import annotations
++
++from typing import Optional
++
++
++class FmfError(Exception):
++    """Base exception for the Frontier Model Framework."""
++
++    def __init__(self, message: str = "") -> None:
++        super().__init__(message)
++        self.message = message
++
++
++class ConfigError(FmfError):
++    pass
++
++
++class AuthError(FmfError):
++    pass
++
++
++class ConnectorError(FmfError):
++    pass
++
++
++class ProcessingError(FmfError):
++    pass
++
++
++class InferenceError(FmfError):
++    def __init__(self, message: str, *, status_code: Optional[int] = None) -> None:
++        super().__init__(message)
++        self.status_code = status_code
++
++
++class ExportError(FmfError):
++    pass
++
++
++__all__ = [
++    "FmfError",
++    "ConfigError",
++    "AuthError",
++    "ConnectorError",
++    "ProcessingError",
++    "InferenceError",
++    "ExportError",
++]
+diff --git a/src/fmf/core/ids.py b/src/fmf/core/ids.py
+new file mode 100644
+index 0000000..fbeca93
+--- /dev/null
++++ b/src/fmf/core/ids.py
+@@ -0,0 +1,63 @@
++from __future__ import annotations
++
++import hashlib
++import os
++from datetime import datetime, timezone
++from typing import Optional
++
++
++def _hash_algo() -> str:
++    return os.getenv("FMF_HASH_ALGO", "blake2b").lower()
++
++
++def hash_bytes(data: bytes, *, namespace: str = "", algo: str | None = None) -> str:
++    algo = (algo or _hash_algo())
++    if algo == "xxh64":  # optional fast hash
++        try:
++            import xxhash  # type: ignore
++
++            h = xxhash.xxh64()
++            if namespace:
++                h.update(namespace.encode("utf-8"))
++            h.update(data)
++            return h.hexdigest()
++        except Exception:
++            algo = "blake2b"
++    h = hashlib.blake2b(digest_size=16)
++    if namespace:
++        h.update(namespace.encode("utf-8"))
++    h.update(data)
++    return h.hexdigest()
++
++
++def document_id(*, source_uri: str, payload: bytes, modified_at: Optional[str] = None) -> str:
++    namespace = source_uri
++    if modified_at:
++        namespace = f"{namespace}|{modified_at}"
++    digest = hash_bytes(payload, namespace=namespace)
++    return f"doc_{digest}"
++
++
++def chunk_id(*, document_id: str, index: int, payload: str) -> str:
++    digest = hash_bytes(payload.encode("utf-8"), namespace=f"{document_id}|{index}")
++    return f"{document_id}_ch_{digest[:12]}"
++
++
++def blob_id(*, document_id: str, media_type: str, payload: bytes) -> str:
++    digest = hash_bytes(payload, namespace=f"{document_id}|{media_type}")
++    return f"blob_{digest[:12]}"
++
++
++def utc_now_iso() -> str:
++    tz = os.getenv("FMF_LOG_TZ", "UTC").upper()
++    zone = timezone.utc if tz in {"UTC", "Z"} else timezone.utc
++    return datetime.now(zone).isoformat().replace("+00:00", "Z")
++
++
++__all__ = [
++    "hash_bytes",
++    "document_id",
++    "chunk_id",
++    "blob_id",
++    "utc_now_iso",
++]
+diff --git a/src/fmf/core/interfaces/__init__.py b/src/fmf/core/interfaces/__init__.py
+index 3cb3451..ccce7f6 100644
+--- a/src/fmf/core/interfaces/__init__.py
++++ b/src/fmf/core/interfaces/__init__.py
+@@ -7,6 +7,7 @@ adopt them without breaking backwards compatibility.
+ 
+ from .models import (
+     ConnectorSpec,
++    ConnectorSelectors,
+     DocumentModel,
+     ChunkModel,
+     ModelSpec,
+@@ -36,6 +37,7 @@ __all__ = [
+     "EmbeddingResponse",
+     "BaseExporter",
+     "ConnectorSpec",
++    "ConnectorSelectors",
+     "DocumentModel",
+     "ChunkModel",
+     "ModelSpec",
+diff --git a/src/fmf/core/interfaces/models.py b/src/fmf/core/interfaces/models.py
+index 7188711..f0caa9c 100644
+--- a/src/fmf/core/interfaces/models.py
++++ b/src/fmf/core/interfaces/models.py
+@@ -3,7 +3,7 @@ from __future__ import annotations
+ import uuid
+ from typing import Any, Dict, List, Literal, Optional
+ 
+-from pydantic import BaseModel, Field
++from pydantic import BaseModel, Field, ConfigDict
+ 
+ 
+ class RunContext(BaseModel):
+@@ -104,15 +104,21 @@ class ModelSpec(BaseModel):
+ class ExportSpec(BaseModel):
+     """Standardised exporter configuration."""
+ 
++    model_config = ConfigDict(populate_by_name=True)
++
+     name: str
+     type: str
+     destination: str | None = None
+     format: Literal["jsonl", "csv", "parquet", "delta", "excel", "native", "custom"] = "jsonl"
+-    mode: Literal["append", "upsert", "overwrite"] = "append"
++    write_mode: Literal["append", "upsert", "overwrite"] = Field("append", alias="mode")
+     key_fields: list[str] | None = None
+     options: dict[str, Any] = Field(default_factory=dict)
+     metadata: dict[str, Any] = Field(default_factory=dict)
+ 
++    @property
++    def mode(self) -> str:
++        return self.write_mode
++
+ 
+ __all__ = [
+     "BlobModel",
+diff --git a/src/fmf/core/interfaces/providers_base.py b/src/fmf/core/interfaces/providers_base.py
+index 6905158..e781eed 100644
+--- a/src/fmf/core/interfaces/providers_base.py
++++ b/src/fmf/core/interfaces/providers_base.py
+@@ -1,7 +1,7 @@
+ from __future__ import annotations
+ 
+ from abc import ABC, abstractmethod
+-from typing import Any, Callable
++from typing import Any, Callable, Iterator
+ 
+ from pydantic import BaseModel, Field
+ 
+@@ -64,17 +64,32 @@ class BaseProvider(ABC):
+     def complete(self, request: CompletionRequest) -> CompletionResponse:
+         """Execute a non-streaming completion."""
+ 
++    def iter_tokens(self, request: CompletionRequest) -> Iterator[str]:  # pragma: no cover - overridable
++        """Yield tokens for streaming use-cases and return the final completion via StopIteration."""
++
++        response = self.complete(request)
++        if response.text:
++            yield response.text
++        return response  # type: ignore[misc]
++
+     def stream(
+         self,
+         request: CompletionRequest,
+         on_token: Callable[[str], None],
+-    ) -> CompletionResponse:  # pragma: no cover - default fallback
+-        """Optional streaming handler; falls back to non-streaming invocation."""
+-
+-        response = self.complete(request)
+-        if response.text:
+-            on_token(response.text)
+-        return response
++    ) -> CompletionResponse:
++        """Default streaming implementation built on ``iter_tokens``."""
++
++        iterator = self.iter_tokens(request)
++        completion: CompletionResponse | None = None
++        while True:
++            try:
++                token = next(iterator)
++            except StopIteration as stop:
++                completion = stop.value if isinstance(stop.value, CompletionResponse) else completion
++                break
++            else:
++                on_token(token)
++        return completion or CompletionResponse(text="")
+ 
+     def embed(self, request: EmbeddingRequest) -> EmbeddingResponse:  # pragma: no cover - optional override
+         raise NotImplementedError("Provider does not implement embeddings")
+diff --git a/src/fmf/exporters/__init__.py b/src/fmf/exporters/__init__.py
+index 9f730e3..ce56315 100644
+--- a/src/fmf/exporters/__init__.py
++++ b/src/fmf/exporters/__init__.py
+@@ -2,6 +2,7 @@ from __future__ import annotations
+ 
+ from typing import Any
+ 
++from ..core.interfaces import ExportSpec
+ from .base import Exporter, ExportError, ExportResult
+ 
+ 
+@@ -19,16 +20,22 @@ def build_exporter(cfg: Any) -> Exporter:
+     if etype == "s3":
+         from .s3 import S3Exporter
+ 
+-        return S3Exporter(
++        spec = ExportSpec(
+             name=name,
+-            bucket=_cfg_get(cfg, "bucket"),
+-            prefix=_cfg_get(cfg, "prefix"),
+-            format=_cfg_get(cfg, "format"),
+-            compression=_cfg_get(cfg, "compression"),
+-            partition_by=_cfg_get(cfg, "partition_by"),
+-            sse=_cfg_get(cfg, "sse"),
+-            kms_key_id=_cfg_get(cfg, "kms_key_id"),
++            type="s3",
++            format=_cfg_get(cfg, "format") or "jsonl",
++            write_mode=_cfg_get(cfg, "mode", "append"),
++            key_fields=_cfg_get(cfg, "key_fields"),
++            options={
++                "bucket": _cfg_get(cfg, "bucket"),
++                "prefix": _cfg_get(cfg, "prefix"),
++                "compression": _cfg_get(cfg, "compression"),
++                "partition_by": _cfg_get(cfg, "partition_by"),
++                "sse": _cfg_get(cfg, "sse"),
++                "kms_key_id": _cfg_get(cfg, "kms_key_id"),
++            },
+         )
++        return S3Exporter(spec=spec)
+     if etype == "dynamodb":
+         from .dynamodb import DynamoDBExporter
+ 
+@@ -79,4 +86,3 @@ def build_exporter(cfg: Any) -> Exporter:
+ 
+ 
+ __all__ = ["Exporter", "ExportError", "ExportResult", "build_exporter"]
+-
+diff --git a/src/fmf/exporters/base.py b/src/fmf/exporters/base.py
+index 1bd373a..f27b733 100644
+--- a/src/fmf/exporters/base.py
++++ b/src/fmf/exporters/base.py
+@@ -3,9 +3,7 @@ from __future__ import annotations
+ from dataclasses import dataclass
+ from typing import Any, Iterable, Literal, Protocol
+ 
+-
+-class ExportError(Exception):
+-    pass
++from ..core.errors import ExportError
+ 
+ 
+ @dataclass
+@@ -22,7 +20,7 @@ class Exporter(Protocol):
+         records: Iterable[dict[str, Any]] | bytes | str,
+         *,
+         schema: dict[str, Any] | None = None,
+-        mode: Literal["append", "upsert", "overwrite"] = "append",
++        mode: Literal["append", "upsert", "overwrite"] | None = None,
+         key_fields: list[str] | None = None,
+         context: dict[str, Any] | None = None,
+     ) -> ExportResult:
+@@ -33,4 +31,3 @@ class Exporter(Protocol):
+ 
+ 
+ __all__ = ["Exporter", "ExportResult", "ExportError"]
+-
+diff --git a/src/fmf/exporters/s3.py b/src/fmf/exporters/s3.py
+index 1f10322..812df91 100644
+--- a/src/fmf/exporters/s3.py
++++ b/src/fmf/exporters/s3.py
+@@ -8,34 +8,62 @@ import os
+ import uuid
+ from typing import Any, Iterable, List, Dict
+ 
++from ..core.ids import utc_now_iso
++from ..core.interfaces import ExportSpec
+ from .base import ExportError, ExportResult
+ 
+ 
+ def _now_date():
+-    return dt.datetime.utcnow().strftime("%Y-%m-%d")
++    return dt.datetime.now(dt.timezone.utc).strftime("%Y-%m-%d")
+ 
+ 
+ class S3Exporter:
+     def __init__(
+         self,
+         *,
+-        name: str,
+-        bucket: str,
++        spec: ExportSpec | None = None,
++        name: str | None = None,
++        bucket: str | None = None,
+         prefix: str | None = None,
+-        format: str | None = "jsonl",
++        format: str | None = None,
+         compression: str | None = None,
+         partition_by: list[str] | None = None,
+         sse: str | None = None,
+         kms_key_id: str | None = None,
++        mode: str | None = None,
+     ) -> None:
+-        self.name = name
+-        self.bucket = bucket
+-        self.prefix = prefix or ""
+-        self.format = (format or "jsonl").lower()
+-        self.compression = (compression or "none").lower()
+-        self.partition_by = partition_by or []
+-        self.sse = sse
+-        self.kms_key_id = kms_key_id
++        if spec is None:
++            if name is None:
++                name = "s3"
++            options = {
++                "bucket": bucket,
++                "prefix": prefix,
++                "compression": compression,
++                "partition_by": partition_by,
++                "sse": sse,
++                "kms_key_id": kms_key_id,
++            }
++            spec = ExportSpec(
++                name=name,
++                type="s3",
++                format=format or "jsonl",
++                write_mode=mode or "append",
++                options=options,
++            )
++        self.spec = spec
++        options = spec.options
++        self.name = spec.name
++        self.bucket = options.get("bucket") or bucket
++        if not self.bucket:
++            raise ExportError("S3 exporter requires a 'bucket' option")
++        self.prefix = (options.get("prefix") or prefix or "").lstrip("/")
++        self.format = (spec.format or format or "jsonl").lower()
++        self.compression = (options.get("compression") or compression or "none").lower()
++        self.partition_by = options.get("partition_by") or partition_by or []
++        self.sse = options.get("sse") or sse
++        self.kms_key_id = options.get("kms_key_id") or kms_key_id
++        self.write_mode = (spec.write_mode or mode or "append").lower()
++        self.key_fields = spec.key_fields or []
+         self._client = None
+ 
+     def _s3(self):
+@@ -48,25 +76,35 @@ class S3Exporter:
+         self._client = boto3.client("s3")
+         return self._client
+ 
+-    def _build_key(self, *, context: dict[str, Any] | None) -> str:
+-        run_id = (context or {}).get("run_id") or dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
+-        prefix = (self.prefix or "").replace("${run_id}", run_id)
+-        parts = [prefix.rstrip("/")]
++    def _ext(self) -> str:
++        mapping = {
++            "jsonl": ".jsonl",
++            "csv": ".csv",
++            "parquet": ".parquet",
++        }
++        ext = mapping.get(self.format, ".bin")
++        if self.compression == "gzip":
++            ext += ".gz"
++        return ext
++
++    def _build_key(self, *, context: dict[str, Any] | None, final: bool = True) -> str:
++        run_id = (context or {}).get("run_id") or utc_now_iso().replace(":", "").replace("-", "")
++        prefix = (self.prefix or "").replace("${run_id}", run_id).strip("/")
++        parts = [prefix] if prefix else []
+         if "date" in (self.partition_by or []):
+             parts.append(f"date={_now_date()}")
+-        # unique object name for append semantics
+-        if self.format == "jsonl":
+-            ext = ".jsonl"
+-        elif self.format == "csv":
+-            ext = ".csv"
+-        elif self.format == "parquet":
+-            ext = ".parquet"
++        base_name = context.get("filename") if isinstance(context, dict) else None  # type: ignore[arg-type]
++        if not base_name:
++            base_name = self.name or "export"
++        if self.write_mode == "overwrite" and final:
++            filename = f"{base_name}{self._ext()}"
+         else:
+-            ext = ".bin"
+-        if self.compression == "gzip":
+-            ext += ".gz"
+-        parts.append(f"part-{uuid.uuid4().hex}{ext}")
+-        return "/".join([p for p in parts if p])
++            filename = f"part-{uuid.uuid4().hex}{self._ext()}"
++        parts.append(filename)
++        key = "/".join([p for p in parts if p])
++        if not final:
++            key = f"{key}.tmp-{uuid.uuid4().hex}"
++        return key
+ 
+     def _ensure_records(self, recs: Iterable[dict[str, Any]] | bytes | str) -> List[Dict[str, Any]]:
+         if isinstance(recs, (bytes, str)):
+@@ -140,20 +178,49 @@ class S3Exporter:
+         records: Iterable[dict[str, Any]] | bytes | str,
+         *,
+         schema: dict[str, Any] | None = None,
+-        mode: str = "append",
++        mode: str | None = None,
+         key_fields: list[str] | None = None,
+         context: dict[str, Any] | None = None,
+     ) -> ExportResult:
+-        key = self._build_key(context=context)
++        active_mode = (mode or self.write_mode or "append").lower()
++        if active_mode == "upsert":  # TODO: implement S3 upsert semantics via merge manifest
++            raise ExportError("S3 upsert mode is not supported yet")
++
+         data = self._serialize(records)
+-        kwargs = {"Bucket": self.bucket, "Key": key, "Body": data}
++        client = self._s3()
++
++        if active_mode == "overwrite":
++            final_key = self._build_key(context=context, final=True)
++            temp_key = self._build_key(context=context, final=False)
++            put_kwargs = {"Bucket": self.bucket, "Key": temp_key, "Body": data}
++            if self.sse == "kms":
++                put_kwargs["ServerSideEncryption"] = "aws:kms"
++                if self.kms_key_id:
++                    put_kwargs["SSEKMSKeyId"] = self.kms_key_id
++            elif self.sse == "s3":
++                put_kwargs["ServerSideEncryption"] = "AES256"
++            client.put_object(**put_kwargs)
++            copy_source = {"Bucket": self.bucket, "Key": temp_key}
++            copy_kwargs = {"Bucket": self.bucket, "Key": final_key, "CopySource": copy_source}
++            if self.sse == "kms":
++                copy_kwargs["ServerSideEncryption"] = "aws:kms"
++                if self.kms_key_id:
++                    copy_kwargs["SSEKMSKeyId"] = self.kms_key_id
++            elif self.sse == "s3":
++                copy_kwargs["ServerSideEncryption"] = "AES256"
++            client.copy_object(**copy_kwargs)
++            client.delete_object(Bucket=self.bucket, Key=temp_key)
++            return ExportResult(count=-1, paths=[f"s3://{self.bucket}/{final_key}"])
++
++        key = self._build_key(context=context, final=True)
++        put_kwargs = {"Bucket": self.bucket, "Key": key, "Body": data}
+         if self.sse == "kms":
+-            kwargs["ServerSideEncryption"] = "aws:kms"
++            put_kwargs["ServerSideEncryption"] = "aws:kms"
+             if self.kms_key_id:
+-                kwargs["SSEKMSKeyId"] = self.kms_key_id
++                put_kwargs["SSEKMSKeyId"] = self.kms_key_id
+         elif self.sse == "s3":
+-            kwargs["ServerSideEncryption"] = "AES256"
+-        self._s3().put_object(**kwargs)
++            put_kwargs["ServerSideEncryption"] = "AES256"
++        client.put_object(**put_kwargs)
+         return ExportResult(count=-1, paths=[f"s3://{self.bucket}/{key}"])
+ 
+     def finalize(self) -> None:
+diff --git a/src/fmf/inference/azure_openai.py b/src/fmf/inference/azure_openai.py
+index 9987c02..1f6cd22 100644
+--- a/src/fmf/inference/azure_openai.py
++++ b/src/fmf/inference/azure_openai.py
+@@ -1,8 +1,10 @@
+ from __future__ import annotations
+ 
+-from typing import Callable, Optional
++import os
++from typing import Any, Callable, Iterable, Optional
+ 
+ from .base_client import Completion, InferenceError, LLMClient, Message, RateLimiter, with_retries
++from .registry import register_provider
+ from ..processing.chunking import estimate_tokens
+ 
+ 
+@@ -15,11 +17,13 @@ class AzureOpenAIClient:
+         deployment: str,
+         rate_per_sec: float = 5.0,
+         transport: Optional[Callable[[dict], dict]] = None,
++        stream_transport: Optional[Callable[[dict], Iterable[dict]]] = None,
+     ) -> None:
+         self.endpoint = endpoint
+         self.api_version = api_version
+         self.deployment = deployment
+         self._transport = transport
++        self._stream_transport = stream_transport
+         self._rl = RateLimiter(rate_per_sec)
+ 
+     def _default_transport(self, payload: dict) -> dict:  # pragma: no cover - requires network
+@@ -75,16 +79,7 @@ class AzureOpenAIClient:
+             "max_tokens": max_tokens,
+         }
+ 
+-        def _do():
+-            self._rl.wait()
+-            transport = self._transport or self._default_transport
+-            data = transport(payload)
+-            # Expected shape: {choices:[{message:{content:...}, finish_reason:...}], usage:{prompt_tokens, completion_tokens}, model:...}
+-            if stream and on_token:
+-                # Simulate streaming by tokenizing the content and invoking callback
+-                text = data.get("choices", [{}])[0].get("message", {}).get("content", "")
+-                for tok in text.split():
+-                    on_token(tok)
++        def _parse_response(data: dict) -> Completion:
+             choice = (data.get("choices") or [{}])[0]
+             msg = (choice.get("message") or {}).get("content", "")
+             finish = choice.get("finish_reason")
+@@ -97,7 +92,76 @@ class AzureOpenAIClient:
+                 ct = estimate_tokens(msg)
+             return Completion(text=msg, model=data.get("model"), stop_reason=finish, prompt_tokens=pt, completion_tokens=ct)
+ 
++        def _stream_enabled() -> bool:
++            value = os.getenv("FMF_EXPERIMENTAL_STREAMING", "")
++            return value.lower() in {"1", "true", "yes", "on"}
++
++        def _stream_payload() -> Optional[Completion]:
++            transport = self._stream_transport
++            if transport is None:
++                return None
++            chunks: list[str] = []
++            finish_reason: Optional[str] = None
++            usage: dict = {}
++            model_name: Optional[str] = None
++            for event in transport(payload):
++                if not isinstance(event, dict):
++                    continue
++                model_name = event.get("model") or model_name
++                usage = event.get("usage") or usage
++                for choice in event.get("choices", []):
++                    delta = choice.get("delta") or {}
++                    content = delta.get("content") or ""
++                    if content:
++                        chunks.append(content)
++                        if on_token is not None:
++                            on_token(content)
++                    finish_reason = choice.get("finish_reason") or finish_reason
++            if not chunks:
++                return None
++            text = "".join(chunks)
++            pt = usage.get("prompt_tokens") if isinstance(usage, dict) else None
++            ct = usage.get("completion_tokens") if isinstance(usage, dict) else None
++            if pt is None:
++                pt = sum(estimate_tokens(m.content) for m in messages)
++            if ct is None:
++                ct = estimate_tokens(text)
++            return Completion(
++                text=text,
++                model=model_name or self.deployment,
++                stop_reason=finish_reason,
++                prompt_tokens=pt,
++                completion_tokens=ct,
++            )
++
++        def _call_transport() -> dict:
++            transport = self._transport or self._default_transport
++            return transport(payload)
++
++        def _do():
++            self._rl.wait()
++            if stream and on_token is not None:
++                if _stream_enabled():
++                    streamed = _stream_payload()
++                    if streamed is not None:
++                        return streamed
++                data = _call_transport()
++                completion = _parse_response(data)
++                if completion.text:
++                    on_token(completion.text)
++                return completion
++            data = _call_transport()
++            return _parse_response(data)
++
+         return with_retries(_do)
+ 
+ 
++@register_provider("azure_openai")
++def _build_from_config(cfg: Any) -> AzureOpenAIClient:  # type: ignore[name-defined]
++    endpoint = getattr(cfg, "endpoint", None) if not isinstance(cfg, dict) else cfg.get("endpoint")
++    api_version = getattr(cfg, "api_version", None) if not isinstance(cfg, dict) else cfg.get("api_version")
++    deployment = getattr(cfg, "deployment", None) if not isinstance(cfg, dict) else cfg.get("deployment")
++    return AzureOpenAIClient(endpoint=endpoint, api_version=api_version, deployment=deployment)
++
++
+ __all__ = ["AzureOpenAIClient"]
+diff --git a/src/fmf/inference/base_client.py b/src/fmf/inference/base_client.py
+index a831536..54aae54 100644
+--- a/src/fmf/inference/base_client.py
++++ b/src/fmf/inference/base_client.py
+@@ -4,6 +4,8 @@ import time
+ from dataclasses import dataclass
+ from typing import Any, Callable, Iterable, Literal, Optional, Protocol
+ 
++from ..core.errors import InferenceError
++
+ 
+ Role = Literal["system", "user", "assistant", "tool"]
+ 
+@@ -23,16 +25,6 @@ class Completion:
+     stop_reason: Optional[str] = None
+     prompt_tokens: Optional[int] = None
+     completion_tokens: Optional[int] = None
+-
+-
+-class InferenceError(Exception):
+-    """Raised on provider errors; may wrap HTTP or SDK errors."""
+-
+-    def __init__(self, message: str, *, status_code: int | None = None):
+-        super().__init__(message)
+-        self.status_code = status_code
+-
+-
+ class LLMClient(Protocol):
+     def complete(
+         self,
+diff --git a/src/fmf/inference/bedrock.py b/src/fmf/inference/bedrock.py
+index 68c3b24..a5e275a 100644
+--- a/src/fmf/inference/bedrock.py
++++ b/src/fmf/inference/bedrock.py
+@@ -1,9 +1,11 @@
+ from __future__ import annotations
+ 
+ import json
+-from typing import Callable, Optional
++import os
++from typing import Any, Callable, Iterable, Optional
+ 
+ from .base_client import Completion, InferenceError, LLMClient, Message, RateLimiter, with_retries
++from .registry import register_provider
+ from ..processing.chunking import estimate_tokens
+ 
+ 
+@@ -15,10 +17,12 @@ class BedrockClient:
+         model_id: str,
+         rate_per_sec: float = 5.0,
+         transport: Optional[Callable[[dict], dict]] = None,
++        stream_transport: Optional[Callable[[dict], Iterable[dict]]] = None,
+     ) -> None:
+         self.region = region
+         self.model_id = model_id
+         self._transport = transport
++        self._stream_transport = stream_transport
+         self._rl = RateLimiter(rate_per_sec)
+ 
+     def _default_transport(self, payload: dict) -> dict:  # pragma: no cover - requires network
+@@ -97,15 +101,8 @@ class BedrockClient:
+             "system": sysmsg,
+         }
+ 
+-        def _do():
+-            self._rl.wait()
+-            transport = self._transport or self._default_transport
+-            data = transport(payload)
+-            # Expected generic structure: {output: {text: ...}, usage: {input_tokens, output_tokens}}
++        def _parse_response(data: dict) -> Completion:
+             text = data.get("output", {}).get("text") or data.get("content") or ""
+-            if stream and on_token:
+-                for tok in text.split():
+-                    on_token(tok)
+             usage = data.get("usage", {}) or {}
+             pt = usage.get("input_tokens")
+             ct = usage.get("output_tokens")
+@@ -115,10 +112,88 @@ class BedrockClient:
+                 ct = estimate_tokens(text)
+             return Completion(text=text, model=self.model_id, stop_reason=data.get("stop_reason"), prompt_tokens=pt, completion_tokens=ct)
+ 
++        def _stream_enabled() -> bool:
++            value = os.getenv("FMF_EXPERIMENTAL_STREAMING", "")
++            return value.lower() in {"1", "true", "yes", "on"}
++
++        def _stream_payload() -> Optional[Completion]:
++            transport = self._stream_transport
++            if transport is None:
++                return None
++            chunks: list[str] = []
++            usage: dict = {}
++            stop_reason: Optional[str] = None
++            for event in transport(payload):
++                if not isinstance(event, dict):
++                    continue
++                usage = event.get("usage") or usage
++                if "delta" in event:
++                    delta = event.get("delta") or {}
++                    content = delta.get("text")
++                    if content:
++                        chunks.append(content)
++                        if on_token is not None:
++                            on_token(content)
++                    stop_reason = delta.get("stop_reason") or stop_reason
++                elif "chunk" in event:
++                    content = event.get("chunk")
++                    if content:
++                        chunks.append(str(content))
++                        if on_token is not None:
++                            on_token(str(content))
++                elif "content" in event:
++                    content = event.get("content")
++                    if isinstance(content, str):
++                        chunks.append(content)
++                        if on_token is not None:
++                            on_token(content)
++            if not chunks:
++                return None
++            text = "".join(chunks)
++            pt = usage.get("input_tokens") if isinstance(usage, dict) else None
++            ct = usage.get("output_tokens") if isinstance(usage, dict) else None
++            if pt is None:
++                pt = sum(estimate_tokens(m.content) for m in messages)
++            if ct is None:
++                ct = estimate_tokens(text)
++            return Completion(
++                text=text,
++                model=self.model_id,
++                stop_reason=stop_reason,
++                prompt_tokens=pt,
++                completion_tokens=ct,
++            )
++
++        def _call_transport() -> dict:
++            transport = self._transport or self._default_transport
++            return transport(payload)
++
++        def _do():
++            self._rl.wait()
++            if stream and on_token is not None:
++                if _stream_enabled():
++                    streamed = _stream_payload()
++                    if streamed is not None:
++                        return streamed
++                data = _call_transport()
++                completion = _parse_response(data)
++                if completion.text:
++                    on_token(completion.text)
++                return completion
++            data = _call_transport()
++            return _parse_response(data)
++
+         try:
+             return with_retries(_do)
+         except InferenceError as e:
+             raise InferenceError(f"Bedrock error: {e}", status_code=e.status_code)
+ 
+ 
++@register_provider("aws_bedrock")
++def _build_from_config(cfg: Any) -> BedrockClient:  # type: ignore[name-defined]
++    region = getattr(cfg, "region", None) if not isinstance(cfg, dict) else cfg.get("region")
++    model_id = getattr(cfg, "model_id", None) if not isinstance(cfg, dict) else cfg.get("model_id")
++    return BedrockClient(region=region, model_id=model_id)
++
++
+ __all__ = ["BedrockClient"]
+diff --git a/src/fmf/inference/providers/template_provider/__init__.py b/src/fmf/inference/providers/template_provider/__init__.py
+index d1bd799..03fa0d8 100644
+--- a/src/fmf/inference/providers/template_provider/__init__.py
++++ b/src/fmf/inference/providers/template_provider/__init__.py
+@@ -1,5 +1,16 @@
+ """Reference provider template for new inference integrations."""
+ 
++from typing import Any
++
++from ....core.interfaces import ModelSpec
++from ...registry import register_provider
+ from .provider import TemplateProvider
+ 
++
++@register_provider("template")
++def _build_template(_cfg: Any) -> TemplateProvider:
++    spec = ModelSpec(provider="template", model="debug", modality="text")
++    return TemplateProvider(spec)
++
++
+ __all__ = ["TemplateProvider"]
+diff --git a/src/fmf/inference/registry.py b/src/fmf/inference/registry.py
+new file mode 100644
+index 0000000..abd66cc
+--- /dev/null
++++ b/src/fmf/inference/registry.py
+@@ -0,0 +1,32 @@
++from __future__ import annotations
++
++from typing import Any, Callable, Dict
++
++ProviderFactory = Callable[[Any], Any]
++
++_REGISTRY: Dict[str, ProviderFactory] = {}
++
++
++def register_provider(name: str) -> Callable[[ProviderFactory], ProviderFactory]:
++    name = name.lower()
++
++    def decorator(func: ProviderFactory) -> ProviderFactory:
++        _REGISTRY[name] = func
++        return func
++
++    return decorator
++
++
++def build_provider(name: str, cfg: Any) -> Any:
++    try:
++        factory = _REGISTRY[name.lower()]
++    except KeyError as exc:
++        raise ValueError(f"Provider '{name}' is not registered") from exc
++    return factory(cfg)
++
++
++def available_providers() -> list[str]:
++    return sorted(_REGISTRY.keys())
++
++
++__all__ = ["register_provider", "build_provider", "available_providers"]
+diff --git a/src/fmf/inference/unified.py b/src/fmf/inference/unified.py
+index fe02feb..0eb9c5e 100644
+--- a/src/fmf/inference/unified.py
++++ b/src/fmf/inference/unified.py
+@@ -2,25 +2,38 @@ from __future__ import annotations
+ 
+ from typing import Any
+ 
++from .registry import build_provider, available_providers
+ from .azure_openai import AzureOpenAIClient
+ from .bedrock import BedrockClient
+ 
+ 
++def _subconfig(cfg: Any, key: str) -> Any:
++    if isinstance(cfg, dict):
++        return cfg.get(key)
++    return getattr(cfg, key, None)
++
++
+ def build_llm_client(cfg: Any):
+     provider = getattr(cfg, "provider", None) if not isinstance(cfg, dict) else cfg.get("provider")
++    if provider is None:
++        raise ValueError("Inference provider not specified in configuration")
++
++    subcfg = _subconfig(cfg, provider)
++    try:
++        return build_provider(provider, subcfg)
++    except ValueError:
++        pass
++
+     if provider == "azure_openai":
+-        acfg = getattr(cfg, "azure_openai", None) if not isinstance(cfg, dict) else cfg.get("azure_openai")
+-        endpoint = getattr(acfg, "endpoint", None) if not isinstance(acfg, dict) else acfg.get("endpoint")
+-        api_version = getattr(acfg, "api_version", None) if not isinstance(acfg, dict) else acfg.get("api_version")
+-        deployment = getattr(acfg, "deployment", None) if not isinstance(acfg, dict) else acfg.get("deployment")
++        endpoint = getattr(subcfg, "endpoint", None) if not isinstance(subcfg, dict) else subcfg.get("endpoint")
++        api_version = getattr(subcfg, "api_version", None) if not isinstance(subcfg, dict) else subcfg.get("api_version")
++        deployment = getattr(subcfg, "deployment", None) if not isinstance(subcfg, dict) else subcfg.get("deployment")
+         return AzureOpenAIClient(endpoint=endpoint, api_version=api_version, deployment=deployment)
+     if provider == "aws_bedrock":
+-        bcfg = getattr(cfg, "aws_bedrock", None) if not isinstance(cfg, dict) else cfg.get("aws_bedrock")
+-        region = getattr(bcfg, "region", None) if not isinstance(bcfg, dict) else bcfg.get("region")
+-        model_id = getattr(bcfg, "model_id", None) if not isinstance(bcfg, dict) else bcfg.get("model_id")
++        region = getattr(subcfg, "region", None) if not isinstance(subcfg, dict) else subcfg.get("region")
++        model_id = getattr(subcfg, "model_id", None) if not isinstance(subcfg, dict) else subcfg.get("model_id")
+         return BedrockClient(region=region, model_id=model_id)
+-    raise ValueError(f"Unsupported inference provider: {provider}")
++    raise ValueError(f"Unsupported inference provider: {provider}. Known providers: {', '.join(available_providers())}")
+ 
+ 
+ __all__ = ["build_llm_client"]
+-
+diff --git a/src/fmf/observability/tracing.py b/src/fmf/observability/tracing.py
+index 331c48d..11e5024 100644
+--- a/src/fmf/observability/tracing.py
++++ b/src/fmf/observability/tracing.py
+@@ -1,11 +1,15 @@
+ from __future__ import annotations
+ 
++import os
+ from contextlib import contextmanager
+ 
+ 
+ @contextmanager
+ def trace_span(name: str):
+     """Start a tracing span if OpenTelemetry is available; otherwise no-op."""
++    if os.getenv("FMF_OBSERVABILITY_OTEL", "false").lower() not in {"1", "true", "yes", "on"}:
++        yield
++        return
+     try:
+         from opentelemetry import trace  # type: ignore
+ 
+@@ -13,9 +17,7 @@ def trace_span(name: str):
+         with tracer.start_as_current_span(name):
+             yield
+     except Exception:
+-        # No OpenTelemetry installed or other failure; silently continue
+         yield
+ 
+ 
+ __all__ = ["trace_span"]
+-
+diff --git a/src/fmf/processing/chunking.py b/src/fmf/processing/chunking.py
+index 08a8338..03e5875 100644
+--- a/src/fmf/processing/chunking.py
++++ b/src/fmf/processing/chunking.py
+@@ -3,6 +3,7 @@ from __future__ import annotations
+ import re
+ from typing import List
+ 
++from ..core.ids import chunk_id as compute_chunk_id
+ from ..types import Chunk
+ 
+ 
+@@ -46,7 +47,16 @@ def chunk_text(
+         u_tokens = estimate_tokens(u)
+         if cur_tokens + u_tokens > max_tokens and cur_parts:
+             chunk_text_val = " ".join(cur_parts).strip()
+-            chunks.append(Chunk(id=f"{doc_id}_ch{cid}", doc_id=doc_id, text=chunk_text_val, tokens_estimate=estimate_tokens(chunk_text_val)))
++            chunk_identifier = compute_chunk_id(document_id=doc_id, index=cid, payload=chunk_text_val)
++            chunks.append(
++                Chunk(
++                    id=chunk_identifier,
++                    doc_id=doc_id,
++                    text=chunk_text_val,
++                    tokens_estimate=estimate_tokens(chunk_text_val),
++                    provenance={"index": cid, "splitter": splitter, "length_chars": len(chunk_text_val)},
++                )
++            )
+             cid += 1
+             # start new chunk with overlap from end of previous
+             if overlap > 0 and chunks[-1].text:
+@@ -63,10 +73,18 @@ def chunk_text(
+ 
+     if cur_parts:
+         chunk_text_val = " ".join(cur_parts).strip()
+-        chunks.append(Chunk(id=f"{doc_id}_ch{cid}", doc_id=doc_id, text=chunk_text_val, tokens_estimate=estimate_tokens(chunk_text_val)))
++        chunk_identifier = compute_chunk_id(document_id=doc_id, index=cid, payload=chunk_text_val)
++        chunks.append(
++            Chunk(
++                id=chunk_identifier,
++                doc_id=doc_id,
++                text=chunk_text_val,
++                tokens_estimate=estimate_tokens(chunk_text_val),
++                provenance={"index": cid, "splitter": splitter, "length_chars": len(chunk_text_val)},
++            )
++        )
+ 
+     return chunks
+ 
+ 
+ __all__ = ["chunk_text", "estimate_tokens"]
+-
+diff --git a/src/fmf/processing/errors.py b/src/fmf/processing/errors.py
+index f832fb7..2d5d587 100644
+--- a/src/fmf/processing/errors.py
++++ b/src/fmf/processing/errors.py
+@@ -1,6 +1,4 @@
+-class ProcessingError(Exception):
+-    """Raised when processing fails (unsupported format, parse error, missing deps)."""
++from ..core.errors import ProcessingError
+ 
+ 
+ __all__ = ["ProcessingError"]
+-
+diff --git a/src/fmf/processing/loaders.py b/src/fmf/processing/loaders.py
+index 5087d7e..f74c5fa 100644
+--- a/src/fmf/processing/loaders.py
++++ b/src/fmf/processing/loaders.py
+@@ -5,6 +5,7 @@ import io
+ import os
+ from typing import Any, Iterable, List, Optional, Tuple
+ 
++from ..core.ids import blob_id as compute_blob_id, document_id as compute_document_id, utc_now_iso
+ from ..types import Blob, Document
+ from .errors import ProcessingError
+ from .text import html_to_text, normalize_text
+@@ -160,8 +161,22 @@ def load_document_from_bytes(
+         blobs = [blob]
+         text_out = None
+ 
+-    return Document(id=os.path.basename(filename), source_uri=source_uri, text=text_out, blobs=blobs, metadata=meta)
++    payload = data if text_out is None else text_out.encode("utf-8")
++    doc_id = compute_document_id(source_uri=source_uri, payload=payload or b"")
++    provenance = {
++        "source_uri": source_uri,
++        "root_filename": os.path.basename(filename),
++        "hash": doc_id.split("_", 1)[-1],
++        "created_at": utc_now_iso(),
++    }
++    if blobs:
++        managed: List[Blob] = []
++        for blob in blobs:
++            data_bytes = blob.data or b""
++            managed.append(blob.with_id(compute_blob_id(document_id=doc_id, media_type=blob.media_type, payload=data_bytes)))
++        blobs = managed
+ 
++    return Document(id=doc_id, source_uri=source_uri, text=text_out, blobs=blobs, metadata=meta, provenance=provenance)
+ 
+-__all__ = ["detect_type", "load_document_from_bytes"]
+ 
++__all__ = ["detect_type", "load_document_from_bytes"]
+diff --git a/src/fmf/types.py b/src/fmf/types.py
+index ec5cab6..04e2496 100644
+--- a/src/fmf/types.py
++++ b/src/fmf/types.py
+@@ -3,7 +3,7 @@ from __future__ import annotations
+ import base64
+ import hashlib
+ import uuid
+-from dataclasses import dataclass, field, asdict
++from dataclasses import dataclass, field
+ from typing import Any, Dict, List, Optional
+ 
+ 
+@@ -18,6 +18,10 @@ class Blob:
+     data: Optional[bytes] = None
+     metadata: Dict[str, Any] = field(default_factory=dict)
+ 
++    def with_id(self, new_id: str) -> "Blob":
++        self.id = new_id
++        return self
++
+     def to_serializable(self) -> Dict[str, Any]:
+         d = dict(id=self.id, media_type=self.media_type, metadata=self.metadata)
+         if self.data is not None:
+@@ -34,6 +38,7 @@ class Document:
+     text: Optional[str] = None
+     blobs: Optional[List[Blob]] = None
+     metadata: Dict[str, Any] = field(default_factory=dict)
++    provenance: Dict[str, Any] = field(default_factory=dict)
+ 
+     def to_serializable(self) -> Dict[str, Any]:
+         return {
+@@ -42,6 +47,7 @@ class Document:
+             "text": self.text,
+             "blobs": [b.to_serializable() for b in (self.blobs or [])],
+             "metadata": self.metadata,
++            "provenance": self.provenance,
+         }
+ 
+ 
+@@ -52,6 +58,7 @@ class Chunk:
+     text: str
+     tokens_estimate: int
+     metadata: Dict[str, Any] = field(default_factory=dict)
++    provenance: Dict[str, Any] = field(default_factory=dict)
+ 
+     def to_serializable(self) -> Dict[str, Any]:
+         return {
+@@ -60,8 +67,8 @@ class Chunk:
+             "text": self.text,
+             "tokens_estimate": self.tokens_estimate,
+             "metadata": self.metadata,
++            "provenance": self.provenance,
+         }
+ 
+ 
+ __all__ = ["Blob", "Document", "Chunk"]
+-
+diff --git a/tests/test_cli_keys.py b/tests/test_cli_keys.py
+index aaacbb9..578f294 100644
+--- a/tests/test_cli_keys.py
++++ b/tests/test_cli_keys.py
+@@ -49,6 +49,8 @@ class TestCliKeys(unittest.TestCase):
+         self.assertIn("API_KEY", out)
+         self.assertIn("****", out)
+         self.assertNotIn("sekrit", out)
++        self.assertIn("Secrets:", out)
++        self.assertIn("Diagnostics:", out)
+ 
+     def test_keys_test_needs_names_without_mapping(self):
+         from fmf.cli import main
+@@ -71,7 +73,41 @@ class TestCliKeys(unittest.TestCase):
+         self.assertEqual(rc, 2)
+         self.assertIn("No secret names provided", buf.getvalue())
+ 
++    def test_keys_diagnostics_detects_missing_fields(self):
++        from fmf.cli import main
++
++        yaml_path = self._write_yaml(
++            """
++            project: fmf
++            auth: { provider: env }
++            connectors:
++              - name: s3_raw
++                type: s3
++            inference:
++              provider: aws_bedrock
++              aws_bedrock: { region: us-east-1 }
++            export:
++              sinks:
++                - name: ddb
++                  type: dynamodb
++            """
++        )
++        os.environ["DUMMY"] = "value"
++
++        buf = io.StringIO()
++        sys_stdout = sys.stdout
++        try:
++            sys.stdout = buf
++            rc = main(["keys", "test", "-c", yaml_path, "DUMMY"])
++        finally:
++            sys.stdout = sys_stdout
++
++        self.assertEqual(rc, 0)
++        out = buf.getvalue()
++        self.assertIn("Connector", out)
++        self.assertIn("WARN", out)
++        self.assertIn("missing fields", out)
++
+ 
+ if __name__ == "__main__":
+     unittest.main()
+-
+diff --git a/tests/test_connectors_hardening.py b/tests/test_connectors_hardening.py
+new file mode 100644
+index 0000000..8d93473
+--- /dev/null
++++ b/tests/test_connectors_hardening.py
+@@ -0,0 +1,74 @@
++from __future__ import annotations
++
++import os
++import sys
++import types
++import unittest
++
++
++class TestConnectorHardening(unittest.TestCase):
++    def setUp(self) -> None:
++        repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
++        src_path = os.path.join(repo_root, "src")
++        if src_path not in sys.path:
++            sys.path.insert(0, src_path)
++
++    def test_local_connector_uses_run_context(self) -> None:
++        from fmf.connectors.local import LocalConnector
++        from fmf.core.interfaces import RunContext
++
++        ctx = RunContext(run_id="test-run")
++        conn = LocalConnector(name="local", root=".")
++        # Should iterate without raising even when selector provided via context (unused)
++        refs = list(conn.list(selector=["**/*.py"], context=ctx))
++        self.assertIsInstance(refs, list)
++
++    def test_s3_open_wraps_body_with_context_manager(self) -> None:
++        from fmf.connectors.s3 import S3Connector
++
++        class DummyBody:
++            def __init__(self) -> None:
++                self.closed = False
++
++            def read(self, *_, **__):
++                return b"data"
++
++            def close(self):
++                self.closed = True
++
++        class DummyClient:
++            def __init__(self) -> None:
++                self.calls = {"list": 0, "get": 0, "head": 0}
++
++            def list_objects_v2(self, **kwargs):
++                self.calls["list"] += 1
++                return {"Contents": [{"Key": "foo.txt"}]}
++
++            def get_object(self, **kwargs):
++                self.calls["get"] += 1
++                return {"Body": DummyBody()}
++
++            def head_object(self, **kwargs):
++                self.calls["head"] += 1
++                return {"ContentLength": 4, "LastModified": None, "ETag": "etag"}
++
++        # Stub boto3 client
++        module = types.SimpleNamespace(client=lambda service, region_name=None: DummyClient())
++        original_boto3 = sys.modules.get("boto3")
++        sys.modules["boto3"] = module  # type: ignore
++        try:
++            conn = S3Connector(name="s3", bucket="b")
++            ref = next(iter(conn.list()))
++            stream = conn.open(ref)
++            with stream as handle:
++                self.assertEqual(handle.read(), b"data")
++            self.assertTrue(stream._body.closed)  # type: ignore[attr-defined]
++        finally:
++            if original_boto3 is None:
++                sys.modules.pop("boto3", None)
++            else:
++                sys.modules["boto3"] = original_boto3
++
++
++if __name__ == "__main__":
++        unittest.main()
+diff --git a/tests/test_export_write_modes.py b/tests/test_export_write_modes.py
+new file mode 100644
+index 0000000..6844edb
+--- /dev/null
++++ b/tests/test_export_write_modes.py
+@@ -0,0 +1,84 @@
++from __future__ import annotations
++
++import sys
++import types
++import unittest
++
++from fmf.core.interfaces import ExportSpec
++
++
++class TestExportWriteModes(unittest.TestCase):
++    def setUp(self) -> None:
++        self._orig_boto3 = sys.modules.get("boto3")
++
++    def tearDown(self) -> None:
++        if self._orig_boto3 is None:
++            sys.modules.pop("boto3", None)
++        else:
++            sys.modules["boto3"] = self._orig_boto3
++
++    def _install_stub(self) -> list[str]:
++        calls: list[str] = []
++
++        class DummyClient:
++            def put_object(self, **kwargs):
++                calls.append("put:" + kwargs["Key"])
++
++            def copy_object(self, **kwargs):
++                calls.append("copy:" + kwargs["Key"])
++
++            def delete_object(self, **kwargs):
++                calls.append("delete:" + kwargs["Key"])
++
++        sys.modules["boto3"] = types.SimpleNamespace(client=lambda *_args, **_kwargs: DummyClient())  # type: ignore
++        return calls
++
++    def test_append_mode_generates_unique_keys(self) -> None:
++        calls = self._install_stub()
++        from fmf.exporters.s3 import S3Exporter
++
++        spec = ExportSpec(
++            name="s3",
++            type="s3",
++            format="jsonl",
++            write_mode="append",
++            options={"bucket": "demo", "prefix": "runs/"},
++        )
++        exporter = S3Exporter(spec=spec)
++        exporter.write([{"a": 1}], context={"run_id": "r1"})
++        self.assertTrue(any(call.startswith("put:runs/") for call in calls))
++
++    def test_overwrite_mode_uses_copy(self) -> None:
++        calls = self._install_stub()
++        from fmf.exporters.s3 import S3Exporter
++
++        spec = ExportSpec(
++            name="s3",
++            type="s3",
++            format="jsonl",
++            write_mode="overwrite",
++            options={"bucket": "demo", "prefix": "runs/"},
++        )
++        exporter = S3Exporter(spec=spec)
++        exporter.write([{"a": 1}], context={"run_id": "r1", "filename": "results"})
++        self.assertTrue(any(call.startswith("copy:") for call in calls))
++        self.assertTrue(any(call.startswith("delete:") for call in calls))
++
++    def test_upsert_not_supported(self) -> None:
++        self._install_stub()
++        from fmf.exporters.s3 import S3Exporter
++
++        spec = ExportSpec(
++            name="s3",
++            type="s3",
++            format="jsonl",
++            write_mode="upsert",
++            options={"bucket": "demo"},
++        )
++        exporter = S3Exporter(spec=spec)
++        with self.assertRaises(Exception):
++            exporter.write([{"a": 1}])
++
++
++if __name__ == "__main__":
++    unittest.main()
+diff --git a/tests/test_ids_provenance.py b/tests/test_ids_provenance.py
+new file mode 100644
+index 0000000..7cf3551
+--- /dev/null
++++ b/tests/test_ids_provenance.py
+@@ -0,0 +1,45 @@
++from __future__ import annotations
++
++import os
++import sys
++import unittest
++
++
++class TestIdsAndProvenance(unittest.TestCase):
++    def setUp(self) -> None:
++        repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
++        src_path = os.path.join(repo_root, "src")
++        if src_path not in sys.path:
++            sys.path.insert(0, src_path)
++
++    def test_document_id_stable(self) -> None:
++        from fmf.processing.loaders import load_document_from_bytes
++
++        payload = b"hello world"
++        doc1 = load_document_from_bytes(
++            source_uri="file:///tmp/doc.txt",
++            filename="doc.txt",
++            data=payload,
++            processing_cfg={},
++        )
++        doc2 = load_document_from_bytes(
++            source_uri="file:///tmp/doc.txt",
++            filename="doc.txt",
++            data=payload,
++            processing_cfg={},
++        )
++        self.assertEqual(doc1.id, doc2.id)
++        self.assertIn("created_at", doc1.provenance)
++        self.assertEqual(doc1.provenance["source_uri"], "file:///tmp/doc.txt")
++
++    def test_chunk_id_stable(self) -> None:
++        from fmf.processing.chunking import chunk_text
++
++        chunks_a = chunk_text(doc_id="doc_abcd", text="One. Two. Three.")
++        chunks_b = chunk_text(doc_id="doc_abcd", text="One. Two. Three.")
++        self.assertEqual([c.id for c in chunks_a], [c.id for c in chunks_b])
++        self.assertTrue(all("index" in c.provenance for c in chunks_a))
++
++
++if __name__ == "__main__":
++    unittest.main()
+diff --git a/tests/test_inference_azure.py b/tests/test_inference_azure.py
+index 579cf99..fcd8bb2 100644
+--- a/tests/test_inference_azure.py
++++ b/tests/test_inference_azure.py
+@@ -14,6 +14,8 @@ class TestAzureOpenAIClient(unittest.TestCase):
+         from fmf.inference.azure_openai import AzureOpenAIClient
+         from fmf.inference.base_client import Message
+ 
++        os.environ["FMF_EXPERIMENTAL_STREAMING"] = "1"
++
+         def transport(payload):
+             assert "messages" in payload
+             return {
+@@ -27,16 +29,36 @@ class TestAzureOpenAIClient(unittest.TestCase):
+                 "usage": {"prompt_tokens": 5, "completion_tokens": 2},
+             }
+ 
+-        client = AzureOpenAIClient(endpoint="https://example", api_version="2024-02-15-preview", deployment="x", transport=transport)
+-        toks = []
+-        comp = client.complete([Message(role="system", content="s"), Message(role="user", content="u")], temperature=0.2, max_tokens=10, stream=True, on_token=toks.append)
++        def stream_transport(payload):
++            yield {"choices": [{"delta": {"content": "Hello "}}]}
++            yield {
++                "choices": [{"delta": {"content": "world"}, "finish_reason": "stop"}],
++                "usage": {"prompt_tokens": 5, "completion_tokens": 2},
++                "model": "gpt-4o-mini",
++            }
++
++        client = AzureOpenAIClient(
++            endpoint="https://example",
++            api_version="2024-02-15-preview",
++            deployment="x",
++            transport=transport,
++            stream_transport=stream_transport,
++        )
++        toks: list[str] = []
++        comp = client.complete(
++            [Message(role="system", content="s"), Message(role="user", content="u")],
++            temperature=0.2,
++            max_tokens=10,
++            stream=True,
++            on_token=toks.append,
++        )
+         self.assertEqual(comp.text, "Hello world")
+         self.assertEqual(comp.model, "gpt-4o-mini")
+         self.assertEqual(comp.prompt_tokens, 5)
+         self.assertEqual(comp.completion_tokens, 2)
+-        self.assertGreater(len(toks), 1)
++        self.assertEqual(toks, ["Hello ", "world"])
++        os.environ.pop("FMF_EXPERIMENTAL_STREAMING", None)
+ 
+ 
+ if __name__ == "__main__":
+     unittest.main()
+-
+diff --git a/tests/test_provider_registry.py b/tests/test_provider_registry.py
+new file mode 100644
+index 0000000..961276c
+--- /dev/null
++++ b/tests/test_provider_registry.py
+@@ -0,0 +1,24 @@
++from __future__ import annotations
++
++import unittest
++
++
++class TestProviderRegistry(unittest.TestCase):
++    def test_registered_providers(self) -> None:
++        from fmf.inference import registry
++        from fmf.inference.azure_openai import AzureOpenAIClient
++
++        providers = registry.available_providers()
++        self.assertIn("azure_openai", providers)
++        client = registry.build_provider("azure_openai", {"endpoint": "https://unit", "api_version": "2024-02-15-preview", "deployment": "demo"})
++        self.assertIsInstance(client, AzureOpenAIClient)
++
++    def test_unknown_provider(self) -> None:
++        from fmf.inference import registry
++
++        with self.assertRaises(ValueError):
++            registry.build_provider("does-not-exist", {})
++
++
++if __name__ == "__main__":
++    unittest.main()
+diff --git a/tests/test_streaming_adapters.py b/tests/test_streaming_adapters.py
+new file mode 100644
+index 0000000..749192c
+--- /dev/null
++++ b/tests/test_streaming_adapters.py
+@@ -0,0 +1,66 @@
++from __future__ import annotations
++
++import os
++import unittest
++
++from fmf.inference.azure_openai import AzureOpenAIClient
++from fmf.inference.bedrock import BedrockClient
++from fmf.inference.base_client import Message
++
++
++class TestStreamingAdapters(unittest.TestCase):
++    def tearDown(self) -> None:
++        os.environ.pop("FMF_EXPERIMENTAL_STREAMING", None)
++
++    def test_azure_streaming_flag_enabled(self) -> None:
++        os.environ["FMF_EXPERIMENTAL_STREAMING"] = "true"
++
++        tokens: list[str] = []
++
++        def stream_transport(payload):
++            yield {"choices": [{"delta": {"content": "A"}}]}
++            yield {"choices": [{"delta": {"content": "B"}, "finish_reason": "stop"}]}
++
++        client = AzureOpenAIClient(
++            endpoint="https://example",
++            api_version="2024-02-15-preview",
++            deployment="demo",
++            transport=lambda payload: {
++                "choices": [{"message": {"content": "fallback"}, "finish_reason": "stop"}],
++                "usage": {},
++                "model": "demo",
++            },
++            stream_transport=stream_transport,
++        )
++        response = client.complete(
++            [Message(role="user", content="hi")],
++            stream=True,
++            on_token=tokens.append,
++        )
++        self.assertEqual(tokens, ["A", "B"])
++        self.assertEqual(response.text, "AB")
++
++    def test_bedrock_streaming_flag_disabled(self) -> None:
++        os.environ.pop("FMF_EXPERIMENTAL_STREAMING", None)
++        tokens: list[str] = []
++
++        client = BedrockClient(
++            region="us-east-1",
++            model_id="anthropic.test",
++            transport=lambda payload: {
++                "output": {"text": "complete"},
++                "usage": {"input_tokens": 2, "output_tokens": 1},
++            },
++            stream_transport=lambda payload: [{"content": "ignored"}],
++        )
++        response = client.complete(
++            [Message(role="user", content="hi")],
++            stream=True,
++            on_token=tokens.append,
++        )
++        self.assertEqual(tokens, ["complete"])
++        self.assertEqual(response.text, "complete")
++
++
++if __name__ == "__main__":
++    unittest.main()
diff --git a/docs/review/IMPLEMENTATION_NOTES.md b/docs/review/IMPLEMENTATION_NOTES.md
new file mode 100644
index 0000000..d1e40a3
--- /dev/null
+++ b/docs/review/IMPLEMENTATION_NOTES.md
@@ -0,0 +1,12 @@
+# Implementation Notes
+
+- Streaming support now honours `FMF_EXPERIMENTAL_STREAMING`; when enabled, Azure and Bedrock adapters consume chunk events and invoke `on_token` per provider delta. Without the flag, they still emit a single chunk (no space-splitting). New tests cover both paths.
+- Connectors inherit the shared `BaseConnector`, gaining optional `RunContext` plumbing and resilient retries. S3 uses exponential backoff with jitter and returns context-managed bodies to avoid leaking sockets. SharePoint retains its Graph client but now exposes selectors/exclude rules.
+- Deterministic IDs are derived from content hashes via `fmf.core.ids`. Documents and chunks record provenance metadata (`created_at`, chunk index, length) to aid audit trails.
+- YAML configuration gains `experimental` (streaming, OTEL), `processing.hash_algo`, and `retries.max_elapsed_s`
+  sections; `load_config` now mirrors these to environment variables so existing code paths remain compatible.
+- Provider registry (`fmf.inference.registry`) lets adapters self-register. `build_llm_client` first consults the registry, then falls back to legacy instantiation for compatibility.
+- S3 exporter respects `write_mode` (`append`/`overwrite`) and uses atomic copy-on-write semantics for overwrites. Upserts are surfaced as TODO via `ExportError`.
+- Error handling is centralised in `fmf.core.errors`, giving CLI/SDK a consistent base for mapping failures to exit codes.
+- Logging remains structured but honours `FMF_OBSERVABILITY_OTEL` before wiring OpenTelemetry spans. Timestamps are now timezone-aware.
+- `fmf keys test` prints a quick diagnostics report for connectors/providers/exporters, flagging missing configuration but avoiding destructive calls.
diff --git a/docs/review/VIABILITY.md b/docs/review/VIABILITY.md
new file mode 100644
index 0000000..3b7c1bf
--- /dev/null
+++ b/docs/review/VIABILITY.md
@@ -0,0 +1,14 @@
+# Viability Assessment – Follow-up Scope
+
+| Item | Current Context (files, APIs) | Feasibility | Risks / Unknowns | Est. Effort | Plan / De-risking |
+| --- | --- | --- | --- | --- | --- |
+| True streaming for providers | `src/fmf/inference/azure_openai.py`, `src/fmf/inference/bedrock.py`, `src/fmf/inference/base_client.py`, no iterable API yet, transport mocked via callables | Medium | SDK streaming support not available offline; need to simulate with mocked chunks without new deps; ensure env flag gating | M | Introduce `iter_tokens` in provider base using generator pattern; gate behind `FMF_EXPERIMENTAL_STREAMING`; add unit tests with fake streaming transports returning chunk lists |
+| Connector hardening | `src/fmf/connectors/local.py`, `s3.py`, `sharepoint.py` still using legacy protocol; no retries/backoff | Medium | SharePoint connector relies on msgraph SDK which may not be installed in tests; need to mock requests; ensure backward compatibility with existing config | M | Create `BaseConnector` subclasses, wrap reads with `contextlib.closing`, add retry helper (decorrelated jitter); implement tests using temp FS + moto stub (already optional) |
+| Deterministic IDs & provenance | `src/fmf/types.py`, `src/fmf/core/interfaces/models.py` currently random UUIDs; no hashing utilities | Medium | Hash algorithm availability (xxhash not installed); need fallback; propagate IDs without breaking existing artefacts | M | Add `core/ids.py` with blake2b default + optional xxhash import; update document/chunk creation flows; tests ensure stable IDs for same content |
+| Provider registry | Providers built via `inference/unified.py` factory with if/else; no registry | High | Minimal risk; ensure compatibility with existing config loader | S | Implement decorator-based registry (`inference/registry.py`), update `build_llm_client`; add unit tests ensuring fallback errors |
+| Export idempotency & write modes | Exporters (e.g., `exporters/s3.py`, `dynamodb.py`, `sharepoint_excel.py`) accept `mode` but inconsistent; no atomic writes | Medium | Atomic writes on S3 require temp keys or conditional PUT; limited to append/overwrite; integration tests need mocking | L | Extend `ExportSpec`; implement local temp file rename; for S3 use upload then copy; add tests using moto or local FS |
+| Error hierarchy | Multiple bespoke exceptions (ConnectorError, AuthError, etc.) | High | Need to avoid breaking except clauses elsewhere | S | Create `core/errors.py` deriving from `Exception`, update modules to alias or subclass existing errors; adjust tests |
+| CI & quality upgrades | `.github/workflows/ci.yml` single-job Py3.12 only; no lint/coverage | Medium | Installing ruff/mypy/coverage offline may fail; coverage threshold may fail initially | M | Extend workflow matrix (3.11/3.12), add lint steps, configure minimal `ruff.toml` & `mypy.ini`, measure coverage with modest threshold (e.g., 70%) |
+| Observability & time | `datetime.utcnow()` scattered (runner, cli, exporters); simple OTEL helper exists; logs need redaction | Medium | Need to ensure timezone change doesn’t break tests expecting string format; optional OTEL flag interacts with existing tracer stub | M | Introduce util `utc_now()` returning timezone-aware; update tests; add log redaction helper; gate OTEL spans behind env flag |
+| “keys test” diagnostics | CLI already has `keys test` basic redaction in `src/fmf/cli.py`; needs richer diagnostics | High | Need to inspect configs without hitting real services; ensure backwards compatibility with tests | S | Extend command to read active profile, simulate connector/provider/exporter checks using safe no-op operations; add tests verifying output |
+
diff --git a/examples/recipes/deterministic_ids_demo.py b/examples/recipes/deterministic_ids_demo.py
new file mode 100644
index 0000000..b326646
--- /dev/null
+++ b/examples/recipes/deterministic_ids_demo.py
@@ -0,0 +1,32 @@
+"""Show deterministic document and chunk identifiers derived from content hashes."""
+
+from __future__ import annotations
+
+from fmf.processing.loaders import load_document_from_bytes
+from fmf.processing.chunking import chunk_text
+
+
+def main() -> None:
+    payload = b"Paragraph one. Paragraph two."
+    doc1 = load_document_from_bytes(
+        source_uri="file:///demo.txt",
+        filename="demo.txt",
+        data=payload,
+        processing_cfg={"text": {"chunking": {"splitter": "by_sentence", "max_tokens": 20}}},
+    )
+    doc2 = load_document_from_bytes(
+        source_uri="file:///demo.txt",
+        filename="demo.txt",
+        data=payload,
+        processing_cfg={},
+    )
+    print("document IDs equal:", doc1.id == doc2.id)
+    print("provenance:", doc1.provenance)
+
+    chunks = chunk_text(doc_id=doc1.id, text=doc1.text or "", splitter="by_sentence")
+    for chunk in chunks:
+        print(f"chunk {chunk.provenance['index']}: id={chunk.id} length={chunk.provenance['length_chars']}")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/recipes/export_write_modes_demo.py b/examples/recipes/export_write_modes_demo.py
new file mode 100644
index 0000000..1d8e566
--- /dev/null
+++ b/examples/recipes/export_write_modes_demo.py
@@ -0,0 +1,51 @@
+"""Illustrate append vs overwrite semantics for the S3 exporter using a stubbed boto3 client."""
+
+from __future__ import annotations
+
+import sys
+import types
+
+from fmf.core.interfaces import ExportSpec
+from fmf.exporters.s3 import S3Exporter
+
+
+def install_stub() -> list[str]:
+    calls: list[str] = []
+
+    class DummyClient:
+        def put_object(self, **kwargs):
+            calls.append(f"put:{kwargs['Key']}")
+
+        def copy_object(self, **kwargs):
+            calls.append(f"copy:{kwargs['Key']}")
+
+        def delete_object(self, **kwargs):
+            calls.append(f"delete:{kwargs['Key']}")
+
+    sys.modules["boto3"] = types.SimpleNamespace(client=lambda *_args, **_kwargs: DummyClient())  # type: ignore
+    return calls
+
+
+def main() -> None:
+    original = sys.modules.get("boto3")
+    calls = install_stub()
+    try:
+        append_spec = ExportSpec(name="s3", type="s3", format="jsonl", write_mode="append", options={"bucket": "demo", "prefix": "runs/"})
+        append_exporter = S3Exporter(spec=append_spec)
+        append_exporter.write([{"row": 1}], context={"run_id": "r-001"})
+
+        overwrite_spec = ExportSpec(name="s3", type="s3", format="jsonl", write_mode="overwrite", options={"bucket": "demo", "prefix": "runs/"})
+        overwrite_exporter = S3Exporter(spec=overwrite_spec)
+        overwrite_exporter.write([{"row": 1}], context={"run_id": "r-001", "filename": "latest"})
+    finally:
+        if original is None:
+            sys.modules.pop("boto3", None)
+        else:
+            sys.modules["boto3"] = original
+
+    for call in calls:
+        print(call)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/recipes/streaming_text_demo.py b/examples/recipes/streaming_text_demo.py
new file mode 100644
index 0000000..49199ec
--- /dev/null
+++ b/examples/recipes/streaming_text_demo.py
@@ -0,0 +1,49 @@
+"""Demonstrate streaming token emission using the experimental flag.
+
+This recipe wires a stub transport into ``AzureOpenAIClient`` so it can run without
+external services. Set ``FMF_EXPERIMENTAL_STREAMING=1`` to enable chunked delivery.
+"""
+
+from __future__ import annotations
+
+import os
+
+from fmf.inference.azure_openai import AzureOpenAIClient
+from fmf.inference.base_client import Message
+
+
+def main() -> None:
+    os.environ.setdefault("FMF_EXPERIMENTAL_STREAMING", "1")
+
+    def transport(_payload):
+        return {
+            "choices": [
+                {"message": {"content": "fallback"}, "finish_reason": "stop"}
+            ],
+            "model": "demo",
+            "usage": {"prompt_tokens": 1, "completion_tokens": 1},
+        }
+
+    def stream_transport(_payload):
+        yield {"choices": [{"delta": {"content": "Hello "}}]}
+        yield {"choices": [{"delta": {"content": "world"}, "finish_reason": "stop"}]}
+
+    client = AzureOpenAIClient(
+        endpoint="https://example",
+        api_version="2024-02-15-preview",
+        deployment="demo",
+        transport=transport,
+        stream_transport=stream_transport,
+    )
+    tokens: list[str] = []
+    completion = client.complete(
+        [Message(role="user", content="Say hello")],
+        stream=True,
+        on_token=tokens.append,
+    )
+    print("streamed tokens:", tokens)
+    print("completion:", completion.text)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/mypy.ini b/mypy.ini
new file mode 100644
index 0000000..60d1f19
--- /dev/null
+++ b/mypy.ini
@@ -0,0 +1,5 @@
+[mypy]
+python_version = 3.11
+ignore_missing_imports = True
+show_error_codes = True
+files = src/fmf/core, src/fmf/inference
diff --git a/ruff.toml b/ruff.toml
new file mode 100644
index 0000000..0716f69
--- /dev/null
+++ b/ruff.toml
@@ -0,0 +1,8 @@
+[tool.ruff]
+line-length = 100
+target-version = "py311"
+select = ["E", "F", "W"]
+ignore = ["E501"]
+
+[tool.ruff.lint]
+preview = false
diff --git a/src/fmf/auth/providers.py b/src/fmf/auth/providers.py
index f5cd42c..0246aec 100644
--- a/src/fmf/auth/providers.py
+++ b/src/fmf/auth/providers.py
@@ -7,10 +7,7 @@ from typing import Dict, Iterable, Mapping, Protocol
 
 # Accept either Pydantic models or dict-like configs to avoid hard dependency at import time
 from ..config.models import AuthConfig  # type: ignore
-
-
-class AuthError(Exception):
-    """Raised when secret resolution fails or a provider is unavailable."""
+from ..core.errors import AuthError
 
 
 def _redact(_: str | None) -> str:
diff --git a/src/fmf/chain/runner.py b/src/fmf/chain/runner.py
index 8980590..99c00d0 100644
--- a/src/fmf/chain/runner.py
+++ b/src/fmf/chain/runner.py
@@ -10,7 +10,7 @@ from typing import Any, Dict, List, Tuple
 from ..connectors import build_connector
 from ..processing.loaders import load_document_from_bytes
 from ..processing.table_rows import iter_table_rows
-from ..processing.chunking import chunk_text
+from ..processing.chunking import chunk_text, estimate_tokens
 from ..types import Chunk, Document
 from ..processing.persist import persist_artefacts, ensure_dir
 from ..inference.unified import build_llm_client
@@ -21,6 +21,7 @@ from ..observability import metrics as _metrics
 from ..observability.tracing import trace_span
 from ..prompts.registry import build_prompt_registry
 from ..rag import build_rag_pipelines
+from ..core.ids import chunk_id as compute_chunk_id
 from .loader import ChainConfig, ChainStep, load_chain
 
 
@@ -387,7 +388,16 @@ def _run_chain_loaded(
                     # Ensure we still create a work item for multimodal steps even without text
                     from ..processing.chunking import estimate_tokens
 
-                    chunks.append(Chunk(id=f"{doc.id}_ch0", doc_id=doc.id, text="", tokens_estimate=estimate_tokens("") ))
+                    chunk_identifier = compute_chunk_id(document_id=doc.id, index=0, payload="")
+                    chunks.append(
+                        Chunk(
+                            id=chunk_identifier,
+                            doc_id=doc.id,
+                            text="",
+                            tokens_estimate=estimate_tokens(""),
+                            provenance={"index": 0, "splitter": "auto", "length_chars": 0},
+                        )
+                    )
 
         # Build image groups if requested
         if input_mode == "images_group" and image_docs:
@@ -664,7 +674,7 @@ def _run_chain_loaded(
             context_all[step.output] = results
 
     # Persist artefacts and write run.yaml
-    run_id = dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
+    run_id = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%dT%H%M%SZ")
     paths = persist_artefacts(artefacts_dir=artefacts_dir or "artefacts", run_id=run_id, documents=documents, chunks=chunks)
     run_dir = os.path.dirname(paths["docs"])
     # write outputs.jsonl for the last step by default
diff --git a/src/fmf/cli.py b/src/fmf/cli.py
index ae0cb02..11ccc6f 100644
--- a/src/fmf/cli.py
+++ b/src/fmf/cli.py
@@ -1,8 +1,9 @@
 from __future__ import annotations
 
 import argparse
+import json
 import sys
-from typing import List
+from typing import Any, List
 
 from .config.loader import load_config
 from .auth import build_provider, AuthError
@@ -18,6 +19,7 @@ from .inference.unified import build_llm_client
 from .inference.base_client import Message
 from .chain.runner import run_chain
 from .exporters import build_exporter
+from .core.errors import FmfError, get_exit_code
 from .sdk import FMF
 
 
@@ -60,6 +62,7 @@ def build_parser() -> argparse.ArgumentParser:
         default=[],
         help="Override config values: key.path=value (repeatable)",
     )
+    keys_test.add_argument("--json", action="store_true", help="Emit machine-readable JSON output")
     # connect subcommands
     connect = subparsers.add_parser("connect", help="List and interact with data connectors")
     connect_sub = connect.add_subparsers(dest="connect_cmd")
@@ -226,9 +229,103 @@ def _cmd_keys_test(args: argparse.Namespace) -> int:
         print(f"Secret resolution failed: {e}")
         return 1
 
+    secrets_output: list[dict[str, str]] = []
+    if not getattr(args, "json", False):
+        print("Secrets:")
     for n in names:
         status = "OK" if n in resolved else "MISSING"
-        print(f"{n}=**** {status}")
+        if getattr(args, "json", False):
+            secrets_output.append({"name": n, "status": status})
+        else:
+            print(f"{n}=**** {status}")
+
+    def _get(obj: Any, key: str, default: Any = None) -> Any:
+        if obj is None:
+            return default
+        if isinstance(obj, dict):
+            return obj.get(key, default)
+        return getattr(obj, key, default)
+
+    diagnostics: list[tuple[str, str, str, str]] = []
+
+    # Connectors diagnostics
+    connectors_cfg = getattr(cfg, "connectors", None) if not isinstance(cfg, dict) else cfg.get("connectors")
+    required_connectors = {
+        "local": ["root"],
+        "s3": ["bucket"],
+        "sharepoint": ["site_url", "drive"],
+    }
+    for c in connectors_cfg or []:
+        name = _get(c, "name", "unknown")
+        ctype = _get(c, "type", "unknown")
+        required = required_connectors.get(ctype, [])
+        missing = [field for field in required if not _get(c, field)]
+        if ctype not in required_connectors:
+            diagnostics.append(("Connector", name, "WARN", f"unknown type '{ctype}'"))
+        elif missing:
+            diagnostics.append(("Connector", name, "WARN", f"missing fields: {', '.join(missing)}"))
+        else:
+            diagnostics.append(("Connector", name, "OK", ""))
+
+    # Inference diagnostics
+    inference_cfg = getattr(cfg, "inference", None) if not isinstance(cfg, dict) else cfg.get("inference")
+    provider_name = _get(inference_cfg, "provider")
+    if provider_name:
+        from .inference.registry import available_providers
+
+        providers = available_providers()
+        if provider_name not in providers:
+            diagnostics.append(("Provider", provider_name, "FAIL", "not registered"))
+        else:
+            subcfg = _get(inference_cfg, provider_name)
+            required = {
+                "azure_openai": ["endpoint", "deployment"],
+                "aws_bedrock": ["region", "model_id"],
+            }.get(provider_name, [])
+            missing = [field for field in required if not _get(subcfg, field)]
+            if missing:
+                diagnostics.append(("Provider", provider_name, "WARN", f"missing fields: {', '.join(missing)}"))
+            else:
+                diagnostics.append(("Provider", provider_name, "OK", ""))
+    else:
+        diagnostics.append(("Provider", "<unset>", "WARN", "inference.provider not configured"))
+
+    # Exporter diagnostics
+    export_cfg = getattr(cfg, "export", None) if not isinstance(cfg, dict) else cfg.get("export")
+    sinks_cfg = _get(export_cfg, "sinks", [])
+    required_sinks = {
+        "s3": ["bucket"],
+        "dynamodb": ["table"],
+        "sharepoint_excel": ["site_url", "drive", "file_path"],
+    }
+    for sink in sinks_cfg or []:
+        name = _get(sink, "name", "unknown")
+        stype = _get(sink, "type", "unknown")
+        required = required_sinks.get(stype, [])
+        missing = [field for field in required if not _get(sink, field)]
+        if stype not in required_sinks:
+            diagnostics.append(("Exporter", name, "WARN", f"unknown type '{stype}'"))
+        elif missing:
+            diagnostics.append(("Exporter", name, "WARN", f"missing fields: {', '.join(missing)}"))
+        else:
+            diagnostics.append(("Exporter", name, "OK", ""))
+
+    if getattr(args, "json", False):
+        payload = {
+            "secrets": secrets_output,
+            "diagnostics": [
+                {"category": category.lower(), "name": name, "status": status, "detail": note}
+                for category, name, status, note in diagnostics
+            ],
+        }
+        print(json.dumps(payload, indent=2))
+    elif diagnostics:
+        print("Diagnostics:")
+        for category, name, status, note in diagnostics:
+            line = f"  {category:<10} {name:<20} {status}"
+            if note:
+                line += f" - {note}"
+            print(line)
 
     return 0
 
@@ -261,7 +358,7 @@ def _cmd_connect_ls(args: argparse.Namespace) -> int:
 
 
 def _gen_run_id() -> str:
-    ts = _dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
+    ts = _dt.datetime.now(_dt.timezone.utc).strftime("%Y%m%dT%H%M%SZ")
     rand = _uuid.uuid4().hex[:6]
     return f"{ts}-{rand}"
 
@@ -444,99 +541,105 @@ def _cmd_export(args: argparse.Namespace) -> int:
 
 def main(argv: list[str] | None = None) -> int:
     parser = build_parser()
-    args = parser.parse_args(argv)
-
-    if args.version:
-        # Defer importing package to avoid side-effects at import time
-        try:
-            import importlib.metadata as importlib_metadata  # py3.8+
-        except Exception:  # pragma: no cover - fallback unlikely needed on 3.12
-            import importlib_metadata  # type: ignore
-
-        try:
-            version = importlib_metadata.version("frontier-model-framework")
-        except importlib_metadata.PackageNotFoundError:
-            version = "0.0.0+local"
-        print(version)
-        return 0
-
-    # For now, show help when no subcommand is provided
-    if not getattr(args, "command", None):
-        parser.print_help()
-        return 0
-
-    if args.command == "keys" and getattr(args, "keys_cmd", None) == "test":
-        return _cmd_keys_test(args)
-    if args.command == "connect" and getattr(args, "connect_cmd", None) in {"ls", "list"}:
-        return _cmd_connect_ls(args)
-    if args.command == "process":
-        return _cmd_process(args)
-    if args.command == "infer":
-        return _cmd_infer(args)
-    if args.command == "run":
-        # Delegate directly to chain runner
-        overrides = getattr(args, "set_overrides", None)
-        res = run_chain(
-            args.chain,
-            fmf_config_path=args.config,
-            set_overrides=overrides or None,
-        )
-        print(f"run_id={res['run_id']}")
-        print(f"run_dir={res['run_dir']}")
-        return 0
-    if args.command == "export":
-        return _cmd_export(args)
-    # SDK wrappers
-    if args.command == "csv" and getattr(args, "csv_cmd", None) == "analyse":
-        f = FMF.from_env(args.config)
-        f.csv_analyse(
-            input=args.input,
-            text_col=args.text_col,
-            id_col=args.id_col,
-            prompt=args.prompt,
-            save_csv=args.save_csv,
-            save_jsonl=args.save_jsonl,
-        )
-        return 0
-    if args.command == "text" and getattr(args, "text_cmd", None) == "infer":
-        f = FMF.from_env(args.config)
-        f.text_files(prompt=args.prompt, select=args.select, save_jsonl=args.save_jsonl)
-        return 0
-    if args.command == "images" and getattr(args, "images_cmd", None) == "analyse":
-        f = FMF.from_env(args.config)
-        f.images_analyse(prompt=args.prompt, select=args.select, save_jsonl=args.save_jsonl)
-        return 0
-    if args.command == "recipe" and getattr(args, "recipe_cmd", None) == "run":
-        f = FMF.from_env(args.config)
-        f.run_recipe(args.file)
-        return 0
-    if args.command == "prompt" and getattr(args, "prompt_cmd", None) == "register":
-        from .prompts.registry import build_prompt_registry
-        cfg = load_config(args.config)
-        preg_cfg = getattr(cfg, "prompt_registry", None) if not isinstance(cfg, dict) else cfg.get("prompt_registry")
-        reg = build_prompt_registry(preg_cfg)
-        pv = reg.register(args.ref)
-        print(f"registered {pv.id}#{pv.version} hash={pv.content_hash}")
-        return 0
+    try:
+        args = parser.parse_args(argv)
+        if args.version:
+            # Defer importing package to avoid side-effects at import time
+            try:
+                import importlib.metadata as importlib_metadata  # py3.8+
+            except Exception:  # pragma: no cover - fallback unlikely needed on 3.12
+                import importlib_metadata  # type: ignore
 
-    if args.command == "doctor":
-        # Minimal diagnostics: report provider and first connector
-        cfg = load_config(getattr(args, "config", "fmf.yaml"))
-        prov = None
-        inference_cfg = getattr(cfg, "inference", None) if not isinstance(cfg, dict) else cfg.get("inference")
-        prov = getattr(inference_cfg, "provider", None) if not isinstance(inference_cfg, dict) else (inference_cfg or {}).get("provider")
-        connectors = getattr(cfg, "connectors", None) if not isinstance(cfg, dict) else cfg.get("connectors")
-        first_conn = None
-        if connectors:
-            c = connectors[0]
-            first_conn = (getattr(c, "name", None) if not isinstance(c, dict) else c.get("name"))
-        print(f"provider={prov}")
-        print(f"connector={first_conn}")
+            try:
+                version = importlib_metadata.version("frontier-model-framework")
+            except importlib_metadata.PackageNotFoundError:
+                version = "0.0.0+local"
+            print(version)
+            return 0
+
+        # For now, show help when no subcommand is provided
+        if not getattr(args, "command", None):
+            parser.print_help()
+            return 0
+
+        if args.command == "keys" and getattr(args, "keys_cmd", None) == "test":
+            return _cmd_keys_test(args)
+        if args.command == "connect" and getattr(args, "connect_cmd", None) in {"ls", "list"}:
+            return _cmd_connect_ls(args)
+        if args.command == "process":
+            return _cmd_process(args)
+        if args.command == "infer":
+            return _cmd_infer(args)
+        if args.command == "run":
+            # Delegate directly to chain runner
+            overrides = getattr(args, "set_overrides", None)
+            res = run_chain(
+                args.chain,
+                fmf_config_path=args.config,
+                set_overrides=overrides or None,
+            )
+            print(f"run_id={res['run_id']}")
+            print(f"run_dir={res['run_dir']}")
+            return 0
+        if args.command == "export":
+            return _cmd_export(args)
+        # SDK wrappers
+        if args.command == "csv" and getattr(args, "csv_cmd", None) == "analyse":
+            f = FMF.from_env(args.config)
+            f.csv_analyse(
+                input=args.input,
+                text_col=args.text_col,
+                id_col=args.id_col,
+                prompt=args.prompt,
+                save_csv=args.save_csv,
+                save_jsonl=args.save_jsonl,
+            )
+            return 0
+        if args.command == "text" and getattr(args, "text_cmd", None) == "infer":
+            f = FMF.from_env(args.config)
+            f.text_files(prompt=args.prompt, select=args.select, save_jsonl=args.save_jsonl)
+            return 0
+        if args.command == "images" and getattr(args, "images_cmd", None) == "analyse":
+            f = FMF.from_env(args.config)
+            f.images_analyse(prompt=args.prompt, select=args.select, save_jsonl=args.save_jsonl)
+            return 0
+        if args.command == "recipe" and getattr(args, "recipe_cmd", None) == "run":
+            f = FMF.from_env(args.config)
+            f.run_recipe(args.file)
+            return 0
+        if args.command == "prompt" and getattr(args, "prompt_cmd", None) == "register":
+            from .prompts.registry import build_prompt_registry
+            cfg = load_config(args.config)
+            preg_cfg = getattr(cfg, "prompt_registry", None) if not isinstance(cfg, dict) else cfg.get("prompt_registry")
+            reg = build_prompt_registry(preg_cfg)
+            pv = reg.register(args.ref)
+            print(f"registered {pv.id}#{pv.version} hash={pv.content_hash}")
+            return 0
+
+        if args.command == "doctor":
+            # Minimal diagnostics: report provider and first connector
+            cfg = load_config(getattr(args, "config", "fmf.yaml"))
+            prov = None
+            inference_cfg = getattr(cfg, "inference", None) if not isinstance(cfg, dict) else cfg.get("inference")
+            prov = getattr(inference_cfg, "provider", None) if not isinstance(inference_cfg, dict) else (inference_cfg or {}).get("provider")
+            connectors = getattr(cfg, "connectors", None) if not isinstance(cfg, dict) else cfg.get("connectors")
+            first_conn = None
+            if connectors:
+                c = connectors[0]
+                first_conn = (getattr(c, "name", None) if not isinstance(c, dict) else c.get("name"))
+            print(f"provider={prov}")
+            print(f"connector={first_conn}")
+            return 0
+
+        # Stub handlers: print a friendly message for unimplemented commands
+        print(f"Command '{args.command}' is not implemented yet.")
         return 0
-
-    # Stub handlers: print a friendly message for unimplemented commands
-    print(f"Command '{args.command}' is not implemented yet.")
-    return 0
+    except FmfError as exc:
+        print(f"{exc.__class__.__name__}: {exc}", file=sys.stderr)
+        return get_exit_code(exc)
+    except Exception as exc:  # pragma: no cover - defensive guard
+        print(f"UnexpectedError: {exc}", file=sys.stderr)
+        return 1
 
 
 if __name__ == "__main__":
diff --git a/src/fmf/config/loader.py b/src/fmf/config/loader.py
index ee711f5..47fc495 100644
--- a/src/fmf/config/loader.py
+++ b/src/fmf/config/loader.py
@@ -104,6 +104,19 @@ def _apply_profile(cfg: dict, env: Mapping[str, str]) -> None:
         _deep_merge(cfg, overlay)
 
 
+def _apply_runtime_toggles(cfg: FmfConfig) -> None:
+    if getattr(cfg, "experimental", None):
+        exp = cfg.experimental
+        if exp and exp.streaming and not os.getenv("FMF_EXPERIMENTAL_STREAMING"):
+            os.environ["FMF_EXPERIMENTAL_STREAMING"] = "1"
+        if exp and exp.observability_otel and not os.getenv("FMF_OBSERVABILITY_OTEL"):
+            os.environ["FMF_OBSERVABILITY_OTEL"] = "1"
+    if getattr(cfg, "processing", None) and cfg.processing and cfg.processing.hash_algo:
+        os.environ.setdefault("FMF_HASH_ALGO", cfg.processing.hash_algo)
+    if getattr(cfg, "retries", None) and cfg.retries and cfg.retries.max_elapsed_s is not None:
+        os.environ.setdefault("FMF_RETRY_MAX_ELAPSED", str(cfg.retries.max_elapsed_s))
+
+
 def load_config(
     path: str,
     *,
@@ -135,6 +148,7 @@ def load_config(
     # Attempt to validate with Pydantic if available
     try:
         model = FmfConfig.model_validate(data)  # type: ignore[attr-defined]
+        _apply_runtime_toggles(model)
         return model
     except Exception:
         # Fallback to raw dict when validation not available
diff --git a/src/fmf/config/models.py b/src/fmf/config/models.py
index 964c41d..7e78d20 100644
--- a/src/fmf/config/models.py
+++ b/src/fmf/config/models.py
@@ -132,6 +132,16 @@ class ProcessingConfig(BaseModel):
     tables: ProcessingTables = Field(default_factory=ProcessingTables)
     images: ProcessingImages = Field(default_factory=ProcessingImages)
     metadata: ProcessingMetadata = Field(default_factory=ProcessingMetadata)
+    hash_algo: Optional[Literal["blake2b", "xxh64"]] = None
+
+
+class ExperimentalConfig(BaseModel):
+    streaming: bool = False
+    observability_otel: bool = False
+
+
+class RetriesConfig(BaseModel):
+    max_elapsed_s: Optional[float] = Field(default=None, ge=0)
 
 
 # Inference
@@ -283,6 +293,8 @@ class FmfConfig(BaseModel):
     prompt_registry: Optional[PromptRegistryConfig] = None
     run: Optional[RunConfig] = None
     rag: Optional[RagConfig] = None
+    experimental: Optional[ExperimentalConfig] = None
+    retries: Optional[RetriesConfig] = None
 
     # allow extra to keep forward-compatible
     model_config = _ConfigDict(extra="allow")
@@ -299,6 +311,8 @@ __all__ = [
     "S3Connector",
     "SharePointConnector",
     "ProcessingConfig",
+    "ExperimentalConfig",
+    "RetriesConfig",
     "InferenceConfig",
     "PromptRegistryConfig",
     "ExportConfig",
diff --git a/src/fmf/connectors/__init__.py b/src/fmf/connectors/__init__.py
index 36f2aa2..23da1d9 100644
--- a/src/fmf/connectors/__init__.py
+++ b/src/fmf/connectors/__init__.py
@@ -3,6 +3,7 @@
 Includes base protocol/types and factories to create connectors from config.
 """
 
+from ..core.interfaces import ConnectorSelectors, ConnectorSpec
 from .base import DataConnector, ResourceInfo, ResourceRef, ConnectorError
 
 
@@ -21,27 +22,53 @@ def build_connector(cfg: object) -> DataConnector:
     if ctype == "local":
         from .local import LocalConnector
 
-        return LocalConnector(name=name, root=_cfg_get(cfg, "root"), include=_cfg_get(cfg, "include"), exclude=_cfg_get(cfg, "exclude"))
+        selectors = ConnectorSelectors(
+            include=list(_cfg_get(cfg, "include") or ["**/*"]),
+            exclude=list(_cfg_get(cfg, "exclude") or []),
+        )
+        spec = ConnectorSpec(
+            name=name,
+            type="local",
+            selectors=selectors,
+            options={"root": _cfg_get(cfg, "root")},
+        )
+        return LocalConnector(spec=spec)
     if ctype == "s3":
         from .s3 import S3Connector
-
-        return S3Connector(
+        selectors = ConnectorSelectors(
+            include=list(_cfg_get(cfg, "include") or ["**/*"]),
+            exclude=list(_cfg_get(cfg, "exclude") or []),
+        )
+        spec = ConnectorSpec(
             name=name,
-            bucket=_cfg_get(cfg, "bucket"),
-            prefix=_cfg_get(cfg, "prefix"),
-            region=_cfg_get(cfg, "region"),
-            kms_required=_cfg_get(cfg, "kms_required"),
+            type="s3",
+            selectors=selectors,
+            options={
+                "bucket": _cfg_get(cfg, "bucket"),
+                "prefix": _cfg_get(cfg, "prefix"),
+                "region": _cfg_get(cfg, "region"),
+                "kms_required": _cfg_get(cfg, "kms_required"),
+            },
         )
+        return S3Connector(spec=spec)
     if ctype == "sharepoint":
         from .sharepoint import SharePointConnector
-
-        return SharePointConnector(
+        selectors = ConnectorSelectors(
+            include=list(_cfg_get(cfg, "include") or ["**/*"]),
+            exclude=list(_cfg_get(cfg, "exclude") or []),
+        )
+        spec = ConnectorSpec(
             name=name,
-            site_url=_cfg_get(cfg, "site_url"),
-            drive=_cfg_get(cfg, "drive"),
-            root_path=_cfg_get(cfg, "root_path"),
-            auth_profile=_cfg_get(cfg, "auth_profile"),
+            type="sharepoint",
+            selectors=selectors,
+            options={
+                "site_url": _cfg_get(cfg, "site_url"),
+                "drive": _cfg_get(cfg, "drive"),
+                "root_path": _cfg_get(cfg, "root_path"),
+                "auth_profile": _cfg_get(cfg, "auth_profile"),
+            },
         )
+        return SharePointConnector(spec=spec)
     raise ValueError(f"Unsupported connector type: {ctype!r}")
 
 
diff --git a/src/fmf/connectors/base.py b/src/fmf/connectors/base.py
index e1a15ba..d8ed7bc 100644
--- a/src/fmf/connectors/base.py
+++ b/src/fmf/connectors/base.py
@@ -4,9 +4,7 @@ import datetime as _dt
 from dataclasses import dataclass
 from typing import IO, Any, Iterable, Protocol
 
-
-class ConnectorError(Exception):
-    """Errors raised by DataConnector implementations."""
+from ..core.errors import ConnectorError
 
 
 @dataclass(frozen=True)
@@ -62,4 +60,3 @@ __all__ = [
     "ResourceInfo",
     "DataConnector",
 ]
-
diff --git a/src/fmf/connectors/local.py b/src/fmf/connectors/local.py
index d205887..05a9a36 100644
--- a/src/fmf/connectors/local.py
+++ b/src/fmf/connectors/local.py
@@ -1,27 +1,38 @@
 from __future__ import annotations
 
-import os
 import fnmatch
+import os
 import pathlib
 import datetime as dt
 from typing import IO, Iterable, List, Optional
 
-from .base import DataConnector, ResourceRef, ResourceInfo, ConnectorError
+from ..core.interfaces import ConnectorSpec, ConnectorSelectors, RunContext
+from ..core.interfaces.connectors_base import BaseConnector
+from .base import ResourceRef, ResourceInfo, ConnectorError
 
 
-class LocalConnector:
+class LocalConnector(BaseConnector):
     def __init__(
         self,
         *,
-        name: str,
-        root: str,
+        spec: ConnectorSpec | None = None,
+        name: str | None = None,
+        root: str | None = None,
         include: Optional[List[str]] = None,
         exclude: Optional[List[str]] = None,
     ) -> None:
-        self.name = name
-        self.root = os.path.abspath(root)
-        self._include = include or ["**/*"]
-        self._exclude = exclude or []
+        if spec is None:
+            if name is None or root is None:
+                raise ValueError("LocalConnector requires either a spec or name/root parameters")
+            selectors = ConnectorSelectors(
+                include=list(include or ["**/*"]),
+                exclude=list(exclude or []),
+            )
+            spec = ConnectorSpec(name=name, type="local", selectors=selectors, options={"root": root})
+        super().__init__(spec)
+        self.root = os.path.abspath(spec.options.get("root", root or "."))
+        self._include = list(spec.selectors.include or ["**/*"])
+        self._exclude = list(spec.selectors.exclude or [])
 
     def _iter_paths(self, selector: List[str] | None) -> Iterable[pathlib.Path]:
         patterns = selector or self._include
@@ -54,12 +65,23 @@ class LocalConnector:
                 seen.add(rel)
                 yield p
 
-    def list(self, selector: list[str] | None = None) -> Iterable[ResourceRef]:
+    def list(
+        self,
+        *,
+        selector: list[str] | None = None,
+        context: RunContext | None = None,
+    ) -> Iterable[ResourceRef]:
         for p in self._iter_paths(selector):
             rel = p.relative_to(self.root).as_posix()
             yield ResourceRef(id=rel, uri=p.resolve().as_uri(), name=p.name)
 
-    def open(self, ref: ResourceRef, mode: str = "rb") -> IO[bytes]:
+    def open(
+        self,
+        ref: ResourceRef,
+        *,
+        mode: str = "rb",
+        context: RunContext | None = None,
+    ) -> IO[bytes]:
         path = pathlib.Path(self.root, ref.id)
         if not path.is_file():
             raise ConnectorError(f"Resource not found: {ref.id}")
@@ -68,7 +90,7 @@ class LocalConnector:
             mode = mode + "b"
         return open(path, mode)
 
-    def info(self, ref: ResourceRef) -> ResourceInfo:
+    def info(self, ref: ResourceRef, *, context: RunContext | None = None) -> ResourceInfo:
         path = pathlib.Path(self.root, ref.id)
         if not path.exists():
             raise ConnectorError(f"Resource not found: {ref.id}")
diff --git a/src/fmf/connectors/s3.py b/src/fmf/connectors/s3.py
index acc6e28..cd92ffd 100644
--- a/src/fmf/connectors/s3.py
+++ b/src/fmf/connectors/s3.py
@@ -3,28 +3,69 @@ from __future__ import annotations
 import datetime as dt
 from typing import IO, Iterable, List, Optional
 
-from .base import DataConnector, ResourceInfo, ResourceRef, ConnectorError
+from ..core.interfaces import ConnectorSpec, ConnectorSelectors, RunContext
+from ..core.interfaces.connectors_base import BaseConnector
+from ..core.retry import default_predicate, retry_call
+from .base import ResourceInfo, ResourceRef, ConnectorError
 
 
-class S3Connector:
+class _ManagedBody:
+    def __init__(self, body) -> None:
+        self._body = body
+
+    def read(self, *args, **kwargs):  # pragma: no cover - passthrough
+        return self._body.read(*args, **kwargs)
+
+    def close(self) -> None:
+        close = getattr(self._body, "close", None)
+        if callable(close):
+            close()
+
+    def __enter__(self):  # pragma: no cover - passthrough
+        return self
+
+    def __exit__(self, exc_type, exc, tb):  # pragma: no cover - passthrough
+        self.close()
+
+
+class S3Connector(BaseConnector):
     def __init__(
         self,
         *,
-        name: str,
-        bucket: str,
+        spec: ConnectorSpec | None = None,
+        name: str | None = None,
+        bucket: str | None = None,
         prefix: Optional[str] = None,
         region: Optional[str] = None,
         kms_required: Optional[bool] = None,
     ) -> None:
-        self.name = name
-        self.bucket = bucket
-        self.prefix = prefix or ""
+        if spec is None:
+            if name is None or bucket is None:
+                raise ValueError("S3Connector requires either a spec or name/bucket parameters")
+            selectors = ConnectorSelectors(include=["**/*"], exclude=[])
+            spec = ConnectorSpec(
+                name=name,
+                type="s3",
+                selectors=selectors,
+                options={
+                    "bucket": bucket,
+                    "prefix": prefix or "",
+                    "region": region,
+                    "kms_required": bool(kms_required),
+                },
+            )
+        super().__init__(spec)
+        options = spec.options
+        self.bucket = options.get("bucket", bucket)
+        self.prefix = options.get("prefix", prefix or "") or ""
         if self.prefix and not self.prefix.endswith("/"):
             # Normalize to directory-like prefix
             self.prefix += "/"
-        self.region = region
-        self.kms_required = bool(kms_required)
+        self.region = options.get("region", region)
+        self.kms_required = bool(options.get("kms_required", kms_required))
         self._client = None
+        self._include = list(spec.selectors.include or ["**/*"])
+        self._exclude = list(spec.selectors.exclude or [])
 
     def _s3(self):
         if self._client is not None:
@@ -36,6 +77,10 @@ class S3Connector:
         self._client = boto3.client("s3", region_name=self.region)
         return self._client
 
+    @staticmethod
+    def _should_retry(exc: Exception) -> bool:
+        return default_predicate(exc)
+
     def _iter_keys(self) -> Iterable[dict]:
         client = self._s3()
         token = None
@@ -43,7 +88,7 @@ class S3Connector:
             kwargs = {"Bucket": self.bucket, "Prefix": self.prefix}
             if token:
                 kwargs["ContinuationToken"] = token
-            resp = client.list_objects_v2(**kwargs)
+            resp = retry_call(client.list_objects_v2, kwargs=kwargs, should_retry=self._should_retry)
             for obj in resp.get("Contents", []) or []:
                 yield obj
             if resp.get("IsTruncated"):
@@ -53,10 +98,15 @@ class S3Connector:
             else:
                 break
 
-    def list(self, selector: list[str] | None = None) -> Iterable[ResourceRef]:
+    def list(
+        self,
+        *,
+        selector: list[str] | None = None,
+        context: RunContext | None = None,
+    ) -> Iterable[ResourceRef]:
         import fnmatch
 
-        patterns = selector or ["**/*"]
+        patterns = selector or self._include or ["**/*"]
         for obj in self._iter_keys():
             key = obj.get("Key")
             if key is None:
@@ -68,22 +118,30 @@ class S3Connector:
                 for pat in patterns
             ):
                 continue
+            if self._exclude and any(fnmatch.fnmatchcase(rel, ex) for ex in self._exclude):
+                continue
             uri = f"s3://{self.bucket}/{key}"
             yield ResourceRef(id=rel, uri=uri, name=rel.split("/")[-1])
 
-    def open(self, ref: ResourceRef, mode: str = "rb") -> IO[bytes]:
+    def open(
+        self,
+        ref: ResourceRef,
+        *,
+        mode: str = "rb",
+        context: RunContext | None = None,
+    ) -> IO[bytes]:
         if "r" not in mode:
             raise ConnectorError("S3Connector only supports reading")
         key = self.prefix + ref.id if self.prefix else ref.id
-        resp = self._s3().get_object(Bucket=self.bucket, Key=key)
+        resp = retry_call(self._s3().get_object, kwargs={"Bucket": self.bucket, "Key": key}, should_retry=self._should_retry)
         body = resp.get("Body")
         if body is None:
             raise ConnectorError("Empty response body")
-        return body  # type: ignore[return-value]
+        return _ManagedBody(body)
 
-    def info(self, ref: ResourceRef) -> ResourceInfo:
+    def info(self, ref: ResourceRef, *, context: RunContext | None = None) -> ResourceInfo:
         key = self.prefix + ref.id if self.prefix else ref.id
-        head = self._s3().head_object(Bucket=self.bucket, Key=key)
+        head = retry_call(self._s3().head_object, kwargs={"Bucket": self.bucket, "Key": key}, should_retry=self._should_retry)
         size = head.get("ContentLength")
         last_modified = head.get("LastModified")
         if isinstance(last_modified, dt.datetime) and last_modified.tzinfo is None:
@@ -103,4 +161,3 @@ class S3Connector:
 
 
 __all__ = ["S3Connector"]
-
diff --git a/src/fmf/connectors/sharepoint.py b/src/fmf/connectors/sharepoint.py
index bde432e..638d87f 100644
--- a/src/fmf/connectors/sharepoint.py
+++ b/src/fmf/connectors/sharepoint.py
@@ -1,14 +1,16 @@
 from __future__ import annotations
 
 import io
-import time
 import urllib.parse as _url
 from typing import IO, Iterable, Optional
 
+from ..core.interfaces import ConnectorSpec, ConnectorSelectors, RunContext
+from ..core.interfaces.connectors_base import BaseConnector
+from ..core.retry import default_predicate, retry_call
 from .base import ConnectorError, ResourceInfo, ResourceRef
 
 
-class SharePointConnector:
+class SharePointConnector(BaseConnector):
     """SharePoint connector using Microsoft Graph SDK.
 
     - Lists and downloads files from a given site + drive + root_path using Graph paths
@@ -19,18 +21,37 @@ class SharePointConnector:
     def __init__(
         self,
         *,
-        name: str,
-        site_url: str,
-        drive: str,
+        spec: ConnectorSpec | None = None,
+        name: str | None = None,
+        site_url: str | None = None,
+        drive: str | None = None,
         root_path: Optional[str] = None,
         auth_profile: Optional[str] = None,
     ) -> None:
-        self.name = name
-        self.site_url = site_url
-        self.drive = drive
-        self.root_path = (root_path or "").strip("/")
-        self.auth_profile = auth_profile
+        if spec is None:
+            if any(v is None for v in (name, site_url, drive)):
+                raise ValueError("SharePointConnector requires either a spec or name/site_url/drive parameters")
+            selectors = ConnectorSelectors(include=["**/*"], exclude=[])
+            spec = ConnectorSpec(
+                name=name,  # type: ignore[arg-type]
+                type="sharepoint",
+                selectors=selectors,
+                options={
+                    "site_url": site_url,
+                    "drive": drive,
+                    "root_path": root_path,
+                    "auth_profile": auth_profile,
+                },
+            )
+        super().__init__(spec)
+        options = spec.options
+        self.site_url = options.get("site_url", site_url)
+        self.drive = options.get("drive", drive)
+        self.root_path = (options.get("root_path", root_path) or "").strip("/")
+        self.auth_profile = options.get("auth_profile", auth_profile)
         self._client = None
+        self._include = list(spec.selectors.include or ["**/*"])
+        self._exclude = list(spec.selectors.exclude or [])
 
     def _client_or_raise(self):  # pragma: no cover - exercised via tests with monkeypatching
         if self._client is not None:
@@ -46,18 +67,10 @@ class SharePointConnector:
         self._client = GraphServiceClient(credential=cred, scopes=["https://graph.microsoft.com/.default"])  # type: ignore[call-arg]
         return self._client
 
-    def _retry(self, func, *args, **kwargs):
-        delay = 0.5
-        for _ in range(6):
-            try:
-                return func(*args, **kwargs)
-            except Exception as e:  # naive throttling/backoff handler
-                status = getattr(e, "status_code", None) or getattr(getattr(e, "response", None), "status_code", None)
-                if status == 429:
-                    time.sleep(delay)
-                    delay = min(delay * 2, 8.0)
-                    continue
-                raise
+    @staticmethod
+    def _should_retry(exc: Exception) -> bool:
+        status = getattr(exc, "status_code", None) or getattr(getattr(exc, "response", None), "status_code", None)
+        return status == 429 or default_predicate(exc)
 
     def _parse_site(self) -> tuple[str, str]:
         # https://contoso.sharepoint.com/sites/HR -> host, path
@@ -70,11 +83,11 @@ class SharePointConnector:
         client = self._client_or_raise()
         host, path = self._parse_site()
         # GET /sites/{host}:/{path}
-        site = self._retry(lambda: client.api(f"/sites/{host}:/{path}").get())
+        site = retry_call(lambda: client.api(f"/sites/{host}:/{path}").get(), should_retry=self._should_retry)
         site_id = site.get("id") if isinstance(site, dict) else getattr(site, "id", None)
         if not site_id:
             raise ConnectorError("Failed to resolve site id")
-        drives = self._retry(lambda: client.api(f"/sites/{site_id}/drives").get())
+        drives = retry_call(lambda: client.api(f"/sites/{site_id}/drives").get(), should_retry=self._should_retry)
         values = drives.get("value") if isinstance(drives, dict) else getattr(drives, "value", [])
         drive_id = None
         for d in values or []:
@@ -89,24 +102,35 @@ class SharePointConnector:
     def _graph_list_children(self, site_id: str, drive_id: str, rel_path: str) -> list[dict]:  # pragma: no cover - tests patch
         client = self._client_or_raise()
         if rel_path:
-            resp = self._retry(lambda: client.api(f"/sites/{site_id}/drives/{drive_id}/root:/{rel_path}:/children").get())
+            resp = retry_call(
+                lambda: client.api(f"/sites/{site_id}/drives/{drive_id}/root:/{rel_path}:/children").get(),
+                should_retry=self._should_retry,
+            )
         else:
-            resp = self._retry(lambda: client.api(f"/sites/{site_id}/drives/{drive_id}/root/children").get())
+            resp = retry_call(
+                lambda: client.api(f"/sites/{site_id}/drives/{drive_id}/root/children").get(),
+                should_retry=self._should_retry,
+            )
         return resp.get("value") if isinstance(resp, dict) else getattr(resp, "value", [])
 
     def _graph_download(self, site_id: str, drive_id: str, rel_path: str):  # pragma: no cover - tests patch
         client = self._client_or_raise()
-        return self._retry(lambda: client.api(f"/sites/{site_id}/drives/{drive_id}/root:/{rel_path}:/content").get())
+        return retry_call(lambda: client.api(f"/sites/{site_id}/drives/{drive_id}/root:/{rel_path}:/content").get(), should_retry=self._should_retry)
 
     def _graph_item_props(self, site_id: str, drive_id: str, rel_path: str):  # pragma: no cover - tests patch
         client = self._client_or_raise()
-        return self._retry(lambda: client.api(f"/sites/{site_id}/drives/{drive_id}/root:/{rel_path}").get())
+        return retry_call(lambda: client.api(f"/sites/{site_id}/drives/{drive_id}/root:/{rel_path}").get(), should_retry=self._should_retry)
 
-    def list(self, selector: list[str] | None = None) -> Iterable[ResourceRef]:
+    def list(
+        self,
+        *,
+        selector: list[str] | None = None,
+        context: RunContext | None = None,
+    ) -> Iterable[ResourceRef]:
         import fnmatch
 
         site_id, drive_id = self._resolve_ids()
-        patterns = selector or ["**/*"]
+        patterns = selector or self._include or ["**/*"]
 
         stack = [self.root_path]
         while stack:
@@ -125,9 +149,17 @@ class SharePointConnector:
                     for pat in patterns
                 ):
                     continue
+                if self._exclude and any(fnmatch.fnmatchcase(within, ex) for ex in self._exclude):
+                    continue
                 yield ResourceRef(id=within, uri=f"sharepoint:/sites/{site_id}/drives/{drive_id}/root:/{rel}", name=name)
 
-    def open(self, ref: ResourceRef, mode: str = "rb") -> IO[bytes]:
+    def open(
+        self,
+        ref: ResourceRef,
+        *,
+        mode: str = "rb",
+        context: RunContext | None = None,
+    ) -> IO[bytes]:
         if "r" not in mode:
             raise ConnectorError("SharePointConnector only supports reading")
         site_id, drive_id = self._resolve_ids()
@@ -137,7 +169,7 @@ class SharePointConnector:
             return io.BytesIO(data)
         return data
 
-    def info(self, ref: ResourceRef) -> ResourceInfo:
+    def info(self, ref: ResourceRef, *, context: RunContext | None = None) -> ResourceInfo:
         import datetime as dt
 
         site_id, drive_id = self._resolve_ids()
diff --git a/src/fmf/core/__init__.py b/src/fmf/core/__init__.py
index affa95e..020f715 100644
--- a/src/fmf/core/__init__.py
+++ b/src/fmf/core/__init__.py
@@ -1,6 +1,9 @@
 """Core abstractions shared across Frontier Model Framework layers."""
 
 from . import interfaces as interfaces
+from . import errors as errors
 from .interfaces import *  # noqa: F401,F403
+from .errors import *  # noqa: F401,F403
 
-__all__ = interfaces.__all__
+combined = list(dict.fromkeys(list(interfaces.__all__) + list(errors.__all__)))
+__all__ = tuple(combined)
diff --git a/src/fmf/core/errors.py b/src/fmf/core/errors.py
new file mode 100644
index 0000000..a756fff
--- /dev/null
+++ b/src/fmf/core/errors.py
@@ -0,0 +1,68 @@
+from __future__ import annotations
+
+from typing import Optional
+
+
+class FmfError(Exception):
+    """Base exception for the Frontier Model Framework."""
+
+    def __init__(self, message: str = "") -> None:
+        super().__init__(message)
+        self.message = message
+
+
+class ConfigError(FmfError):
+    pass
+
+
+class AuthError(FmfError):
+    pass
+
+
+class ConnectorError(FmfError):
+    pass
+
+
+class ProcessingError(FmfError):
+    pass
+
+
+class InferenceError(FmfError):
+    def __init__(self, message: str, *, status_code: Optional[int] = None) -> None:
+        super().__init__(message)
+        self.status_code = status_code
+
+
+class ExportError(FmfError):
+    pass
+
+
+EXIT_CODES: dict[type[FmfError], int] = {
+    FmfError: 1,
+    ConfigError: 2,
+    AuthError: 3,
+    ConnectorError: 4,
+    ProcessingError: 5,
+    InferenceError: 6,
+    ExportError: 7,
+}
+
+
+def get_exit_code(exc: FmfError) -> int:
+    for cls in exc.__class__.__mro__:
+        if cls in EXIT_CODES:
+            return EXIT_CODES[cls]  # type: ignore[index]
+    return 1
+
+
+__all__ = [
+    "FmfError",
+    "ConfigError",
+    "AuthError",
+    "ConnectorError",
+    "ProcessingError",
+    "InferenceError",
+    "ExportError",
+    "EXIT_CODES",
+    "get_exit_code",
+]
diff --git a/src/fmf/core/ids.py b/src/fmf/core/ids.py
new file mode 100644
index 0000000..4e376a9
--- /dev/null
+++ b/src/fmf/core/ids.py
@@ -0,0 +1,85 @@
+from __future__ import annotations
+
+import hashlib
+import os
+import unicodedata
+from datetime import datetime, timezone
+from typing import Optional
+
+
+def _hash_algo() -> str:
+    return os.getenv("FMF_HASH_ALGO", "blake2b").lower()
+
+def normalize_text(text: str) -> bytes:
+    """Normalize textual content for hashing.
+
+    - Canonicalises Unicode to NFC
+    - Strips UTF-8 BOM if present
+    - Converts Windows/Mac newlines to ``\n``
+    - Returns UTF-8 encoded bytes
+    """
+
+    if text.startswith("\ufeff"):
+        text = text.lstrip("\ufeff")
+    normalised = unicodedata.normalize("NFC", text)
+    normalised = normalised.replace("\r\n", "\n").replace("\r", "\n")
+    return normalised.encode("utf-8")
+
+
+def hash_bytes(data: bytes, *, namespace: str = "", algo: str | None = None) -> str:
+    algo = (algo or _hash_algo())
+    if algo == "xxh64":  # optional fast hash
+        try:
+            import xxhash  # type: ignore
+
+            h = xxhash.xxh64()
+            if namespace:
+                h.update(namespace.encode("utf-8"))
+            h.update(data)
+            return h.hexdigest()
+        except Exception:
+            algo = "blake2b"
+    h = hashlib.blake2b(digest_size=16)
+    if namespace:
+        h.update(namespace.encode("utf-8"))
+    h.update(data)
+    return h.hexdigest()
+
+
+def document_id(*, source_uri: str, payload: bytes, modified_at: Optional[str] = None) -> str:
+    namespace = source_uri
+    if modified_at:
+        try:
+            dt_value = datetime.fromisoformat(modified_at.replace("Z", "+00:00"))
+        except ValueError:
+            dt_value = datetime.now(timezone.utc)
+        if dt_value.tzinfo is None:
+            dt_value = dt_value.replace(tzinfo=timezone.utc)
+        namespace = f"{namespace}|{dt_value.astimezone(timezone.utc).isoformat()}"
+    digest = hash_bytes(payload, namespace=namespace)
+    return f"doc_{digest}"
+
+
+def chunk_id(*, document_id: str, index: int, payload: str) -> str:
+    digest = hash_bytes(payload.encode("utf-8"), namespace=f"{document_id}|{index}")
+    return f"{document_id}_ch_{digest[:12]}"
+
+
+def blob_id(*, document_id: str, media_type: str, payload: bytes) -> str:
+    digest = hash_bytes(payload, namespace=f"{document_id}|{media_type}")
+    return f"blob_{digest[:12]}"
+
+
+def utc_now_iso() -> str:
+    tz = os.getenv("FMF_LOG_TZ", "UTC").upper()
+    zone = timezone.utc if tz in {"UTC", "Z"} else timezone.utc
+    return datetime.now(zone).isoformat().replace("+00:00", "Z")
+
+
+__all__ = [
+    "hash_bytes",
+    "document_id",
+    "chunk_id",
+    "blob_id",
+    "utc_now_iso",
+]
diff --git a/src/fmf/core/interfaces/__init__.py b/src/fmf/core/interfaces/__init__.py
index 3cb3451..ccce7f6 100644
--- a/src/fmf/core/interfaces/__init__.py
+++ b/src/fmf/core/interfaces/__init__.py
@@ -7,6 +7,7 @@ adopt them without breaking backwards compatibility.
 
 from .models import (
     ConnectorSpec,
+    ConnectorSelectors,
     DocumentModel,
     ChunkModel,
     ModelSpec,
@@ -36,6 +37,7 @@ __all__ = [
     "EmbeddingResponse",
     "BaseExporter",
     "ConnectorSpec",
+    "ConnectorSelectors",
     "DocumentModel",
     "ChunkModel",
     "ModelSpec",
diff --git a/src/fmf/core/interfaces/models.py b/src/fmf/core/interfaces/models.py
index 7188711..f0caa9c 100644
--- a/src/fmf/core/interfaces/models.py
+++ b/src/fmf/core/interfaces/models.py
@@ -3,7 +3,7 @@ from __future__ import annotations
 import uuid
 from typing import Any, Dict, List, Literal, Optional
 
-from pydantic import BaseModel, Field
+from pydantic import BaseModel, Field, ConfigDict
 
 
 class RunContext(BaseModel):
@@ -104,15 +104,21 @@ class ModelSpec(BaseModel):
 class ExportSpec(BaseModel):
     """Standardised exporter configuration."""
 
+    model_config = ConfigDict(populate_by_name=True)
+
     name: str
     type: str
     destination: str | None = None
     format: Literal["jsonl", "csv", "parquet", "delta", "excel", "native", "custom"] = "jsonl"
-    mode: Literal["append", "upsert", "overwrite"] = "append"
+    write_mode: Literal["append", "upsert", "overwrite"] = Field("append", alias="mode")
     key_fields: list[str] | None = None
     options: dict[str, Any] = Field(default_factory=dict)
     metadata: dict[str, Any] = Field(default_factory=dict)
 
+    @property
+    def mode(self) -> str:
+        return self.write_mode
+
 
 __all__ = [
     "BlobModel",
diff --git a/src/fmf/core/interfaces/providers_base.py b/src/fmf/core/interfaces/providers_base.py
index 6905158..05ec843 100644
--- a/src/fmf/core/interfaces/providers_base.py
+++ b/src/fmf/core/interfaces/providers_base.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 from abc import ABC, abstractmethod
-from typing import Any, Callable
+from typing import Any, Callable, Iterator
 
 from pydantic import BaseModel, Field
 
@@ -64,17 +64,44 @@ class BaseProvider(ABC):
     def complete(self, request: CompletionRequest) -> CompletionResponse:
         """Execute a non-streaming completion."""
 
+    def iter_tokens(self, request: CompletionRequest) -> Iterator[str]:  # pragma: no cover - overridable
+        """Yield streaming chunks and convey the final :class:`CompletionResponse` via ``StopIteration.value``.
+
+        Providers overriding this generator should ``yield`` each token (or chunk) and finally
+        ``raise StopIteration(completion)`` so that :meth:`stream` can return the full
+        :class:`CompletionResponse`. The default implementation delegates to :meth:`complete` and
+        emits the full text as a single chunk.
+        """
+
+        response = self.complete(request)
+        if response.text:
+            yield response.text
+        return response  # type: ignore[misc]
+
     def stream(
         self,
         request: CompletionRequest,
         on_token: Callable[[str], None],
-    ) -> CompletionResponse:  # pragma: no cover - default fallback
-        """Optional streaming handler; falls back to non-streaming invocation."""
-
-        response = self.complete(request)
-        if response.text:
-            on_token(response.text)
-        return response
+    ) -> CompletionResponse:
+        """Stream tokens to ``on_token`` and return the final :class:`CompletionResponse`.
+
+        The generator returned by :meth:`iter_tokens` is consumed until exhaustion. Providers
+        are expected to raise ``StopIteration`` with the final :class:`CompletionResponse`
+        attached to ``StopIteration.value`` (see :pep:`380`). A fallback empty response is
+        returned when no completion is supplied.
+        """
+
+        iterator = self.iter_tokens(request)
+        completion: CompletionResponse | None = None
+        while True:
+            try:
+                token = next(iterator)
+            except StopIteration as stop:
+                completion = stop.value if isinstance(stop.value, CompletionResponse) else completion
+                break
+            else:
+                on_token(token)
+        return completion or CompletionResponse(text="")
 
     def embed(self, request: EmbeddingRequest) -> EmbeddingResponse:  # pragma: no cover - optional override
         raise NotImplementedError("Provider does not implement embeddings")
diff --git a/src/fmf/core/retry.py b/src/fmf/core/retry.py
new file mode 100644
index 0000000..f10d378
--- /dev/null
+++ b/src/fmf/core/retry.py
@@ -0,0 +1,61 @@
+from __future__ import annotations
+
+import random
+import time
+from typing import Any, Callable, Optional
+
+
+RetryPredicate = Callable[[BaseException], bool]
+
+
+def default_predicate(exc: BaseException) -> bool:
+    status = getattr(exc, "status_code", None)
+    if status is None:
+        response = getattr(exc, "response", None)
+        if isinstance(response, dict):
+            status = response.get("status_code") or response.get("ResponseMetadata", {}).get("HTTPStatusCode")
+    return status in {429} or (isinstance(status, int) and 500 <= status < 600)
+
+
+def retry_call(
+    func: Callable[..., Any],
+    *,
+    args: tuple[Any, ...] = (),
+    kwargs: Optional[dict[str, Any]] = None,
+    should_retry: Optional[RetryPredicate] = None,
+    max_attempts: int = 5,
+    base_delay: float = 0.2,
+    max_delay: float = 5.0,
+    max_elapsed: float = 30.0,
+    sleep: Callable[[float], None] = time.sleep,
+    now: Callable[[], float] = time.monotonic,
+) -> Any:
+    """Retry ``func`` with decorrelated jitter until success or limits exceeded."""
+
+    predicate = should_retry or default_predicate
+    attempts = 0
+    start = now()
+    delay = base_delay
+    last_exc: Optional[BaseException] = None
+    while attempts < max_attempts:
+        try:
+            return func(*args, **(kwargs or {}))
+        except BaseException as exc:  # pragma: no cover - fully exercised in tests
+            last_exc = exc
+            attempts += 1
+            if not predicate(exc):
+                raise
+            elapsed = now() - start
+            if attempts >= max_attempts or elapsed >= max_elapsed:
+                raise
+            max_window = max(base_delay, delay * 3)
+            delay = min(max_delay, random.uniform(base_delay, max_window))
+            if elapsed + delay > max_elapsed:
+                raise
+            sleep(delay)
+    if last_exc is not None:
+        raise last_exc
+    return func(*args, **(kwargs or {}))
+
+
+__all__ = ["retry_call", "default_predicate"]
diff --git a/src/fmf/exporters/__init__.py b/src/fmf/exporters/__init__.py
index 9f730e3..ce56315 100644
--- a/src/fmf/exporters/__init__.py
+++ b/src/fmf/exporters/__init__.py
@@ -2,6 +2,7 @@ from __future__ import annotations
 
 from typing import Any
 
+from ..core.interfaces import ExportSpec
 from .base import Exporter, ExportError, ExportResult
 
 
@@ -19,16 +20,22 @@ def build_exporter(cfg: Any) -> Exporter:
     if etype == "s3":
         from .s3 import S3Exporter
 
-        return S3Exporter(
+        spec = ExportSpec(
             name=name,
-            bucket=_cfg_get(cfg, "bucket"),
-            prefix=_cfg_get(cfg, "prefix"),
-            format=_cfg_get(cfg, "format"),
-            compression=_cfg_get(cfg, "compression"),
-            partition_by=_cfg_get(cfg, "partition_by"),
-            sse=_cfg_get(cfg, "sse"),
-            kms_key_id=_cfg_get(cfg, "kms_key_id"),
+            type="s3",
+            format=_cfg_get(cfg, "format") or "jsonl",
+            write_mode=_cfg_get(cfg, "mode", "append"),
+            key_fields=_cfg_get(cfg, "key_fields"),
+            options={
+                "bucket": _cfg_get(cfg, "bucket"),
+                "prefix": _cfg_get(cfg, "prefix"),
+                "compression": _cfg_get(cfg, "compression"),
+                "partition_by": _cfg_get(cfg, "partition_by"),
+                "sse": _cfg_get(cfg, "sse"),
+                "kms_key_id": _cfg_get(cfg, "kms_key_id"),
+            },
         )
+        return S3Exporter(spec=spec)
     if etype == "dynamodb":
         from .dynamodb import DynamoDBExporter
 
@@ -79,4 +86,3 @@ def build_exporter(cfg: Any) -> Exporter:
 
 
 __all__ = ["Exporter", "ExportError", "ExportResult", "build_exporter"]
-
diff --git a/src/fmf/exporters/base.py b/src/fmf/exporters/base.py
index 1bd373a..f27b733 100644
--- a/src/fmf/exporters/base.py
+++ b/src/fmf/exporters/base.py
@@ -3,9 +3,7 @@ from __future__ import annotations
 from dataclasses import dataclass
 from typing import Any, Iterable, Literal, Protocol
 
-
-class ExportError(Exception):
-    pass
+from ..core.errors import ExportError
 
 
 @dataclass
@@ -22,7 +20,7 @@ class Exporter(Protocol):
         records: Iterable[dict[str, Any]] | bytes | str,
         *,
         schema: dict[str, Any] | None = None,
-        mode: Literal["append", "upsert", "overwrite"] = "append",
+        mode: Literal["append", "upsert", "overwrite"] | None = None,
         key_fields: list[str] | None = None,
         context: dict[str, Any] | None = None,
     ) -> ExportResult:
@@ -33,4 +31,3 @@ class Exporter(Protocol):
 
 
 __all__ = ["Exporter", "ExportResult", "ExportError"]
-
diff --git a/src/fmf/exporters/s3.py b/src/fmf/exporters/s3.py
index 1f10322..5ea0b57 100644
--- a/src/fmf/exporters/s3.py
+++ b/src/fmf/exporters/s3.py
@@ -1,41 +1,77 @@
 from __future__ import annotations
 
+import base64
 import datetime as dt
 import gzip
+import hashlib
 import io
 import json
 import os
 import uuid
 from typing import Any, Iterable, List, Dict
 
+from ..core.ids import utc_now_iso
+from ..core.interfaces import ExportSpec
 from .base import ExportError, ExportResult
 
 
 def _now_date():
-    return dt.datetime.utcnow().strftime("%Y-%m-%d")
+    return dt.datetime.now(dt.timezone.utc).strftime("%Y-%m-%d")
 
 
 class S3Exporter:
+    """Write artefacts to Amazon S3.
+
+    ``append`` mode emits a new object per invocation (no guarantees about ordering). ``overwrite``
+    writes to a deterministic key via upload-to-temp + copy-to-final to achieve atomic swaps. Upsert
+    is currently unsupported and raises :class:`ExportError`.
+    """
     def __init__(
         self,
         *,
-        name: str,
-        bucket: str,
+        spec: ExportSpec | None = None,
+        name: str | None = None,
+        bucket: str | None = None,
         prefix: str | None = None,
-        format: str | None = "jsonl",
+        format: str | None = None,
         compression: str | None = None,
         partition_by: list[str] | None = None,
         sse: str | None = None,
         kms_key_id: str | None = None,
+        mode: str | None = None,
     ) -> None:
-        self.name = name
-        self.bucket = bucket
-        self.prefix = prefix or ""
-        self.format = (format or "jsonl").lower()
-        self.compression = (compression or "none").lower()
-        self.partition_by = partition_by or []
-        self.sse = sse
-        self.kms_key_id = kms_key_id
+        if spec is None:
+            if name is None:
+                name = "s3"
+            options = {
+                "bucket": bucket,
+                "prefix": prefix,
+                "compression": compression,
+                "partition_by": partition_by,
+                "sse": sse,
+                "kms_key_id": kms_key_id,
+            }
+            spec = ExportSpec(
+                name=name,
+                type="s3",
+                format=format or "jsonl",
+                write_mode=mode or "append",
+                options=options,
+            )
+        self.spec = spec
+        options = spec.options
+        self.name = spec.name
+        self.bucket = options.get("bucket") or bucket
+        if not self.bucket:
+            raise ExportError("S3 exporter requires a 'bucket' option")
+        self.prefix = (options.get("prefix") or prefix or "").lstrip("/")
+        self.format = (spec.format or format or "jsonl").lower()
+        self.compression = (options.get("compression") or compression or "none").lower()
+        self.partition_by = options.get("partition_by") or partition_by or []
+        self.sse = options.get("sse") or sse
+        self.kms_key_id = options.get("kms_key_id") or kms_key_id
+        self.write_mode = (spec.write_mode or mode or "append").lower()
+        self.key_fields = spec.key_fields or []
         self._client = None
 
     def _s3(self):
@@ -48,25 +84,35 @@ class S3Exporter:
         self._client = boto3.client("s3")
         return self._client
 
-    def _build_key(self, *, context: dict[str, Any] | None) -> str:
-        run_id = (context or {}).get("run_id") or dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
-        prefix = (self.prefix or "").replace("${run_id}", run_id)
-        parts = [prefix.rstrip("/")]
+    def _ext(self) -> str:
+        mapping = {
+            "jsonl": ".jsonl",
+            "csv": ".csv",
+            "parquet": ".parquet",
+        }
+        ext = mapping.get(self.format, ".bin")
+        if self.compression == "gzip":
+            ext += ".gz"
+        return ext
+
+    def _build_key(self, *, context: dict[str, Any] | None, final: bool = True) -> str:
+        run_id = (context or {}).get("run_id") or utc_now_iso().replace(":", "").replace("-", "")
+        prefix = (self.prefix or "").replace("${run_id}", run_id).strip("/")
+        parts = [prefix] if prefix else []
         if "date" in (self.partition_by or []):
             parts.append(f"date={_now_date()}")
-        # unique object name for append semantics
-        if self.format == "jsonl":
-            ext = ".jsonl"
-        elif self.format == "csv":
-            ext = ".csv"
-        elif self.format == "parquet":
-            ext = ".parquet"
+        base_name = context.get("filename") if isinstance(context, dict) else None  # type: ignore[arg-type]
+        if not base_name:
+            base_name = self.name or "export"
+        if self.write_mode == "overwrite" and final:
+            filename = f"{base_name}{self._ext()}"
         else:
-            ext = ".bin"
-        if self.compression == "gzip":
-            ext += ".gz"
-        parts.append(f"part-{uuid.uuid4().hex}{ext}")
-        return "/".join([p for p in parts if p])
+            filename = f"part-{uuid.uuid4().hex}{self._ext()}"
+        parts.append(filename)
+        key = "/".join([p for p in parts if p])
+        if not final:
+            key = f"{key}.tmp-{uuid.uuid4().hex}"
+        return key
 
     def _ensure_records(self, recs: Iterable[dict[str, Any]] | bytes | str) -> List[Dict[str, Any]]:
         if isinstance(recs, (bytes, str)):
@@ -140,20 +186,57 @@ class S3Exporter:
         records: Iterable[dict[str, Any]] | bytes | str,
         *,
         schema: dict[str, Any] | None = None,
-        mode: str = "append",
+        mode: str | None = None,
         key_fields: list[str] | None = None,
         context: dict[str, Any] | None = None,
     ) -> ExportResult:
-        key = self._build_key(context=context)
+        active_mode = (mode or self.write_mode or "append").lower()
+        if active_mode == "upsert":  # TODO: implement S3 upsert semantics via merge manifest
+            raise ExportError("S3 upsert mode is not supported yet")
+
         data = self._serialize(records)
-        kwargs = {"Bucket": self.bucket, "Key": key, "Body": data}
+        client = self._s3()
+        md5_digest = hashlib.md5(data).digest()
+        content_md5 = base64.b64encode(md5_digest).decode("ascii")
+        quoted_etag = f'"{md5_digest.hex()}"'
+
+        if active_mode == "overwrite":
+            final_key = self._build_key(context=context, final=True)
+            temp_key = self._build_key(context=context, final=False)
+            put_kwargs = {"Bucket": self.bucket, "Key": temp_key, "Body": data, "ContentMD5": content_md5}
+            if self.sse == "kms":
+                put_kwargs["ServerSideEncryption"] = "aws:kms"
+                if self.kms_key_id:
+                    put_kwargs["SSEKMSKeyId"] = self.kms_key_id
+            elif self.sse == "s3":
+                put_kwargs["ServerSideEncryption"] = "AES256"
+            client.put_object(**put_kwargs)
+            copy_source = {"Bucket": self.bucket, "Key": temp_key}
+            copy_kwargs = {
+                "Bucket": self.bucket,
+                "Key": final_key,
+                "CopySource": copy_source,
+                "CopySourceIfMatch": quoted_etag,
+            }
+            if self.sse == "kms":
+                copy_kwargs["ServerSideEncryption"] = "aws:kms"
+                if self.kms_key_id:
+                    copy_kwargs["SSEKMSKeyId"] = self.kms_key_id
+            elif self.sse == "s3":
+                copy_kwargs["ServerSideEncryption"] = "AES256"
+            client.copy_object(**copy_kwargs)
+            client.delete_object(Bucket=self.bucket, Key=temp_key)
+            return ExportResult(count=-1, paths=[f"s3://{self.bucket}/{final_key}"])
+
+        key = self._build_key(context=context, final=True)
+        put_kwargs = {"Bucket": self.bucket, "Key": key, "Body": data, "ContentMD5": content_md5}
         if self.sse == "kms":
-            kwargs["ServerSideEncryption"] = "aws:kms"
+            put_kwargs["ServerSideEncryption"] = "aws:kms"
             if self.kms_key_id:
-                kwargs["SSEKMSKeyId"] = self.kms_key_id
+                put_kwargs["SSEKMSKeyId"] = self.kms_key_id
         elif self.sse == "s3":
-            kwargs["ServerSideEncryption"] = "AES256"
-        self._s3().put_object(**kwargs)
+            put_kwargs["ServerSideEncryption"] = "AES256"
+        client.put_object(**put_kwargs)
         return ExportResult(count=-1, paths=[f"s3://{self.bucket}/{key}"])
 
     def finalize(self) -> None:
diff --git a/src/fmf/inference/azure_openai.py b/src/fmf/inference/azure_openai.py
index 9987c02..1f6cd22 100644
--- a/src/fmf/inference/azure_openai.py
+++ b/src/fmf/inference/azure_openai.py
@@ -1,8 +1,10 @@
 from __future__ import annotations
 
-from typing import Callable, Optional
+import os
+from typing import Any, Callable, Iterable, Optional
 
 from .base_client import Completion, InferenceError, LLMClient, Message, RateLimiter, with_retries
+from .registry import register_provider
 from ..processing.chunking import estimate_tokens
 
 
@@ -15,11 +17,13 @@ class AzureOpenAIClient:
         deployment: str,
         rate_per_sec: float = 5.0,
         transport: Optional[Callable[[dict], dict]] = None,
+        stream_transport: Optional[Callable[[dict], Iterable[dict]]] = None,
     ) -> None:
         self.endpoint = endpoint
         self.api_version = api_version
         self.deployment = deployment
         self._transport = transport
+        self._stream_transport = stream_transport
         self._rl = RateLimiter(rate_per_sec)
 
     def _default_transport(self, payload: dict) -> dict:  # pragma: no cover - requires network
@@ -75,16 +79,7 @@ class AzureOpenAIClient:
             "max_tokens": max_tokens,
         }
 
-        def _do():
-            self._rl.wait()
-            transport = self._transport or self._default_transport
-            data = transport(payload)
-            # Expected shape: {choices:[{message:{content:...}, finish_reason:...}], usage:{prompt_tokens, completion_tokens}, model:...}
-            if stream and on_token:
-                # Simulate streaming by tokenizing the content and invoking callback
-                text = data.get("choices", [{}])[0].get("message", {}).get("content", "")
-                for tok in text.split():
-                    on_token(tok)
+        def _parse_response(data: dict) -> Completion:
             choice = (data.get("choices") or [{}])[0]
             msg = (choice.get("message") or {}).get("content", "")
             finish = choice.get("finish_reason")
@@ -97,7 +92,76 @@ class AzureOpenAIClient:
                 ct = estimate_tokens(msg)
             return Completion(text=msg, model=data.get("model"), stop_reason=finish, prompt_tokens=pt, completion_tokens=ct)
 
+        def _stream_enabled() -> bool:
+            value = os.getenv("FMF_EXPERIMENTAL_STREAMING", "")
+            return value.lower() in {"1", "true", "yes", "on"}
+
+        def _stream_payload() -> Optional[Completion]:
+            transport = self._stream_transport
+            if transport is None:
+                return None
+            chunks: list[str] = []
+            finish_reason: Optional[str] = None
+            usage: dict = {}
+            model_name: Optional[str] = None
+            for event in transport(payload):
+                if not isinstance(event, dict):
+                    continue
+                model_name = event.get("model") or model_name
+                usage = event.get("usage") or usage
+                for choice in event.get("choices", []):
+                    delta = choice.get("delta") or {}
+                    content = delta.get("content") or ""
+                    if content:
+                        chunks.append(content)
+                        if on_token is not None:
+                            on_token(content)
+                    finish_reason = choice.get("finish_reason") or finish_reason
+            if not chunks:
+                return None
+            text = "".join(chunks)
+            pt = usage.get("prompt_tokens") if isinstance(usage, dict) else None
+            ct = usage.get("completion_tokens") if isinstance(usage, dict) else None
+            if pt is None:
+                pt = sum(estimate_tokens(m.content) for m in messages)
+            if ct is None:
+                ct = estimate_tokens(text)
+            return Completion(
+                text=text,
+                model=model_name or self.deployment,
+                stop_reason=finish_reason,
+                prompt_tokens=pt,
+                completion_tokens=ct,
+            )
+
+        def _call_transport() -> dict:
+            transport = self._transport or self._default_transport
+            return transport(payload)
+
+        def _do():
+            self._rl.wait()
+            if stream and on_token is not None:
+                if _stream_enabled():
+                    streamed = _stream_payload()
+                    if streamed is not None:
+                        return streamed
+                data = _call_transport()
+                completion = _parse_response(data)
+                if completion.text:
+                    on_token(completion.text)
+                return completion
+            data = _call_transport()
+            return _parse_response(data)
+
         return with_retries(_do)
 
 
+@register_provider("azure_openai")
+def _build_from_config(cfg: Any) -> AzureOpenAIClient:  # type: ignore[name-defined]
+    endpoint = getattr(cfg, "endpoint", None) if not isinstance(cfg, dict) else cfg.get("endpoint")
+    api_version = getattr(cfg, "api_version", None) if not isinstance(cfg, dict) else cfg.get("api_version")
+    deployment = getattr(cfg, "deployment", None) if not isinstance(cfg, dict) else cfg.get("deployment")
+    return AzureOpenAIClient(endpoint=endpoint, api_version=api_version, deployment=deployment)
+
+
 __all__ = ["AzureOpenAIClient"]
diff --git a/src/fmf/inference/base_client.py b/src/fmf/inference/base_client.py
index a831536..54aae54 100644
--- a/src/fmf/inference/base_client.py
+++ b/src/fmf/inference/base_client.py
@@ -4,6 +4,8 @@ import time
 from dataclasses import dataclass
 from typing import Any, Callable, Iterable, Literal, Optional, Protocol
 
+from ..core.errors import InferenceError
+
 
 Role = Literal["system", "user", "assistant", "tool"]
 
@@ -23,16 +25,6 @@ class Completion:
     stop_reason: Optional[str] = None
     prompt_tokens: Optional[int] = None
     completion_tokens: Optional[int] = None
-
-
-class InferenceError(Exception):
-    """Raised on provider errors; may wrap HTTP or SDK errors."""
-
-    def __init__(self, message: str, *, status_code: int | None = None):
-        super().__init__(message)
-        self.status_code = status_code
-
-
 class LLMClient(Protocol):
     def complete(
         self,
diff --git a/src/fmf/inference/bedrock.py b/src/fmf/inference/bedrock.py
index 68c3b24..a5e275a 100644
--- a/src/fmf/inference/bedrock.py
+++ b/src/fmf/inference/bedrock.py
@@ -1,9 +1,11 @@
 from __future__ import annotations
 
 import json
-from typing import Callable, Optional
+import os
+from typing import Any, Callable, Iterable, Optional
 
 from .base_client import Completion, InferenceError, LLMClient, Message, RateLimiter, with_retries
+from .registry import register_provider
 from ..processing.chunking import estimate_tokens
 
 
@@ -15,10 +17,12 @@ class BedrockClient:
         model_id: str,
         rate_per_sec: float = 5.0,
         transport: Optional[Callable[[dict], dict]] = None,
+        stream_transport: Optional[Callable[[dict], Iterable[dict]]] = None,
     ) -> None:
         self.region = region
         self.model_id = model_id
         self._transport = transport
+        self._stream_transport = stream_transport
         self._rl = RateLimiter(rate_per_sec)
 
     def _default_transport(self, payload: dict) -> dict:  # pragma: no cover - requires network
@@ -97,15 +101,8 @@ class BedrockClient:
             "system": sysmsg,
         }
 
-        def _do():
-            self._rl.wait()
-            transport = self._transport or self._default_transport
-            data = transport(payload)
-            # Expected generic structure: {output: {text: ...}, usage: {input_tokens, output_tokens}}
+        def _parse_response(data: dict) -> Completion:
             text = data.get("output", {}).get("text") or data.get("content") or ""
-            if stream and on_token:
-                for tok in text.split():
-                    on_token(tok)
             usage = data.get("usage", {}) or {}
             pt = usage.get("input_tokens")
             ct = usage.get("output_tokens")
@@ -115,10 +112,88 @@ class BedrockClient:
                 ct = estimate_tokens(text)
             return Completion(text=text, model=self.model_id, stop_reason=data.get("stop_reason"), prompt_tokens=pt, completion_tokens=ct)
 
+        def _stream_enabled() -> bool:
+            value = os.getenv("FMF_EXPERIMENTAL_STREAMING", "")
+            return value.lower() in {"1", "true", "yes", "on"}
+
+        def _stream_payload() -> Optional[Completion]:
+            transport = self._stream_transport
+            if transport is None:
+                return None
+            chunks: list[str] = []
+            usage: dict = {}
+            stop_reason: Optional[str] = None
+            for event in transport(payload):
+                if not isinstance(event, dict):
+                    continue
+                usage = event.get("usage") or usage
+                if "delta" in event:
+                    delta = event.get("delta") or {}
+                    content = delta.get("text")
+                    if content:
+                        chunks.append(content)
+                        if on_token is not None:
+                            on_token(content)
+                    stop_reason = delta.get("stop_reason") or stop_reason
+                elif "chunk" in event:
+                    content = event.get("chunk")
+                    if content:
+                        chunks.append(str(content))
+                        if on_token is not None:
+                            on_token(str(content))
+                elif "content" in event:
+                    content = event.get("content")
+                    if isinstance(content, str):
+                        chunks.append(content)
+                        if on_token is not None:
+                            on_token(content)
+            if not chunks:
+                return None
+            text = "".join(chunks)
+            pt = usage.get("input_tokens") if isinstance(usage, dict) else None
+            ct = usage.get("output_tokens") if isinstance(usage, dict) else None
+            if pt is None:
+                pt = sum(estimate_tokens(m.content) for m in messages)
+            if ct is None:
+                ct = estimate_tokens(text)
+            return Completion(
+                text=text,
+                model=self.model_id,
+                stop_reason=stop_reason,
+                prompt_tokens=pt,
+                completion_tokens=ct,
+            )
+
+        def _call_transport() -> dict:
+            transport = self._transport or self._default_transport
+            return transport(payload)
+
+        def _do():
+            self._rl.wait()
+            if stream and on_token is not None:
+                if _stream_enabled():
+                    streamed = _stream_payload()
+                    if streamed is not None:
+                        return streamed
+                data = _call_transport()
+                completion = _parse_response(data)
+                if completion.text:
+                    on_token(completion.text)
+                return completion
+            data = _call_transport()
+            return _parse_response(data)
+
         try:
             return with_retries(_do)
         except InferenceError as e:
             raise InferenceError(f"Bedrock error: {e}", status_code=e.status_code)
 
 
+@register_provider("aws_bedrock")
+def _build_from_config(cfg: Any) -> BedrockClient:  # type: ignore[name-defined]
+    region = getattr(cfg, "region", None) if not isinstance(cfg, dict) else cfg.get("region")
+    model_id = getattr(cfg, "model_id", None) if not isinstance(cfg, dict) else cfg.get("model_id")
+    return BedrockClient(region=region, model_id=model_id)
+
+
 __all__ = ["BedrockClient"]
diff --git a/src/fmf/inference/providers/template_provider/__init__.py b/src/fmf/inference/providers/template_provider/__init__.py
index d1bd799..03fa0d8 100644
--- a/src/fmf/inference/providers/template_provider/__init__.py
+++ b/src/fmf/inference/providers/template_provider/__init__.py
@@ -1,5 +1,16 @@
 """Reference provider template for new inference integrations."""
 
+from typing import Any
+
+from ....core.interfaces import ModelSpec
+from ...registry import register_provider
 from .provider import TemplateProvider
 
+
+@register_provider("template")
+def _build_template(_cfg: Any) -> TemplateProvider:
+    spec = ModelSpec(provider="template", model="debug", modality="text")
+    return TemplateProvider(spec)
+
+
 __all__ = ["TemplateProvider"]
diff --git a/src/fmf/inference/registry.py b/src/fmf/inference/registry.py
new file mode 100644
index 0000000..abd66cc
--- /dev/null
+++ b/src/fmf/inference/registry.py
@@ -0,0 +1,32 @@
+from __future__ import annotations
+
+from typing import Any, Callable, Dict
+
+ProviderFactory = Callable[[Any], Any]
+
+_REGISTRY: Dict[str, ProviderFactory] = {}
+
+
+def register_provider(name: str) -> Callable[[ProviderFactory], ProviderFactory]:
+    name = name.lower()
+
+    def decorator(func: ProviderFactory) -> ProviderFactory:
+        _REGISTRY[name] = func
+        return func
+
+    return decorator
+
+
+def build_provider(name: str, cfg: Any) -> Any:
+    try:
+        factory = _REGISTRY[name.lower()]
+    except KeyError as exc:
+        raise ValueError(f"Provider '{name}' is not registered") from exc
+    return factory(cfg)
+
+
+def available_providers() -> list[str]:
+    return sorted(_REGISTRY.keys())
+
+
+__all__ = ["register_provider", "build_provider", "available_providers"]
diff --git a/src/fmf/inference/unified.py b/src/fmf/inference/unified.py
index fe02feb..0eb9c5e 100644
--- a/src/fmf/inference/unified.py
+++ b/src/fmf/inference/unified.py
@@ -2,25 +2,38 @@ from __future__ import annotations
 
 from typing import Any
 
+from .registry import build_provider, available_providers
 from .azure_openai import AzureOpenAIClient
 from .bedrock import BedrockClient
 
 
+def _subconfig(cfg: Any, key: str) -> Any:
+    if isinstance(cfg, dict):
+        return cfg.get(key)
+    return getattr(cfg, key, None)
+
+
 def build_llm_client(cfg: Any):
     provider = getattr(cfg, "provider", None) if not isinstance(cfg, dict) else cfg.get("provider")
+    if provider is None:
+        raise ValueError("Inference provider not specified in configuration")
+
+    subcfg = _subconfig(cfg, provider)
+    try:
+        return build_provider(provider, subcfg)
+    except ValueError:
+        pass
+
     if provider == "azure_openai":
-        acfg = getattr(cfg, "azure_openai", None) if not isinstance(cfg, dict) else cfg.get("azure_openai")
-        endpoint = getattr(acfg, "endpoint", None) if not isinstance(acfg, dict) else acfg.get("endpoint")
-        api_version = getattr(acfg, "api_version", None) if not isinstance(acfg, dict) else acfg.get("api_version")
-        deployment = getattr(acfg, "deployment", None) if not isinstance(acfg, dict) else acfg.get("deployment")
+        endpoint = getattr(subcfg, "endpoint", None) if not isinstance(subcfg, dict) else subcfg.get("endpoint")
+        api_version = getattr(subcfg, "api_version", None) if not isinstance(subcfg, dict) else subcfg.get("api_version")
+        deployment = getattr(subcfg, "deployment", None) if not isinstance(subcfg, dict) else subcfg.get("deployment")
         return AzureOpenAIClient(endpoint=endpoint, api_version=api_version, deployment=deployment)
     if provider == "aws_bedrock":
-        bcfg = getattr(cfg, "aws_bedrock", None) if not isinstance(cfg, dict) else cfg.get("aws_bedrock")
-        region = getattr(bcfg, "region", None) if not isinstance(bcfg, dict) else bcfg.get("region")
-        model_id = getattr(bcfg, "model_id", None) if not isinstance(bcfg, dict) else bcfg.get("model_id")
+        region = getattr(subcfg, "region", None) if not isinstance(subcfg, dict) else subcfg.get("region")
+        model_id = getattr(subcfg, "model_id", None) if not isinstance(subcfg, dict) else subcfg.get("model_id")
         return BedrockClient(region=region, model_id=model_id)
-    raise ValueError(f"Unsupported inference provider: {provider}")
+    raise ValueError(f"Unsupported inference provider: {provider}. Known providers: {', '.join(available_providers())}")
 
 
 __all__ = ["build_llm_client"]
-
diff --git a/src/fmf/observability/tracing.py b/src/fmf/observability/tracing.py
index 331c48d..11e5024 100644
--- a/src/fmf/observability/tracing.py
+++ b/src/fmf/observability/tracing.py
@@ -1,11 +1,15 @@
 from __future__ import annotations
 
+import os
 from contextlib import contextmanager
 
 
 @contextmanager
 def trace_span(name: str):
     """Start a tracing span if OpenTelemetry is available; otherwise no-op."""
+    if os.getenv("FMF_OBSERVABILITY_OTEL", "false").lower() not in {"1", "true", "yes", "on"}:
+        yield
+        return
     try:
         from opentelemetry import trace  # type: ignore
 
@@ -13,9 +17,7 @@ def trace_span(name: str):
         with tracer.start_as_current_span(name):
             yield
     except Exception:
-        # No OpenTelemetry installed or other failure; silently continue
         yield
 
 
 __all__ = ["trace_span"]
-
diff --git a/src/fmf/processing/chunking.py b/src/fmf/processing/chunking.py
index 08a8338..03e5875 100644
--- a/src/fmf/processing/chunking.py
+++ b/src/fmf/processing/chunking.py
@@ -3,6 +3,7 @@ from __future__ import annotations
 import re
 from typing import List
 
+from ..core.ids import chunk_id as compute_chunk_id
 from ..types import Chunk
 
 
@@ -46,7 +47,16 @@ def chunk_text(
         u_tokens = estimate_tokens(u)
         if cur_tokens + u_tokens > max_tokens and cur_parts:
             chunk_text_val = " ".join(cur_parts).strip()
-            chunks.append(Chunk(id=f"{doc_id}_ch{cid}", doc_id=doc_id, text=chunk_text_val, tokens_estimate=estimate_tokens(chunk_text_val)))
+            chunk_identifier = compute_chunk_id(document_id=doc_id, index=cid, payload=chunk_text_val)
+            chunks.append(
+                Chunk(
+                    id=chunk_identifier,
+                    doc_id=doc_id,
+                    text=chunk_text_val,
+                    tokens_estimate=estimate_tokens(chunk_text_val),
+                    provenance={"index": cid, "splitter": splitter, "length_chars": len(chunk_text_val)},
+                )
+            )
             cid += 1
             # start new chunk with overlap from end of previous
             if overlap > 0 and chunks[-1].text:
@@ -63,10 +73,18 @@ def chunk_text(
 
     if cur_parts:
         chunk_text_val = " ".join(cur_parts).strip()
-        chunks.append(Chunk(id=f"{doc_id}_ch{cid}", doc_id=doc_id, text=chunk_text_val, tokens_estimate=estimate_tokens(chunk_text_val)))
+        chunk_identifier = compute_chunk_id(document_id=doc_id, index=cid, payload=chunk_text_val)
+        chunks.append(
+            Chunk(
+                id=chunk_identifier,
+                doc_id=doc_id,
+                text=chunk_text_val,
+                tokens_estimate=estimate_tokens(chunk_text_val),
+                provenance={"index": cid, "splitter": splitter, "length_chars": len(chunk_text_val)},
+            )
+        )
 
     return chunks
 
 
 __all__ = ["chunk_text", "estimate_tokens"]
-
diff --git a/src/fmf/processing/errors.py b/src/fmf/processing/errors.py
index f832fb7..2d5d587 100644
--- a/src/fmf/processing/errors.py
+++ b/src/fmf/processing/errors.py
@@ -1,6 +1,4 @@
-class ProcessingError(Exception):
-    """Raised when processing fails (unsupported format, parse error, missing deps)."""
+from ..core.errors import ProcessingError
 
 
 __all__ = ["ProcessingError"]
-
diff --git a/src/fmf/processing/loaders.py b/src/fmf/processing/loaders.py
index 5087d7e..e2890c4 100644
--- a/src/fmf/processing/loaders.py
+++ b/src/fmf/processing/loaders.py
@@ -5,6 +5,12 @@ import io
 import os
 from typing import Any, Iterable, List, Optional, Tuple
 
+from ..core.ids import (
+    blob_id as compute_blob_id,
+    document_id as compute_document_id,
+    normalize_text as normalize_text_bytes,
+    utc_now_iso,
+)
 from ..types import Blob, Document
 from .errors import ProcessingError
 from .text import html_to_text, normalize_text
@@ -160,8 +166,22 @@ def load_document_from_bytes(
         blobs = [blob]
         text_out = None
 
-    return Document(id=os.path.basename(filename), source_uri=source_uri, text=text_out, blobs=blobs, metadata=meta)
+    payload = data if text_out is None else normalize_text_bytes(text_out)
+    doc_id = compute_document_id(source_uri=source_uri, payload=payload or b"")
+    provenance = {
+        "source_uri": source_uri,
+        "root_filename": os.path.basename(filename),
+        "hash": doc_id.split("_", 1)[-1],
+        "created_at": utc_now_iso(),
+    }
+    if blobs:
+        managed: List[Blob] = []
+        for blob in blobs:
+            data_bytes = blob.data or b""
+            managed.append(blob.with_id(compute_blob_id(document_id=doc_id, media_type=blob.media_type, payload=data_bytes)))
+        blobs = managed
 
+    return Document(id=doc_id, source_uri=source_uri, text=text_out, blobs=blobs, metadata=meta, provenance=provenance)
 
-__all__ = ["detect_type", "load_document_from_bytes"]
 
+__all__ = ["detect_type", "load_document_from_bytes"]
diff --git a/src/fmf/types.py b/src/fmf/types.py
index ec5cab6..04e2496 100644
--- a/src/fmf/types.py
+++ b/src/fmf/types.py
@@ -3,7 +3,7 @@ from __future__ import annotations
 import base64
 import hashlib
 import uuid
-from dataclasses import dataclass, field, asdict
+from dataclasses import dataclass, field
 from typing import Any, Dict, List, Optional
 
 
@@ -18,6 +18,10 @@ class Blob:
     data: Optional[bytes] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
 
+    def with_id(self, new_id: str) -> "Blob":
+        self.id = new_id
+        return self
+
     def to_serializable(self) -> Dict[str, Any]:
         d = dict(id=self.id, media_type=self.media_type, metadata=self.metadata)
         if self.data is not None:
@@ -34,6 +38,7 @@ class Document:
     text: Optional[str] = None
     blobs: Optional[List[Blob]] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
+    provenance: Dict[str, Any] = field(default_factory=dict)
 
     def to_serializable(self) -> Dict[str, Any]:
         return {
@@ -42,6 +47,7 @@ class Document:
             "text": self.text,
             "blobs": [b.to_serializable() for b in (self.blobs or [])],
             "metadata": self.metadata,
+            "provenance": self.provenance,
         }
 
 
@@ -52,6 +58,7 @@ class Chunk:
     text: str
     tokens_estimate: int
     metadata: Dict[str, Any] = field(default_factory=dict)
+    provenance: Dict[str, Any] = field(default_factory=dict)
 
     def to_serializable(self) -> Dict[str, Any]:
         return {
@@ -60,8 +67,8 @@ class Chunk:
             "text": self.text,
             "tokens_estimate": self.tokens_estimate,
             "metadata": self.metadata,
+            "provenance": self.provenance,
         }
 
 
 __all__ = ["Blob", "Document", "Chunk"]
-
diff --git a/tests/test_cli_exits.py b/tests/test_cli_exits.py
new file mode 100644
index 0000000..1398c92
--- /dev/null
+++ b/tests/test_cli_exits.py
@@ -0,0 +1,35 @@
+from __future__ import annotations
+
+import sys
+import unittest
+from unittest import mock
+
+
+class TestCliExitCodes(unittest.TestCase):
+    def setUp(self) -> None:
+        repo_root = __import__("os").path.abspath(__import__("os").path.join(__import__("os").path.dirname(__file__), ".."))
+        src_path = __import__("os").path.join(repo_root, "src")
+        if src_path not in sys.path:
+            sys.path.insert(0, src_path)
+
+    def test_connector_error_maps_exit_code(self) -> None:
+        from fmf.cli import main
+        from fmf.core.errors import ConnectorError
+
+        with mock.patch("fmf.cli._cmd_connect_ls", side_effect=ConnectorError("boom")):
+            rc = main(["connect", "ls", "demo"])
+        self.assertEqual(rc, 4)
+
+    def test_unexpected_error_returns_one(self) -> None:
+        from fmf.cli import main
+
+        with mock.patch("fmf.cli.build_parser") as patched:
+            fake_parser = mock.Mock()
+            fake_parser.parse_args.side_effect = RuntimeError("boom")
+            patched.return_value = fake_parser
+            rc = main([])
+        self.assertEqual(rc, 1)
+
+
+if __name__ == "__main__":
+    unittest.main()
diff --git a/tests/test_cli_keys.py b/tests/test_cli_keys.py
index aaacbb9..e926ebd 100644
--- a/tests/test_cli_keys.py
+++ b/tests/test_cli_keys.py
@@ -1,4 +1,5 @@
 import io
+import json
 import os
 import sys
 import tempfile
@@ -49,6 +50,8 @@ class TestCliKeys(unittest.TestCase):
         self.assertIn("API_KEY", out)
         self.assertIn("****", out)
         self.assertNotIn("sekrit", out)
+        self.assertIn("Secrets:", out)
+        self.assertIn("Diagnostics:", out)
 
     def test_keys_test_needs_names_without_mapping(self):
         from fmf.cli import main
@@ -71,7 +74,65 @@ class TestCliKeys(unittest.TestCase):
         self.assertEqual(rc, 2)
         self.assertIn("No secret names provided", buf.getvalue())
 
+    def test_keys_diagnostics_detects_missing_fields(self):
+        from fmf.cli import main
+
+        yaml_path = self._write_yaml(
+            """
+            project: fmf
+            auth: { provider: env }
+            connectors:
+              - name: s3_raw
+                type: s3
+            inference:
+              provider: aws_bedrock
+              aws_bedrock: { region: us-east-1 }
+            export:
+              sinks:
+                - name: ddb
+                  type: dynamodb
+            """
+        )
+        os.environ["DUMMY"] = "value"
+
+        buf = io.StringIO()
+        sys_stdout = sys.stdout
+        try:
+            sys.stdout = buf
+            rc = main(["keys", "test", "-c", yaml_path, "DUMMY"])
+        finally:
+            sys.stdout = sys_stdout
+
+        self.assertEqual(rc, 0)
+        out = buf.getvalue()
+        self.assertIn("Connector", out)
+        self.assertIn("WARN", out)
+        self.assertIn("missing fields", out)
+
+    def test_keys_json_output(self):
+        from fmf.cli import main
+
+        yaml_path = self._write_yaml(
+            """
+            project: fmf
+            auth: { provider: env }
+            """
+        )
+        os.environ["SECRET"] = "value"
+
+        buf = io.StringIO()
+        sys_stdout = sys.stdout
+        try:
+            sys.stdout = buf
+            rc = main(["keys", "test", "-c", yaml_path, "--json", "SECRET"])
+        finally:
+            sys.stdout = sys_stdout
+
+        self.assertEqual(rc, 0)
+        payload = json.loads(buf.getvalue())
+        self.assertEqual(payload["secrets"][0]["name"], "SECRET")
+        self.assertEqual(payload["secrets"][0]["status"], "OK")
+
 
 if __name__ == "__main__":
     unittest.main()
-
diff --git a/tests/test_config_models.py b/tests/test_config_models.py
index dd07afd..2c89bfe 100644
--- a/tests/test_config_models.py
+++ b/tests/test_config_models.py
@@ -125,8 +125,34 @@ class TestConfigModels(unittest.TestCase):
         as_dict = cfg.model_dump() if hasattr(cfg, "model_dump") else cfg
         self.assertEqual(as_dict["processing"]["text"]["chunking"]["max_tokens"], 256)
         self.assertEqual(as_dict["processing"]["text"]["chunking"]["overlap"], 32)
-        self.assertIs(as_dict["processing"]["text"]["normalize_whitespace"], True)
-        self.assertIsInstance(as_dict["export"]["sinks"], list)
+
+    def test_experimental_toggles_raise_environment(self):
+        from fmf.config.loader import load_config
+
+        yaml_path = self._write_yaml(
+            """
+            project: frontier-model-framework
+            experimental:
+              streaming: true
+              observability_otel: true
+            processing:
+              hash_algo: xxh64
+            retries:
+              max_elapsed_s: 12
+            """
+        )
+
+        os.environ.pop("FMF_EXPERIMENTAL_STREAMING", None)
+        os.environ.pop("FMF_OBSERVABILITY_OTEL", None)
+        os.environ.pop("FMF_HASH_ALGO", None)
+        os.environ.pop("FMF_RETRY_MAX_ELAPSED", None)
+
+        load_config(yaml_path)
+
+        self.assertEqual(os.environ.get("FMF_EXPERIMENTAL_STREAMING"), "1")
+        self.assertEqual(os.environ.get("FMF_OBSERVABILITY_OTEL"), "1")
+        self.assertEqual(os.environ.get("FMF_HASH_ALGO"), "xxh64")
+        self.assertEqual(os.environ.get("FMF_RETRY_MAX_ELAPSED"), "12.0")
 
 
 if __name__ == "__main__":
diff --git a/tests/test_connectors_hardening.py b/tests/test_connectors_hardening.py
new file mode 100644
index 0000000..8d93473
--- /dev/null
+++ b/tests/test_connectors_hardening.py
@@ -0,0 +1,74 @@
+from __future__ import annotations
+
+import os
+import sys
+import types
+import unittest
+
+
+class TestConnectorHardening(unittest.TestCase):
+    def setUp(self) -> None:
+        repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
+        src_path = os.path.join(repo_root, "src")
+        if src_path not in sys.path:
+            sys.path.insert(0, src_path)
+
+    def test_local_connector_uses_run_context(self) -> None:
+        from fmf.connectors.local import LocalConnector
+        from fmf.core.interfaces import RunContext
+
+        ctx = RunContext(run_id="test-run")
+        conn = LocalConnector(name="local", root=".")
+        # Should iterate without raising even when selector provided via context (unused)
+        refs = list(conn.list(selector=["**/*.py"], context=ctx))
+        self.assertIsInstance(refs, list)
+
+    def test_s3_open_wraps_body_with_context_manager(self) -> None:
+        from fmf.connectors.s3 import S3Connector
+
+        class DummyBody:
+            def __init__(self) -> None:
+                self.closed = False
+
+            def read(self, *_, **__):
+                return b"data"
+
+            def close(self):
+                self.closed = True
+
+        class DummyClient:
+            def __init__(self) -> None:
+                self.calls = {"list": 0, "get": 0, "head": 0}
+
+            def list_objects_v2(self, **kwargs):
+                self.calls["list"] += 1
+                return {"Contents": [{"Key": "foo.txt"}]}
+
+            def get_object(self, **kwargs):
+                self.calls["get"] += 1
+                return {"Body": DummyBody()}
+
+            def head_object(self, **kwargs):
+                self.calls["head"] += 1
+                return {"ContentLength": 4, "LastModified": None, "ETag": "etag"}
+
+        # Stub boto3 client
+        module = types.SimpleNamespace(client=lambda service, region_name=None: DummyClient())
+        original_boto3 = sys.modules.get("boto3")
+        sys.modules["boto3"] = module  # type: ignore
+        try:
+            conn = S3Connector(name="s3", bucket="b")
+            ref = next(iter(conn.list()))
+            stream = conn.open(ref)
+            with stream as handle:
+                self.assertEqual(handle.read(), b"data")
+            self.assertTrue(stream._body.closed)  # type: ignore[attr-defined]
+        finally:
+            if original_boto3 is None:
+                sys.modules.pop("boto3", None)
+            else:
+                sys.modules["boto3"] = original_boto3
+
+
+if __name__ == "__main__":
+        unittest.main()
diff --git a/tests/test_export_write_modes.py b/tests/test_export_write_modes.py
new file mode 100644
index 0000000..a2ef5ce
--- /dev/null
+++ b/tests/test_export_write_modes.py
@@ -0,0 +1,92 @@
+from __future__ import annotations
+
+import sys
+import types
+import unittest
+
+from fmf.core.interfaces import ExportSpec
+
+
+class TestExportWriteModes(unittest.TestCase):
+    def setUp(self) -> None:
+        self._orig_boto3 = sys.modules.get("boto3")
+
+    def tearDown(self) -> None:
+        if self._orig_boto3 is None:
+            sys.modules.pop("boto3", None)
+        else:
+            sys.modules["boto3"] = self._orig_boto3
+
+    def _install_stub(self) -> list[tuple[str, dict[str, str]]]:
+        calls: list[tuple[str, dict[str, str]]] = []
+
+        class DummyClient:
+            def put_object(self, **kwargs):
+                calls.append(("put", kwargs))
+
+            def copy_object(self, **kwargs):
+                calls.append(("copy", kwargs))
+
+            def delete_object(self, **kwargs):
+                calls.append(("delete", kwargs))
+
+        sys.modules["boto3"] = types.SimpleNamespace(client=lambda *_args, **_kwargs: DummyClient())  # type: ignore
+        return calls
+
+    def test_append_mode_generates_unique_keys(self) -> None:
+        calls = self._install_stub()
+        from fmf.exporters.s3 import S3Exporter
+
+        spec = ExportSpec(
+            name="s3",
+            type="s3",
+            format="jsonl",
+            write_mode="append",
+            options={"bucket": "demo", "prefix": "runs/"},
+        )
+        exporter = S3Exporter(spec=spec)
+        exporter.write([{"a": 1}], context={"run_id": "r1"})
+        put_calls = [c for c in calls if c[0] == "put"]
+        self.assertTrue(any(c[1]["Key"].startswith("runs/") for c in put_calls))
+        self.assertTrue(all("ContentMD5" in c[1] for c in put_calls))
+
+    def test_overwrite_mode_uses_copy(self) -> None:
+        calls = self._install_stub()
+        from fmf.exporters.s3 import S3Exporter
+
+        spec = ExportSpec(
+            name="s3",
+            type="s3",
+            format="jsonl",
+            write_mode="overwrite",
+            options={"bucket": "demo", "prefix": "runs/"},
+        )
+        exporter = S3Exporter(spec=spec)
+        exporter.write([{"a": 1}], context={"run_id": "r1", "filename": "results"})
+        put_temp = [c for c in calls if c[0] == "put"]
+        copy_calls = [c for c in calls if c[0] == "copy"]
+        delete_calls = [c for c in calls if c[0] == "delete"]
+        self.assertTrue(any(".tmp-" in c[1]["Key"] for c in put_temp))
+        self.assertTrue(copy_calls)
+        self.assertTrue(delete_calls)
+        self.assertTrue(all("ContentMD5" in c[1] if c[0] == "put" else True for c in calls))
+        self.assertTrue(all("CopySourceIfMatch" in c[1] for c in copy_calls))
+
+    def test_upsert_not_supported(self) -> None:
+        self._install_stub()
+        from fmf.exporters.s3 import S3Exporter
+
+        spec = ExportSpec(
+            name="s3",
+            type="s3",
+            format="jsonl",
+            write_mode="upsert",
+            options={"bucket": "demo"},
+        )
+        exporter = S3Exporter(spec=spec)
+        with self.assertRaises(Exception):
+            exporter.write([{"a": 1}])
+
+
+if __name__ == "__main__":
+    unittest.main()
diff --git a/tests/test_ids_provenance.py b/tests/test_ids_provenance.py
new file mode 100644
index 0000000..f532eae
--- /dev/null
+++ b/tests/test_ids_provenance.py
@@ -0,0 +1,59 @@
+from __future__ import annotations
+
+import os
+import sys
+import unittest
+
+
+class TestIdsAndProvenance(unittest.TestCase):
+    def setUp(self) -> None:
+        repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
+        src_path = os.path.join(repo_root, "src")
+        if src_path not in sys.path:
+            sys.path.insert(0, src_path)
+
+    def test_document_id_stable(self) -> None:
+        from fmf.processing.loaders import load_document_from_bytes
+
+        payload = b"hello world"
+        doc1 = load_document_from_bytes(
+            source_uri="file:///tmp/doc.txt",
+            filename="doc.txt",
+            data=payload,
+            processing_cfg={},
+        )
+        doc2 = load_document_from_bytes(
+            source_uri="file:///tmp/doc.txt",
+            filename="doc.txt",
+            data=payload,
+            processing_cfg={},
+        )
+        self.assertEqual(doc1.id, doc2.id)
+        self.assertIn("created_at", doc1.provenance)
+        self.assertEqual(doc1.provenance["source_uri"], "file:///tmp/doc.txt")
+
+    def test_normalize_handles_windows_newlines(self) -> None:
+        from fmf.core.ids import normalize_text, document_id
+
+        unix = "Line1\nLine2"
+        windows = "Line1\r\nLine2"
+        mac = "Line1\rLine2"
+        base = normalize_text(unix)
+        self.assertEqual(base, normalize_text(windows))
+        self.assertEqual(base, normalize_text(mac))
+
+        doc_a = document_id(source_uri="mem://x", payload=normalize_text(unix))
+        doc_b = document_id(source_uri="mem://x", payload=normalize_text(windows))
+        self.assertEqual(doc_a, doc_b)
+
+    def test_chunk_id_stable(self) -> None:
+        from fmf.processing.chunking import chunk_text
+
+        chunks_a = chunk_text(doc_id="doc_abcd", text="One. Two. Three.")
+        chunks_b = chunk_text(doc_id="doc_abcd", text="One. Two. Three.")
+        self.assertEqual([c.id for c in chunks_a], [c.id for c in chunks_b])
+        self.assertTrue(all("index" in c.provenance for c in chunks_a))
+
+
+if __name__ == "__main__":
+    unittest.main()
diff --git a/tests/test_inference_azure.py b/tests/test_inference_azure.py
index 579cf99..fcd8bb2 100644
--- a/tests/test_inference_azure.py
+++ b/tests/test_inference_azure.py
@@ -14,6 +14,8 @@ class TestAzureOpenAIClient(unittest.TestCase):
         from fmf.inference.azure_openai import AzureOpenAIClient
         from fmf.inference.base_client import Message
 
+        os.environ["FMF_EXPERIMENTAL_STREAMING"] = "1"
+
         def transport(payload):
             assert "messages" in payload
             return {
@@ -27,16 +29,36 @@ class TestAzureOpenAIClient(unittest.TestCase):
                 "usage": {"prompt_tokens": 5, "completion_tokens": 2},
             }
 
-        client = AzureOpenAIClient(endpoint="https://example", api_version="2024-02-15-preview", deployment="x", transport=transport)
-        toks = []
-        comp = client.complete([Message(role="system", content="s"), Message(role="user", content="u")], temperature=0.2, max_tokens=10, stream=True, on_token=toks.append)
+        def stream_transport(payload):
+            yield {"choices": [{"delta": {"content": "Hello "}}]}
+            yield {
+                "choices": [{"delta": {"content": "world"}, "finish_reason": "stop"}],
+                "usage": {"prompt_tokens": 5, "completion_tokens": 2},
+                "model": "gpt-4o-mini",
+            }
+
+        client = AzureOpenAIClient(
+            endpoint="https://example",
+            api_version="2024-02-15-preview",
+            deployment="x",
+            transport=transport,
+            stream_transport=stream_transport,
+        )
+        toks: list[str] = []
+        comp = client.complete(
+            [Message(role="system", content="s"), Message(role="user", content="u")],
+            temperature=0.2,
+            max_tokens=10,
+            stream=True,
+            on_token=toks.append,
+        )
         self.assertEqual(comp.text, "Hello world")
         self.assertEqual(comp.model, "gpt-4o-mini")
         self.assertEqual(comp.prompt_tokens, 5)
         self.assertEqual(comp.completion_tokens, 2)
-        self.assertGreater(len(toks), 1)
+        self.assertEqual(toks, ["Hello ", "world"])
+        os.environ.pop("FMF_EXPERIMENTAL_STREAMING", None)
 
 
 if __name__ == "__main__":
     unittest.main()
-
diff --git a/tests/test_provider_registry.py b/tests/test_provider_registry.py
new file mode 100644
index 0000000..961276c
--- /dev/null
+++ b/tests/test_provider_registry.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+import unittest
+
+
+class TestProviderRegistry(unittest.TestCase):
+    def test_registered_providers(self) -> None:
+        from fmf.inference import registry
+        from fmf.inference.azure_openai import AzureOpenAIClient
+
+        providers = registry.available_providers()
+        self.assertIn("azure_openai", providers)
+        client = registry.build_provider("azure_openai", {"endpoint": "https://unit", "api_version": "2024-02-15-preview", "deployment": "demo"})
+        self.assertIsInstance(client, AzureOpenAIClient)
+
+    def test_unknown_provider(self) -> None:
+        from fmf.inference import registry
+
+        with self.assertRaises(ValueError):
+            registry.build_provider("does-not-exist", {})
+
+
+if __name__ == "__main__":
+    unittest.main()
diff --git a/tests/test_retry.py b/tests/test_retry.py
new file mode 100644
index 0000000..1b1bbf2
--- /dev/null
+++ b/tests/test_retry.py
@@ -0,0 +1,34 @@
+from __future__ import annotations
+
+import unittest
+
+from fmf.core.retry import retry_call
+
+
+class TestRetryHelpers(unittest.TestCase):
+    def test_retry_succeeds_after_transient(self) -> None:
+        calls = {"count": 0}
+
+        class Dummy(Exception):
+            status_code = 500
+
+        def func():
+            calls["count"] += 1
+            if calls["count"] < 3:
+                raise Dummy()
+            return "ok"
+
+        result = retry_call(func, max_attempts=5, base_delay=0.0, max_delay=0.0, sleep=lambda _d: None)
+        self.assertEqual(result, "ok")
+        self.assertEqual(calls["count"], 3)
+
+    def test_retry_stops_on_non_retriable(self) -> None:
+        class CustomError(Exception):
+            status_code = 400
+
+        with self.assertRaises(CustomError):
+            retry_call(lambda: (_ for _ in ()).throw(CustomError()), max_attempts=2, sleep=lambda _d: None)
+
+
+if __name__ == "__main__":
+    unittest.main()
diff --git a/tests/test_streaming_adapters.py b/tests/test_streaming_adapters.py
new file mode 100644
index 0000000..b72187e
--- /dev/null
+++ b/tests/test_streaming_adapters.py
@@ -0,0 +1,87 @@
+from __future__ import annotations
+
+import os
+import unittest
+
+from fmf.core.interfaces import BaseProvider, CompletionRequest, CompletionResponse, ModelSpec
+from fmf.inference.azure_openai import AzureOpenAIClient
+from fmf.inference.bedrock import BedrockClient
+from fmf.inference.base_client import Message
+
+
+class TestStreamingAdapters(unittest.TestCase):
+    def tearDown(self) -> None:
+        os.environ.pop("FMF_EXPERIMENTAL_STREAMING", None)
+
+    def test_iter_tokens_default_returns_completion(self) -> None:
+        from fmf.core.interfaces import BaseProvider, CompletionRequest, CompletionResponse, ModelSpec
+
+        class EchoProvider(BaseProvider):
+            def __init__(self) -> None:
+                super().__init__(ModelSpec(provider="echo", model="m"))
+
+            def complete(self, request: CompletionRequest) -> CompletionResponse:
+                payload = "".join(part if isinstance(part, str) else str(part) for part in request.messages)
+                return CompletionResponse(text=payload)
+
+        provider = EchoProvider()
+        tokens: list[str] = []
+        response = provider.stream(
+            CompletionRequest(messages=[{"role": "user", "content": "hello"}]),
+            on_token=tokens.append,
+        )
+        self.assertEqual(tokens, ["role='user' content='hello'"])
+        self.assertEqual(response.text, "role='user' content='hello'")
+
+    def test_azure_streaming_flag_enabled(self) -> None:
+        os.environ["FMF_EXPERIMENTAL_STREAMING"] = "true"
+
+        tokens: list[str] = []
+
+        def stream_transport(payload):
+            yield {"choices": [{"delta": {"content": "A"}}]}
+            yield {"choices": [{"delta": {"content": "B"}, "finish_reason": "stop"}]}
+
+        client = AzureOpenAIClient(
+            endpoint="https://example",
+            api_version="2024-02-15-preview",
+            deployment="demo",
+            transport=lambda payload: {
+                "choices": [{"message": {"content": "fallback"}, "finish_reason": "stop"}],
+                "usage": {},
+                "model": "demo",
+            },
+            stream_transport=stream_transport,
+        )
+        response = client.complete(
+            [Message(role="user", content="hi")],
+            stream=True,
+            on_token=tokens.append,
+        )
+        self.assertEqual(tokens, ["A", "B"])
+        self.assertEqual(response.text, "AB")
+
+    def test_bedrock_streaming_flag_disabled(self) -> None:
+        os.environ.pop("FMF_EXPERIMENTAL_STREAMING", None)
+        tokens: list[str] = []
+
+        client = BedrockClient(
+            region="us-east-1",
+            model_id="anthropic.test",
+            transport=lambda payload: {
+                "output": {"text": "complete"},
+                "usage": {"input_tokens": 2, "output_tokens": 1},
+            },
+            stream_transport=lambda payload: [{"content": "ignored"}],
+        )
+        response = client.complete(
+            [Message(role="user", content="hi")],
+            stream=True,
+            on_token=tokens.append,
+        )
+        self.assertEqual(tokens, ["complete"])
+        self.assertEqual(response.text, "complete")
+
+
+if __name__ == "__main__":
+    unittest.main()
